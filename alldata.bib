@ARTICLE{Miller202265,
type={ARTICLE},
author={Miller, G.J.},
title={Artificial Intelligence Project Success Factorsâ€”Beyond the Ethical Principles},
journal={Lecture Notes in Business Information Processing},
year={2022},
volume={442 LNBIP},
pages={65-96},
doi={10.1007/978-3-030-98997-2_4},
note={cited By 0; Conference of 16th Conference on Information Systems Management, ISM 2021 and Information Systems and Technologies conference track, FedCSIS-IST 2021 Held as Part of 16th Conference on Computer Science and Information Systems, FedCSIS 2021 ; Conference Date: 2 September 2021 Through 5 September 2021;  Conference Code:275459},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127835873&doi=10.1007%2f978-3-030-98997-2_4&partnerID=40&md5=4c7ebbe5c81928d5ffa1ddd6db9c4973},
affiliation={maxmetrics, Heidelberg, Germany},
abstract={The algorithms implemented through artificial intelligence (AI) and big data projects are used in life-and-death situations. Despite research that addresses varying aspects of moral decision-making based upon algorithms, the definition of project success is less clear. Nevertheless, researchers place the burden of responsibility for ethical decisions on the developers of AI systems. This study used a systematic literature review to identify five categories of AI project success factors in 17 groups related to moral decision-making with algorithms. It translates AI ethical principles into practical project deliverables and actions that underpin the success of AI projects. It considers success over time by investigating the development, usage, and consequences of moral decision-making by algorithmic systems. Moreover, the review reveals and defines AI success factors within the project management literature. Project managers and sponsors can use the results during project planning and execution. Â© 2022, The Author(s).},
author_keywords={Algorithms;  Artificial intelligence;  Big data;  Critical success factors;  Moral decision-making;  Project management},
keywords={Artificial intelligence;  Big data;  Ethical technology;  Project management, Artificial intelligence systems;  Critical success factor;  Decisions makings;  Ethical principles;  Life; death;  Moral decision-making;  Project success;  Project success factors;  Success factors;  Systematic literature review, Decision making},
references={Helberger, N., Araujo, T., de Vreese, C.H., Who is the fairest of them all? Public attitudes and expectations regarding automated decision-making (2020) Comput. Law Secur. Rev., 39 (1-16). , https://dx.doi.org/10.1016/j.clsr.2020.105456; Garfinkel, S., Matthews, J., Shapiro, S.S., Smith, J.M., Toward algorithmic transparency and accountability (2017) Commun. ACM, 60 (9), p. 5. , https://dx.doi.org/10.1145/3125780; Boonjing, V., Pimchangthong, D., Data mining for positive customer reaction to advertising in social media (2018) AITM/ISM-2017. LNBIP, 311, pp. 83-95. , https://dx.doi.org/10.1007/978-3-319-77721-4_5, Ziemba, E. (ed.), vol., pp. , Springer, Cham; Yadav, G., Kumar, Y., Sahoo, G., Predication of Parkinsonâ€™s disease using data mining methods: A comparative analysis of tree, statistical and support vector machine classifiers (2012) Proceedings International Conference Computing Communication Systems, Pp. 1âˆ’8. IEEE, , https://doi.org/10.1109/NCCCS.2012.6413034; Abdelaal, M.M.A., Sena, H.A., Farouq, M.W., Salem, A.-B.M.: Using data mining for assessing diagnosis of breast cancer. In: Proceedings International Multiconference Computing Science Information Technology, pp. 11âˆ’17. IEEE (2010). https://dx.doi.org/10.1109/IMCSIT. 2010.5679647; Hamon, R., Junklewitz, H., Malgieri, G., de Hert, P., Beslay, L., Sanchez, I., Impossible explanations? Beyond explainable AI in the GDPR from a COVID-19 use case scenario (2021) Facct 2021: Proceedings 2021 ACM Conference Fairness Accountability and Transparency, Pp. 549âˆ’559. ACM, , https://dx.doi.org/10.1145/3442188.3445917; Sherer, J.A., When is a chair not a chair?: Big data algorithms, disparate impact, and considerations of modular programming (2017) Comput. Internet Lawyer, 34 (8), pp. 6-10; BonsÃ³n, E., Lavorato, D., Lamboglia, R., Mancini, D., Artificial intelligence activities and ethical approaches in leading listed companies in the European union (2021) Int. J. Account. Inf. Syst., 43. , https://doi.org/10.1016/j.accinf.2021.100535; Shenhar, A.J., Dvir, D., Levy, O., Maltz, A.C., Project success: A multidimensional strategic concept (2001) Long Range Plan, 34 (6), pp. 699-725. , https://doi.org/10.1016/S0024-630 1(01)00097-8; Davis, K., An empirical investigation into different stakeholder groups perception of project success (2017) Int. J. Project Manage., 35 (4), pp. 604-617. , https://dx.doi.org/10.1016/j.ijproman. 2017.02.004; Mitchell, R.K., Agle, B.R., Wood, D.J., Toward a theory of stakeholder identification and salience: defining the principle of who and what really counts (1997) Acad. Manage. Rev., 22 (4), pp. 853-886. , https://dx.doi.org/10.5465/amr.1997.9711022105; Ryan, M., Stahl, B.C., Artificial intelligence ethics guidelines for developers and users: Clarifying their content and normative implications (2021) J. Inf. Commun. Ethics Soc., 19 (1), pp. 61-86. , https://dx.doi.org/10.1108/JICES-12-2019-0138; Leyh, C., Critical success factors for ERP projects in small and medium-sized enterprises-the perspective of selected German SMEs (2014) Proceedings 2014 Federated Conference Computing Science Information Systems Fedcsis 2014, Pp. 1181âˆ’1190. ACSIS, , https://dx.doi.org/10.15439/2014F243; Mittelstadt, B., Principles alone cannot guarantee ethical AI (2019) Nat. Mach. Intell., 1 (11), pp. 501-507. , https://doi.org/10.1038/s42256-019-0114-4; Manders-Huits, N., Moral responsibility and it for human enhancement (2006) SAC 2006: Proceedings 2006 ACM Symposium Application Computing, pp. 267-271. , https://dx.doi.org/10.1145/1141277.1141340, , pp. , ACM; Martin, K., Ethical implications and accountability of algorithms (2018) J. Bus. Ethics, 160 (4), pp. 835-850. , https://dx.doi.org/10.1007/s10551-018-3921-3; Wachnik, B.: Moral hazard in IT project completion. An analysis of supplier and client behavior in polish and German enterprises. In: Ziemba, E. (ed.) Information Technology for Management. LNBIP, vol. 243, pp. 77â€“90. Springer, Cham (2016). https://dx.doi.org/10. 1007/978-3-319-30528-8_5; Ika, L.A., Project success as a topic in project management journals (2009) Proj. Manag. J., 40 (4), pp. 6-19. , https://dx.doi.org/10.1002/pmj.20137; Weninger, C., Project initiation and sustainability principles: What global project management standards can learn from development projects when analyzing investments (2012) PMI Research Education Conference, , Newtown Square, PA: Project Management Institute; Turner, R.J., Zolin, R., Forecasting success on large projects: Developing reliable scales to predict multiple perspectives by multiple stakeholders over multiple time frames (2012) Proj. Manag. J., 43 (5), pp. 87-99. , https://dx.doi.org/10.1002/pmj.21289; Pinto, J.K., Slevin, D.P., Critical success factors across the project life cycle (1988) Proj. Manag. J., 19 (3), pp. 67-75; Leyh, C., KÃ¶ppel, K., Neuschl, S., Pentrack, M., Critical success factors for digitalization projects (2021) Proceedings16th Conference Computing Science Intelligent System Fedcsis 2021, Pp. 427âˆ’436. ACSIS, , https://dx.doi.org/10.15439/2021F122; WÅ‚odarski, R., Poniszewska-MaraÅ„da, A., Measuring dimensions of software engineering projectsâ€™ success in an academic context (2017) Proceedings 2017 Federated Conference Computing Science Information System Fedcsis 2017, Pp. 1207âˆ’1210. ACSIS, , https://dx.doi.org/10.15439/2017F295; Ralph, P., Kelly, P., The dimensions of software engineering success (2014) Proceedings-2017 IEEE/ACM 39Th International Conference Software Engineering, pp. 24-35. , https://doi.org/10.1145/2568225.2568261, , pp. , ACM; Chatzoglou, P., Chatzoudes, D., Fragidis, L., Symeonidis, S., Examining the critical success factors for ERP implementation: An explanatory study conducted in SMEs (2017) AITM/ISM-2016. LNBIP, 277, pp. 179-201. , https://dx. doi.org/10.1007/978-3-319-53076-5_10, Ziemba, E. (ed.), vol., pp. , Springer, Cham; Leyh, C., Gebhardt, A., Berton, P., Implementing ERP systems in higher education institutes critical success factors revisited (2017) Proceedings 2017 Federated Conference Computing Science Information System Fedcsis 2017, Pp. 913âˆ’917. ACSIS, , https://dx.doi.org/10.15439/2017F364; Miller, G.J., A conceptual framework for interdisciplinary decision support project success (2019) 2019 IEEE Technology Engineering Management Conference TEMSCON 2019, Pp. 1âˆ’8. IEEE, , https://dx.doi.org/10.1109/TEMSCON.2019.8813650; Miller, G.J., Quantitative comparison of big data analytics and business intelligence project success factors (2019) AITM/ISM-2018. LNBIP, 346, pp. 53-72. , https://dx.doi.org/10.1007/978-3-030-15154-6_4, Ziemba, E. (ed.), vol., pp. , Springer, Cham; Petter, S., McLean, E.R., A meta-analytic assessment of the delone and mclean is success model: An examination of is success at the individual level (2009) Inform. Manage., 46 (3), pp. 159-166. , https://dx.doi.org/10.1016/j.im.2008.12.006; Umar Bashir, M., Sharma, S., Kar, A.K., Manmohan Prasad, G., Critical success factors for integrating artificial intelligence and robotics (2020) Digit. Policy Regul. Gov., 22 (4), pp. 307-331. , https://doi.org/10.1108/DPRG-03-2020-0032; Iqbal, R., Doctor, F., More, B., Mahmud, S., Yousuf, U., Big data analytics and computational intelligence for cyber-physical systems: Recent trends and state of the art applications (2017) Future Gener. Comput. Syst., 105, pp. 766-778. , https://doi.org/10.1016/j.future.2017.10.021; Aggarwal, J., Kumar, S., A survey on artificial intelligence (2018) Int. J. Res. Eng. Sci. Manage., 1 (12), pp. 244-245. , https://dx.doi.org/10.31224/osf.io/47a85; Homayounfar, P., Owoc, M.L., Data mining research trends in computerized patient records (2011) Proceedings 2011 Federated Conference Computing Science Information System Fedcsis 2011, Pp. 133âˆ’139. IEEE; (2019) OECD: Artificial Intelligence in Society, , OECD Publishing, Paris; Jones, T.M., Ethical decision making by individuals in organizations: An issue-contingent model (1991) Acad. Manage. Rev., 16 (2), pp. 366-395; Anscombe, G.E.M., Modern moral philosophy (1969) The Is-Ought Question. CP, pp. 175-195. , https://doi.org/10.1007/978-1-349-15336-7_19, Hudson, W.D. (ed.), pp. , Palgrave Macmillan UK, London; Shaw, N.P., StÃ¶ckel, A., Orr, R.W., Lidbetter, T.F., Cohen, R.: Towards provably moral AI agents in bottom-up learning frameworks. In: AIES 2018: Proceedings 2018 AAAI/ACM Conference AI, Ethics Society, pp. 271â€“277. ACM (2018). https://dx.doi.org/10.1145/327 8721.3278728; Cohen, I.G., Amarasingham, R., Shah, A., Xie, B., Lo, B., The legal and ethical concerns that arise from using complex predictive analytics in health care (2014) Health Affair, 33 (7), pp. 1139-1147. , https://dx.doi.org/10.1377/hlthaff.2014.0048; Jobin, A., Ienca, M., Vayena, E., The global landscape of AI ethics guidelines (2019) Nat. Mach. Intell., 1 (9), pp. 389-399. , https://doi.org/10.1038/s42256-019-0088-2; Hopkins, A., Booth, S.: Machine learning practices outside big tech: How resource constraints challenge responsible development. In: AIES 2018: Proceedings 2018 AAAI/ACM Conference AI, Ethics Society, pp. 134â€“145. ACM (2021). https://dx.doi.org/10.1145/3461702.346 2527; Moher, D., Liberati, A., Tetzlaff, J., Altman, D.G., Preferred reporting items for systematic reviews and meta-analyses: The prisma statement (2010) Int. J. Surg., 8 (5), pp. 336-341. , https://dx.doi.org/10.1016/j.ijsu.2010.02.007; Wieringa, M., What to account for when accounting for algorithms: A systematic literature review on algorithmic accountability (2020) FAT* 2020-Proceedings 2020 Conference Fairness Accountability Transparency, pp. 1-18. , https://dx.doi.org/10.1145/3351095. 3372833, , pp. , ACM; Aguirre, A., Dempsey, G., Surden, H., Reiner, P.B., AI loyalty: A new paradigm for aligning stakeholder interests (2020) IEEE Trans. Technol. Soc., 1 (3), pp. 128-137. , https://dx.doi.org/10. 1109/TTS.2020.3013490; Brady, A.P., Neri, E., Artificial intelligence in radiologyâ€”ethical considerations (2020) Diagnostics, 10 (4), p. 231. , https://dx.doi.org/10.3390/diagnostics10040231; Cobbe, J., Lee, M.S.A., Singh, J., Reviewable automated decision-making: A framework for accountable algorithmic systems (2021) Facct 2021: Proceedings 2021 ACM Conference Fairness Accountability Transparency, pp. 598-609. , https://dx.doi.org/10.1145/3442188.3445921, , pp. , ACM; Jacovi, A., Marasovi: Formalizing trust in artificial intelligence: prerequisites, causes and goals of human trust in AI (2021) Facct 2021: Proceedings 2021 ACM Conference Fairness Accountability Transparency, pp. 624-635. , https://dx.doi.org/10.1145/3442188.3445923, , pp. , ACM; Loi, M., Heitz, C., Christen, M., A comparative assessment and synthesis of twenty ethics codes on AI and big data (2020) 7Th Swiss Conference Data Science, Pp. 41â€“46. IEEE, , https://dx.doi.org/10.1109/SDS49233.2020.00015; McGrath, S.K., Whitty, S.J., Accountability and responsibility defined (2018) Int. J. Manag. Proj. Bus., 11 (3), pp. 687-707. , https://dx.doi.org/10.1108/IJMPB-06-2017-0058; Rezania, D., Baker, R., Nixon, A., Exploring project managersâ€™ accountability (2019) Int. J. Manag. Proj. Bus., 12 (4), pp. 919-937. , https://dx.doi.org/10.1108/IJMPB-03-2018-0037; Bondi, E., Xu, L., Acosta-Navas, D., Killian, J.A.: Envisioning communities: a participatory approach towards AI for social good. In: AIES 2018: Proceedings 2018 AAAI/ACM Conference AI, Ethics Society, pp. 425â€“436. ACM (2021). https://dx.doi.org/10.1145/3461702.346 2612; Bertino, E., Kundu, A., Sura, Z., Data transparency with blockchain and AI ethics (2019) ACM J. Data Inf. Qual., 11 (4), pp. 1-8. , https://dx.doi.org/10.1145/3312750; Rossi, A., Lenzini, G., Transparency by design in data-informed research: A collection of information design patterns (2020) Comput. Law Secur. Rev., 37, pp. 1-22. , https://dx.doi.org/10. 1016/j.clsr.2020.105402; Rodrigues, R., Legal and human rights issues of AI: Gaps, challenges and vulnerabilities (2020) J Responsible Tech, 4. , https://doi.org/10.1016/j.jrt.2020.100005; Lim, J.H., Kwon, H.Y., A study on the modeling of major factors for the principles of AI ethics (2021) DG.O2021: 22Nd Annual International Conference Digital Government Research, pp. 208-218. , https://doi.org/10.1145/3463677.3463733, , pp. , ACM; Unceta, I., Nin, J., Pujol, O.: Risk mitigation in algorithmic accountability: the role of machine learning copies. PLoS One 15(11), e0241286 (2020). https://dx.doi.org/10.1371/journal.pone. 0241286; Langer, M., Landers, R.N., The future of artificial intelligence at work: A review on effects of decision automation and augmentation on workers targeted by algorithms and third-party observers (2021) Comput. Hum. Behav., 123 (1068). , https://dx.doi.org/10.1016/j.chb.2021; Metcalf, J., Moss, E., Watkins, E.A., Singh, R., Elish, M.C., Algorithmic impact assessments and accountability: The co-construction of impacts (2021) Facct 2021: Proceedings 2021 ACM Conference Fairness Accountability Transparency, pp. 735-746. , https://dx.doi. org/10.1145/3442188.3445935, , pp. , ACM; Eslami, M., Vaccaro, K., Lee, M.K., On, A.E.B., Gilbert, E., Karahalios, K., User attitudes towards algorithmic opacity and transparency in online reviewing platforms (2019) CHI 2019: Proceedings 2019 CHI Conference Human Factors Computing Systems, pp. 1-14. , https://dx.doi.org/10.1145/3290605.3300724, , pp. , ACM; Shneiderman, B., Bridging the gap between ethics and practice: guidelines for reliable, safe, and trustworthy human-centered AI systems (2020) ACM Trans. Interact. Intell. Syst., 10 (4), pp. 1-31. , https://dx.doi.org/10.1145/3419764; BÃ¼chi, M., Fosch-Villaronga, E., Lutz, C., TamÃ²-Larrieux, A., Velidi, S., Viljoen, S., The chilling effects of algorithmic profiling: Mapping the issues (2020) Comput. Law Secur. Rev., 36 (1-15). , https://dx.doi.org/10.1016/j.clsr.2019.105367; Munoko, I., Brown-Liburd, H.L., Vasarhelyi, M., The ethical implications of using artificial intelligence in auditing (2020) J. Bus. Ethics, 167 (2), pp. 209-234. , https://dx.doi.org/10.1007/s10551-019-04407-1; Gebru, T., et al.: Datasheets for datasets. arXiv preprint https://arxiv.org/abs/1803.09010v7 (2018); Hutchinson, B., Towards accountability for machine learning datasets: Practices from software engineering and infrastructure (2021) Facct 2021: Proceedings. 2021 ACM Conference Fairness Accountability Transparency, pp. 560-575. , https://dx.doi.org/10.1145/3442188.3445918, , pp. , ACM; Wagner, B., Rozgonyi, K., Sekwenz, M.-T., Cobbe, J., Singh, J., Regulating transparency? Facebook, twitter and the German network enforcement act (2020) FAT* 2020-Proceedings 2020 Conference Fairness Accountability Transparency, pp. 261-271. , https://doi.org/10.1145/3351095.3372856, , pp. , ACM; Watson, H.J., Conner, N., Addressing the growing need for algorithmic transparency (2019) Commun. Assoc. Inf. Syst., 45, pp. 488-510. , https://dx.doi.org/10.17705/1CAIS.04526; Shin, D., Park, Y.J.: Role of fairness, accountability, and transparency in algorithmic affor-dance. Comput. Hum. Behav. 98, 277â€“284 (2019). https://dx.doi.org/10.1016/j.chb.2019. 04.019; Adam, H., The ghost in the legal machine: Algorithmic governmentality, economy, and the practice of law (2018) J. Inf. Commun. Ethics Soc., 16 (1), pp. 16-31. , https://dx.doi.org/10.1108/JICES-09-2016-0038; Alasadi, J., Al Hilli, A., Singh, V.K., Toward fairness in face matching algorithms (2019) FAT* 2019-Proceedings 2019 Conference Fairness Accountability Transparency Multimedia, pp. 19-25. , https://dx.doi.org/10.1145/3347447.3356751, , pp. , ACM; Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S., On the dangers of stochastic parrots: Can language models be too big? (2021) Facct 2021: Proceedings 2021 ACM Conference Fairness Accountability Transparency, pp. 610-623. , https://dx.doi.org/10.1145/3442188.3445922, , pp. , ACM; Kang, Y., Chiu, Y.W., Lin, M.Y., Su, F.Y., Huang, S.T., Towards model-informed precision dosing with expert-in-the-loop machine learning (2021) Proceedings-2021 IEEE 22Nd International Conference Information Reuse Integrated Data Science IRI 2021, Pp. 342â€“347. IEEE, , https://dx.doi.org/10.1109/IRI51335.2021.00053; Mitchell, M., Model cards for model reporting (2019) FAT* 2019-Proceedings. 2019 Conference Fairness Accountability Transparency, pp. 220-229. , https://dx.doi. org/10.1145/3287560.3287596, , pp. , ACM; Wan, W.X., Lindenthal, T., Towards accountability in machine learning applications: A system-testing approach (2021) SSRN Electron. J., 1. , https://dx.doi.org/10.2139/ssrn.3758451; Harrison, G., Hanson, J., Jacinto, C., Ramirez, J., Ur, B., An empirical study on the perceived fairness of realistic, imperfect machine learning models (2020) FAT* 2020-Proceedings 2020 Conference Fairness Accountability Transparency, pp. 392-402. , https://dx.doi. org/10.1145/3351095.3372831, , pp. , ACM; Gandy, O.H., Engaging rational discrimination: Exploring reasons for placing regulatory constraints on decision support systems (2010) Ethics Inf. Technol., 12 (1), pp. 29-42. , https://dx.doi. org/10.1007/s10676-009-9198-6; Chazette, L., Brunotte, W., Speith, T.: Exploring explainability: a definition, a model, and a knowledge catalogue. In: Proceedings-2021 IEEE 29th International Requirements Engineering Conference RE 2021, pp. 197â€“208. IEEE (2021). https://dx.doi.org/10.1109/RE5 1729.2021.00025; Mariotti, E., Alonso, J.M., Confalonieri, R., A framework for analyzing fairness, accountability, transparency and ethics: A use-case in banking services (2021) 2021 IEEE International Conference Fuzzy System (FUZZ-IEEE), Pp. 1â€“6. IEEE, , https://dx.doi.org/10.1109/FUZZ45933.2021.9494481; Albrecht, U.-V., Transparency of health-apps for trust and decision making (2013) J. Med. Internet Res., 15 (12), pp. 1-5. , https://dx.doi.org/10.2196/jmir.2981; Givens, A.R., Morris, M.R., Centering disability perspectives in algorithmic fairness, accountability and transparency (2020) FAT* 2020-Proceedings 2020 Conference Fairness Accountability Transparency, p. 684. , https://dx.doi.org/10.1145/3351095.337 5686, , p. , ACM; Vallejos, E.P., Koene, A., Portillo, V., Dowthwaite, L., Cano, M., Young peopleâ€™s policy recommendations on algorithm fairness (2017) Websci 2017: Proceedings 2017 ACM Web Science Conference, pp. 247-251. , https://dx.doi.org/10.1145/3091478.3091512, , pp. , ACM; Janssen, M., Brous, P., Estevez, E., Barbosa, L.S., Janowski, T., Data governance: organizing data for trustworthy artificial intelligence (2020) Gov. Inf. Q., 37 (3). , https://doi.org/10.1016/j.giq.2020.101493; Bhatt, U., Explainable machine learning in deployment (2020) FAT* 2020-Proceedings 2020 Conference Fairness Accountability Transparency, pp. 648-657. , https://dx.doi.org/10.1145/3351095.3375624, , pp. , ACM; Scoleze Ferrer Paulo, S., GalvÃ£o Graziela Darla, A., de Carvalho Marly, M., Tensions between compliance, internal controls and ethics in the domain of project governance (2020) Int. J. Manag. Proj. Bus., 13 (4), pp. 845-865. , https://dx.doi.org/10.1108/IJMPB-07-2019-0171; Mowbray, A., Chung, P., Greenleaf, G., Utilising AI in the legal assistance sectorâ€”testing a role for legal information institutes (2020) Comput. Law Secur. Rev., 38, pp. 1-9. , https://dx.doi. org/10.1016/j.clsr.2020.105407; Joerin, A., Rauws, M., Fulmer, R., Black, V., Ethical artificial intelligence for digital health organizations (2020) Cureus, 12 (3), p. e7202. , https://dx.doi.org/10.7759/cureus.7202; Matthews, J., Patterns and antipatterns, principles, and pitfalls: Accountability and transparency in artificial intelligence (2020) AI Mag, 41 (1), pp. 82-89; Proposal for a regulation of the European Parliament and of the Council: Laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain union legislative acts C (2021) Artificial Intelligence Act, , F.R; Ziemba, E., The ICT adoption in enterprises in the context of the sustainable information society (2017) Proceedings 2017 Federated Conference Computing Science Information System Fedcsis 2017, Pp. 1031â€“1038. ACSIS, , https://dx.doi.org/10.15439/2017F89},
correspondence_address1={Miller, G.J.; maxmetricsGermany; email: g.j.m@ieee.org},
editor={Ziemba E., Chmielarz W.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18651348},
isbn={9783030989965},
language={English},
abbrev_source_title={Lect. Notes Bus. Inf. Process.},
document_type={Conference Paper},
source={Scopus},
number={NA},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Telkamp2022,
type={ARTICLE},
author={Telkamp, J.B. and Anderson, M.H.},
title={The Implications of Diverse Human Moral Foundations for Assessing the Ethicality of Artificial Intelligence},
journal={Journal of Business Ethics},
year={2022},
volume={NA},
pages={NA},
doi={10.1007/s10551-022-05057-6},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124730522&doi=10.1007%2fs10551-022-05057-6&partnerID=40&md5=919d2ebdae744cd9f91f178cd6c6b22c},
affiliation={Department of Management and Entrepreneurship, Ivy College of Business, Steve and Becky Smith Management and Marketing Suite, Iowa State University, 2350 Gerdin Business Building, 2167 Union Drive, Ames, IA  50011-2027, United States},
abstract={Organizations are making massive investments in artificial intelligence (AI), and recent demonstrations and achievements highlight the immense potential for AI to improve organizational and human welfare. Yet realizing the potential of AI necessitates a better understanding of the various ethical issues involved with deciding to use AI, training and maintaining it, and allowing it to make decisions that have moral consequences. People want organizations using AI and the AI systems themselves to behave ethically, but ethical behavior means different things to different people, and many ethical dilemmas require trade-offs such that no course of action is universally considered ethical. How should organizations using AIâ€”and the AI itselfâ€”process ethical dilemmas where humans disagree on the morally right course of action? Though a variety of ethical AI frameworks have been suggested, these approaches do not adequately address how people make ethical evaluations of AI systems or how to incorporate the fundamental disagreements people have regarding what is and is not ethical behavior. Drawing on moral foundations theory, we theorize that a person will perceive an organizationâ€™s use of AI, its data procedures, and the resulting AI decisions as ethical to the extent that those decisions resonate with the personâ€™s moral foundations. Since people hold diverse moral foundations, this highlights the crucial need to consider individual moral differences at multiple levels of AI. We discuss several unresolved issues and suggest potential approaches (such as moral reframing) for thinking about conflicts in moral judgments concerning AI. Â© 2022, The Author(s), under exclusive licence to Springer Nature B.V.},
author_keywords={Artificial intelligence;  Ethical AI frameworks;  Moral foundations;  Moral judgment},
keywords={NA},
references={Atari, M., Graham, J., Dehghani, M., Foundations of morality in Iran (2020) Evolution and Human Behavior, 41 (5), pp. 367-384; Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., Bonnefon, J.-F., Rahwan, I., The moral machine experiment (2018) Nature, 563 (7729), pp. 59-64; (2020), https://behavioralscientist.org/why-we-should-crowdsource-ai-ethics-and-how-to-do-so-responsibly/, Levine, S, (, September 7), Why we should crowdsource AI ethics (and how to do so responsibly). Behavioral Scientist. Retrieved from; Badaracco, J.L., Jr., (1997) Defining moments: When managers must choose between right and right, , Harvard Business School Press; Bongard, A., Automating talent acquisition: Smart recruitment, predictive hiring algorithms, and the data-driven nature of artificial intelligence (2019) Psychosociological Issues in Human Resource Management, 7 (1), pp. 36-41; Booth, R., (2019) UK Businesses Using Artificial Intelligence to Monitor Staff Activity. the Guardian, , https://www.theguardian.com/technology/2019/apr/07/uk-businesses-using-artifical-intelligence-to-monitor-staff-activity, April 7, Retrieved from; Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Winter, C., (2020) Language Models are Few-Shot Learners, , http://arxiv.org/abs/2005.14165, ArXiv, Retrieved from; Clifford, S., Iyengar, V., Cabeza, R., Sinnott-Armstrong, W., Moral foundations vignettes: A standardized stimulus database of scenarios based on moral foundations theory (2015) Behavior Research Methods, 47 (4), pp. 1178-1198; Cook, W., Kuhn, K.M., Off-duty deviance in the eye of the beholder: Implications of moral foundations theory in the age of social media (2021) Journal of Business Ethics, 172 (3), pp. 605-620; Crone, D.L., Laham, S.M., Multiple moral foundations predict responses to sacrificial dilemmas (2015) Personality and Individual Differences, 85, pp. 60-65; Cutter, C., Your next job interview may be with a robot (2018) Wall Street Journal, , https://www.wsj.com/articles/its-time-for-your-job-interview-youll-be-talking-to-yourself-1543418495, November 28, Retrieved from; Dastin, J., (2018) Amazon Scraps Secret AI Recruiting Tool that Showed Bias against Women. Reuters, , https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G, October 10, Retrieved from; DoÄŸruyol, B., Alper, S., Yilmaz, O., The five-factor model of the moral foundations theory is stable across WEIRD and non-WEIRD cultures (2019) Personality and Individual Differences, 151, p. 109547; Donaldson, T., Dunfee, T.W., Toward a unified conception of business ethics: Integrative social contracts theory (1994) Academy of Management Review, 19 (2), pp. 252-284; Egorov, M., Kalshoven, K., Pircher Verdorfer, A., Peus, C., Itâ€™s a match: Moralization and the effects of moral foundations congruence on ethical and unethical leadership perception (2020) Journal of Business Ethics, 167 (4), pp. 707-723; Etzioni, A., Moral dialogues (2018) Happiness is the wrong metric: A liberal communitarian response to populism, pp. 65-86. , Etzioni A, (ed), Springer; Fehr, R., Yam, K.C., Dang, C., Moralized leadership: The construction and consequences of ethical leader perceptions (2015) Academy of Management Review, 40 (2), pp. 182-209; Feinberg, M., Willer, R., The moral roots of environmental attitudes (2013) Psychological Science, 24 (1), pp. 56-62; Feinberg, M., Willer, R., Moral reframing: A technique for effective and persuasive communication across political divides (2019) Social and Personality Psychology Compass, 13 (12); Floridi, L., Cowls, J., A unified framework of five principles for AI in society (2019) Harvard Data Science Review; Floridi, L., Cowls, J., Beltrametti, M., Chatila, R., Chazerand, P., Dignum, V., Luetge, C., Vayena, E., AI4peopleâ€”An ethical framework for a good AI society: Opportunities, risks, principles, and recommendations (2018) Minds and Machines, 28 (4), pp. 689-707; Freeman, R.E., (1984) Strategic management: A stakeholder approach, , Pitman; Frey, B.S., Homberg, F., Osterloh, M., Organizational control systems and pay-for-performance in the public service (2013) Organization Studies, 34 (7), pp. 949-972; Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J.W., Wallach, H., DaumÃ©, I.I.I.H., Crawford, K., (2020) Datasheets for Datasets, , http://arxiv.org/abs/1803.09010, ArXiv, Retrieved from; Gilligan, C., (1982) In a different voice, , Harvard University Press; Gioia, D.A., Pinto fires and personal ethics: A script analysis of missed opportunities (1992) Journal of Business Ethics, 11 (5), pp. 379-389; Glikson, E., Woolley, A.W., Human trust in artificial intelligence: Review of empirical research (2020) Academy of Management Annals, 14 (2), pp. 627-660; Goodwin, D.K., (2005) Team of rivals: The political genius of Abraham Lincoln, , Simon and Schuster; Graham, J., Haidt, J., Koleva, S., Motyl, M., Iyer, R., Wojcik, S.P., Ditto, P.H., Moral foundations theory: The pragmatic validity of moral pluralism (2013) Advances in Experimental Social Psychology, 47, pp. 55-130; Graham, J., Haidt, J., Motyl, M., Meindl, P., Iskiwitch, C., Mooijman, M., Moral foundations theory: On the advantages of moral pluralism over moral monism (2018) Atlas of moral psychology, pp. 211-222. , Gray K, Graham J, (eds), The Guilford Press; Graham, J., Haidt, J., Nosek, B.A., Liberals and conservatives rely on different sets of moral foundations (2009) Journal of Personality and Social Psychology, 96 (5), pp. 1029-1046; Graham, J., Nosek, B.A., Haidt, J., Iyer, R., Koleva, S., Ditto, P.H., Mapping the moral domain (2011) Journal of Personality and Social Psychology, 101 (2), pp. 366-385; Hagendorff, T., The ethics of AI ethics: An evaluation of guidelines (2020) Minds and Machines, 30 (1), pp. 99-120; Haidt, J., The emotional dog and its rational tail: A social intuitionist approach to moral judgment (2001) Psychological Review, 108 (4), pp. 814-834; Haidt, J., (2012) The righteous mind: Why good people are divided by politics and religion, , Penguin; Haidt, J., Graham, J., When morality opposes justice: Conservatives have moral intuitions that liberals may not recognize (2007) Social Justice Research, 20 (1), pp. 98-116; Haidt, J., Joseph, C., Intuitive ethics: How innately prepared intuitions generate culturally variable virtues (2004) Daedalus, 133 (4), pp. 55-66; Henrich, J., Heine, S.J., Norenzayan, A., The weirdest people in the world? (2010) Behavioral and Brain Sciences, 33 (2-3), pp. 61-83; Hobbes, T., (1968) Leviathan. Penguin Books; Huang, M.H., Rust, R., Maksimovic, V., The feeling economy: Managing in the next generation of artificial intelligence (AI) (2019) California Management Review, 61 (4), pp. 43-65; Iyer, R., Koleva, S., Graham, J., Ditto, P., Haidt, J., Understanding libertarian morality: The psychological dispositions of self-identified libertarians (2012) PLoS ONE, 7 (8); Jobin, A., Ienca, M., Vayena, E., The global landscape of AI ethics guidelines (2019) Nature Machine Intelligence, 1 (9), pp. 389-399; Kaplan, A., Haenlein, M., Siri, Siri, in my hand: Whoâ€™s the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence (2019) Business Horizons, 62 (1), pp. 15-25; Kaplan, A., Haenlein, M., Rulers of the world, unite! The challenges and opportunities of artificial intelligence (2020) Business Horizons, 63 (1), pp. 37-50; Kellogg, K.C., Valentine, M.A., Christin, A., Algorithms at work: The new contested terrain of control (2020) Academy of Management Annals, 14 (1), pp. 366-410; Kohlberg, L., (1969) Stage and sequence: The cognitive-developmental approach to socialization, , Rand McNally; Kohlberg, L., From is to ought: How to commit the naturalistic fallacy and get away with it in the study of moral development (1971) Cognitive development and epistemology, pp. 151-235. , Mischel T, (ed), Academic Press; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Lee, I., Shin, Y.J., Machine learning for enterprises: Applications, algorithm selection, and challenges (2020) Business Horizons, 63 (2), pp. 157-170; Leicht-Deobald, U., Busch, T., Schank, C., Weibel, A., Schafheitle, S., Wildhaber, I., Kasper, G., The challenges of algorithm-based HR decision-making for personal integrity (2019) Journal of Business Ethics, 160 (2), pp. 377-392; Martin, K., Ethical implications and accountability of algorithms (2019) Journal of Business Ethics, 160 (4), pp. 835-850; Martin, K., Shilton, K., Smith, J., Business and the ethical implications of technology: Introduction to the symposium (2019) Journal of Business Ethics, 160 (2), pp. 307-317; Mitchell, M.S., Vogel, R.M., Folger, R., Third partiesâ€™ reactions to the abusive supervision of coworkers (2015) Journal of Applied Psychology, 100 (4), pp. 1040-1055; Mittelstadt, B., Principles alone cannot guarantee ethical AI (2019) Nature Machine Intelligence, 1 (11), pp. 501-507; Mooijman, M., Meindl, P., Oyserman, D., Monterosso, J., Dehghani, M., Doris, J.M., Graham, J., Resisting temptation for the good of the group: Binding moral values and the moralization of self-control (2018) Journal of Personality and Social Psychology, 115 (3), pp. 585-599; Morley, J., Floridi, L., Kinsey, L., Elhalal, A., From what to how: An initial review of publicly available AI ethics tools, methods and research to translate principles into practices (2020) Science and Engineering Ethics, 26 (4), pp. 2141-2168; Morse, L., Teodorescu, M.H.M., Awwad, Y., Kane, G.C., Do the ends justify the means? Variation in the distributive and procedural fairness of machine learning algorithms (2021) Journal of Business Ethics; Munoko, I., Brown-Liburd, H.L., Vasarhelyi, M., The ethical implications of using artificial intelligence in auditing (2020) Journal of Business Ethics, 167 (2), pp. 209-234; Murray, A., Rhymer, J., Sirmon, D.G., Humans and technology: Forms of conjoined agency in organizations (2021) Academy of Management Review, 46 (3), pp. 552-571; Neubert, M.J., MontaÃ±ez, G.D., Virtue as a framework for the design and use of artificial intelligence (2020) Business Horizons, 63 (2), pp. 195-204; Newman, D.T., Fast, N.J., Harmon, D.J., When eliminating bias isnâ€™t fair: Algorithmic reductionism and procedural justice in human resource decisions (2020) Organizational Behavior and Human Decision Processes, 160, pp. 149-167; Oâ€™Meara, S., Will China overtake the U.S. in artificial intelligence research? (2019) Scientific American, , https://www.scientificamerican.com/article/will-china-overtake-the-u-s-in-artificial-intelligence-research/, August 24, Retrieved from; Ouchi, W.G., The relationship between organizational structure and organizational control (1977) Administrative Science Quarterly, 22 (1), pp. 95-113; Porr, L., (2020) My GPT-3 Blog Got 26 Thousand Visitors in 2 Weeks, , https://liamp.substack.com/p/my-gpt-3-blog-got-26-thousand-visitors, Retrieved from; Raayoni, G., Gottlieb, S., Manor, Y., Pisha, G., Harris, Y., Mendlovic, U., Haviv, D., Kaminer, I., Generating conjectures on fundamental constants with the Ramanujan Machine (2021) Nature, 590 (7844), pp. 67-73; Rahim, M.A., Toward a theory of managing organizational conflict (2002) International Journal of Conflict Management, 13 (3), pp. 206-235; Rahwan, I., Society-in-the-loop: Programming the algorithmic social contract (2018) Ethics and Information Technology, 20 (1), pp. 5-14; Reynolds, S.J., Moral attentiveness: Who pays attention to the moral aspects of life? (2008) Journal of Applied Psychology, 93 (5), pp. 1027-1041; Seele, P., Dierksmeier, C., Hofstetter, R., Schultz, M.D., Mapping the ethicality of algorithmic pricing: A review of dynamic and personalized pricing (2021) Journal of Business Ethics, 170 (4), pp. 697-719; Shafer-Landau, R., (2014) The fundamentals of ethics, , 3, Oxford University Press; Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Hassabis, D., A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play (2018) Science, 362 (6419), pp. 1140-1144; Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Hassabis, D., Mastering the game of Go without human knowledge (2017) Nature, 550 (7676), pp. 354-359; Strubell, E., Ganesh, A., McCallum, A., Energy and policy considerations for deep learning in NLP (2019) Proceedings of the 57Th Annual Meeting of the Association for Computational Linguistics, pp. 3645-3650. , Florence, Italy; Vincent, J., OpenAIâ€™s latest breakthrough is astonishingly powerful, but still fighting its flaws (2020) The Verge, , https://www.theverge.com/21346343/gpt-3-explainer-openai-examples-errors-agi-potential, July 30; Wang, Y., Kosinski, M., Deep neural networks are more accurate than humans at detecting sexual orientation from facial images (2018) Journal of Personality and Social Psychology, 114 (2), pp. 246-257; Warren, D.E., Peytcheva, M., Gaspar, J.P., When ethical tones at the top conflict: Adapting priority rules to reconcile conflicting tones (2015) Business Ethics Quarterly, 25 (4), pp. 559-582; Waytz, A., Dungan, J., Young, L., The whistleblowerâ€™s dilemma and the fairnessâ€“loyalty tradeoff (2013) Journal of Experimental Social Psychology, 49 (6), pp. 1027-1033; Wright, S.A., Schultz, A.E., The rising tide of artificial intelligence and business automation: Developing an ethical framework (2018) Business Horizons, 61 (6), pp. 823-832; Yilmaz, O., Harma, M., BahÃ§ekapili, H.G., Cesur, S., Validation of the Moral Foundations Questionnaire in Turkey and its relation to cultural schemas of individualism and collectivism (2016) Personality and Individual Differences, 99, pp. 149-154; Zapko-Willmes, A., Schwartz, S.H., Richter, J., Kandler, C., Basic value orientations and moral foundations: Convergent or discriminant constructs? (2021) Journal of Research in Personality},
correspondence_address1={Telkamp, J.B.; Department of Management and Entrepreneurship, 2350 Gerdin Business Building, 2167 Union Drive, United States; email: jaketelk@iastate.edu},
editor={NA},
publisher={Springer Science and Business Media B.V.},
issn={01674544},
isbn={NA},
language={English},
abbrev_source_title={J. Bus. Ethics},
document_type={Article},
source={Scopus},
number={NA},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Bergmann202299,
type={ARTICLE},
author={Bergmann, L.T.},
title={Ethical Issues in Automated Drivingâ€”Opportunities, Dangers, andÂ Obligations},
journal={Studies in Computational Intelligence},
year={2022},
volume={980},
pages={99-121},
doi={10.1007/978-3-030-77726-5_5},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122349388&doi=10.1007%2f978-3-030-77726-5_5&partnerID=40&md5=77b8808582bba66458a6b6e289f37c40},
affiliation={Interdisciplinary Research Lab for Bioethics, Czech Academy of Sciences, Prague, Czech Republic},
abstract={Automated vehicles (AVs) not only face questions of technical feasibility but also of moral desirability. Traffic is one of the major sources of death and injury in modern societyâ€“human error causing about ninety percent of traffic accidents. Prima facie this yields a strong ethical obligation to further the development and adoption of AVs. However, moral desirability cannot be analyzed solely in terms of increased safety. Broad societal adoption of automated vehicles will entail many ethical issues. One cluster of ethical issues concerns the role of non-human entities occupying positions that usually are reserved for proper moral agents: how should AVs make decisions? And who could be held responsible for their choices? Another cluster of issues concerns the impact widespread adoption of AVs could have on society: which social groups would be negatively affected by the widespread adoption of AVs? Is society becoming too reliant on technology and which potential for abuse is entailed by this dependence? Should citizens remain free to drive vehicles themselves, though they make traffic less safe for everyone? In this paper, I advocate a cautionary position, mindful of the inevitability of technological progress and its great potential, attempting to highlight the obligation to steer this development towards an ethically acceptable trajectory. Â© 2022, Springer Nature Switzerland AG.},
author_keywords={Artificial intelligence;  Automated driving;  Autonomous driving;  AV;  Ethical issues;  Ethics;  Technology funding_textÂ 1={My thanks go out to Larissa Schlicht, Carmen Meixner, and Felix Blind for contributing to a predecessor of this text. Thanks to Achim Stephan, Gordon Pipa, and Jannik Zeiser for comments and feedback on the manuscript.},
keywords={NA},
references={Albright, J., Bell, A., Schneider, J., Nyce, C., (2015) Automobile Insurance in the Era of Autonomous Vehicles, , KPMG; Bergmann, L.T., Schlicht, L., Meixner, C., KÃ¶nig, P., Pipa, G., Boshammer, S., Stephan, A., Autonomous Vehicles Require Socio-Political Acceptance-An Empirical and Philosophical Perspective on the Problem of Moral Decision Making (2018) Frontiers in Behavioral Neuroscience, 12 (31); Blincoe, L., Miller, T.R., Zaloshnja, E., Lawrence, B.A., The economic and societal impact of motor vehicle crashes, 2010 (Revised) (2015) U.S. Nat. Highway Traffic Safety Admin. (NHTSA), Washington, DC, USA, Tech. Rep. DOT HS, 812, p. 013; (2017) Bericht Der Ethik-Kommission Automatisiertes Und Vernetztes Fahren, , Berlin; Boeglin, J., The costs of self-driving cars: Reconciling freedom and privacy with tort liability in autonomous vehicle regulation (2015) Yale JL & Tech, 17, p. 171; Arbeit, B.F., Anzahl der Berufskraftfahrer im deutschen StraÃŸengueter-verkehr (Stand: 30 (2013) Juni, p. 2013. , https://de.statista.com/statistik/daten/studie/294128/umfrage/anzahl-der-berufskraftfahrer-im-gueterverkehr/; Campa, R., Technological growth and unemployment: A global scenario analysis (2014) J. EVOLU-TION & TECH, 24, pp. 86-89; Coeckelbergh, M., The moral standing of machines: Towards a relational and non-cartesian moral hermeneutics (2014) Philosophy & Technology, 27 (1), pp. 61-77; Coeckelbergh, M., Responsibility and the moral phenomenology of using self-driving cars (2016) Applied Artificial Intelligence, 30 (8), pp. 748-757; Deutscher Taxi-und Mietwagenverband: Anzahl der Taxibetriebe in Deutschland im Zeitraum von 1960 bis 2018 (2020). https://de.statista.com/statistik/daten/studie/12498/umfrage/taxibetriebe-in-deutschland-seit-1960/; Dworkin, G., Paternalism (2016) Zalta EN (Ed) the Stanford Encyclopedia of Philosophy. Summer 2016 Edition; Fagnant, D.J., Kockelman, K., Preparing a nation for autonomous vehicles: Opportunities, barriers and policy recommendations for capitalizing on self-driven vehicles (2014) Transp. Rese., 20; Faulhaber, A.K., Dittmer, A., Blind, F., WÃ¤chter, M.A., Timm, S., SÃ¼tfeld, L.R., Stephan, P., KÃ¶nig, P., Human decisions in moral dilemmas are largely described by utilitarianism: Virtual car driving study provides guidelines for autonomous driving vehicles (2018) Sci. Eng. Ethics, 1 (20); Crime in the United Statesâ€”by Volume and Rate per 100,000 Inhabitants, 1998â€“2017 (2018). Retrieved from https://ucr.fbi.gov/crime-in-the-u.s/2017/crime-in-the-u.s.-2017/topic-pages/tables/table-1; Foot, P., The problem of abortion and the doctrine of double effect (1967) Oxford Review, 5, pp. 5-15; Floridi, L., Enveloping the World for AI (2011) The Philosophersâ€™ Magazine, (54), pp. 20-21; Friedrich, B., The Effect of Autonomous Vehicles on Traffic (2016) Autonomous Driving, , Maurer, M., Gerdes, J., Lenz, B., Winner, H. (eds.) , Springer, Berlin, Heidelberg; Glancy, D.J., Privacy in autonomous vehicles (2012) Santa Clara Law Review, 52, p. 1171; (2016) Google Self-Driving Car Project Monthly Report, , February; Greenberg, A., Hackers Remotely Kill a Jeep on the Highway. Wired (2015) Retrived From, , https://www.wired.com/2015/07/hackers-remotely-kill-jeep-highway/; Greene, J.D.: Moral Tribes: motion, Reason and the Gap between us and them. Penguin Books, London, UK (2014); GrÃ¼ne-Yanoff, T., Old wine in new casks: Libertarian paternalism still violates liberal principles (2012) Social Choice and Welfare, 38 (4), pp. 635-645; Grunwald, A., Societal risk constellations for autonomous driving. Analysis, historical context and assessment (2016) Autonomous Driving, pp. 641-663. , , pp. , Springer, Berlin; Grush, L., Google engineer apologizes after Photos app tags two black people as gorillas (2015) Theverge.Com, , https://www.theverge.com/2015/7/1/8880363/; Hevelke, A., Nida-RÃ¼melin, J., Selbstfahrende Autos und Trolley-Probleme: Zum Aufrechnen von Menschenleben im Falle unausweichlicher UnfÃ¤lle (2014) Jahrbuch fÃ¼r Wissenschaft Und Ethik, 19 (1), pp. 5-24; Hevelke, A., Nida-RÃ¼melin, J., Ethische Fragen zum Verhalten selbstfahrender Autos (2015) Zeitschrift fÃ¼r Philosophische Forschung, 69 (2), pp. 217-224; Hevelke, A., Nida-RÃ¼melin, J., Responsibility for crashes of autonomous vehicles: An ethical analysis (2015) Sci. Eng. Ethics, 21 (3), pp. 619-630; Jamjoom, M., Saudi cleric warns driving could damage womenâ€™s ovaries (2013) Cnn.Com., , http://edition.cnn.com/2013/09/29/world/meast/saudi-arabia-women-driving-cleric/; Kraftfahrt-Bundesamt. Anzahl der Kraftomnibusse in Deutschland von 1970 bis 2016 (2016); Lenz, B., Nobis, C., KÃ¶hler, K., Mehlin, M., Follmer, R., Gruschwitz, D., Jesske, B., Quandt, S., (2010) MobilitÃ¤t in Deutschland 2008; Lewis, K., (2016) Saudi Arabiaâ€™s Top Islamic Cleric Says Women Would Be â€˜exposed to evilâ€™ If Allowed to Drive, , http://www.independent.co.uk/news/world/middle-east/saudi-arabia-women-driving-~islamic-grand-mufti-sheikh-exposed-to-evil-rights-a6978621.html, independent.co.uk; Liu, Y., Singh, S., Subramanian, R., Motor vehicle traffic crashes as a leading cause of death in the United States, 2010 and 2011 (No (2015) DOT HS, 812, p. 203; Litman, T., (2017) Autonomous Vehicle Implementation Predictionss. Victoria Transport Policy Institute, , Victoria, Canada; Lord, S., Despres, C., Ramadier, T., When mobility makes sense: A qualitative and longitudinal study of the daily mobility of the elderly (2011) Journal of Environmental Psychology, 31 (1), pp. 52-61; Lowy, J., (2016) Will Robot Cars Drive Traffic Congestion off a Cliff?, , https://www.pbs.org/newshour/nation/will-robot-cars-drive-traffic-congestion-off-a-cliff; Marchant, G.E., Lindor, R.A., The coming collision between autonomous vehicles and the liability system. Santa Clara L (2012) Rev., 52, p. 1321; Deaths by Cause, Age, Sex, by Country and by Region (2018), 2016. , Geneva, World Health Organization; NHTSA: National motor vehicle crash causation survey: Report to congress (2008) National Highway Traffic Safety Administration Technical Report DOT HS, 811, p. 059; (2016) NHTSA: 2013 Motor Vehicle Crashes: Overview, , https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812101; NHTSA: 2015 motor vehicle crashes: overview (2016). Traffic safety facts research note, 2016:1â€“9. https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/81231; Rachels, J., Active and passive euthanasia, 292 new eng (1975) J. MED, 78, pp. 78-86; Rhue, L., Emotion-reading tech fails the racial bias test (2019) Theconversation.Com, , https://theconversation.com/emotion-reading-tech-fails-the-racial-bias-test-108404; Savage, I., Comparing the fatality risks in united states transportation across modes and over time (2013) Research in Transportation Economics, 43 (1), pp. 9-22; Statistisches Bundesamt (2017). Todesursachen in Deutschland 2015. Fachserie 12 Reihe 4. https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Gesundheit/Todesursachen/Publikationen/Downloads-Todesursachen/todesursachen-2120400157004. pdf?__blob=publicationFile&v=5. Accessed 30 May 2019; SÃ¼tfeld, L.R., Gast, R., KÃ¶nig, P., Pipa, G., Using virtual reality to assess ethical decisions in road traffic scenarios: Applicability of value-of-life-based models and influences of time pressure (2017) Frontiers in Behavioral Neuroscience, 11 (122); Statistisches Bundesamt (2019). UnfÃ¤lle und verunglÃ¼ckte im straÃŸenverkehr. https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Verkehrsunfaelle/Tabellen/unfaelle-verunglueckte.html. Accessed 30 May 2019; Tacken, M., Mobility of the elderly in time and space in the Netherlands: An analysis of the dutch national travel survey (1998) Transportation, 379, p. 393; The Impact of Racial Bias in Facial Recognition Software (2018). medium.com. https://medium.com/@ODSC/the-impact-of-racial-bias-in-facial-recognition-software-36f37113604c; Thomson, J.J., Killing, letting die, and the trolley problem (1976) The Monist, 59 (2), pp. 204-217; Thomson, J.J., Turning the trolley (2008) Philosophy & Public Affairs, 36 (4), pp. 359-374; van Boom, W.H., Ogus, A., Introducing, defining and balancing autonomy v (2010) Paternalism. Erasmus L. Rev., 3, p. 1; Vanderbilt, T., (2008) Traffic: Why We Drive the Way We Do (And What It Says about Us), , Allen Lane, London; Vorndran, I., Unfallstatistik-Verkehrsmittel im Risikovergleich (2010) Statistisches Bundesamt-Wirtschaft Und Statistik, 12, pp. 1083-1088; Vorndran, I., Unfallentwicklung auf deutschen StraÃŸen,: Statistisches Bundesamt (2011) Wirtschaft Und Statistik, 2012, pp. 583-595; (2013) Global Study on Homicide, p. 2013. , Vienna: UNODC; Wilde, G.J., Risk homeostasis theory: An overview (1998) Injury Prevention, 4 (2), pp. 89-91; Williams, B.A., Brooks, C.F., Shmargad, Y., How algorithms discriminate based on data they lack: Challenges, solutions, and policy implications (2018) Journal of Information Policy, 8, pp. 78-115; Winkle, T., Safety benefits of automated vehicles: Extended findings from accident research for development, validation and testing (2016) Autonomous Driving, pp. 335-364. , , pp , Springer, Berlin; Xu, J., Murphy, S.L., Kochanek, K.D., Bastian, B.A., (2016) Deaths: Final Data for 2013, , https://www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64_02.pdf},
correspondence_address1={Bergmann, L.T.; Interdisciplinary Research Lab for Bioethics, Czech Republic; email: lasse.bergmann@uos.de},
editor={NA},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={1860949X},
isbn={NA},
language={English},
abbrev_source_title={Stud. Comput. Intell.},
document_type={Book Chapter},
source={Scopus},
number={NA},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Formosa2021595,
type={ARTICLE},
author={Formosa, P.},
title={Robot Autonomy vs. Human Autonomy: Social Robots, Artificial Intelligence (AI), and the Nature of Autonomy},
journal={Minds and Machines},
year={2021},
volume={31},
pages={595-616},
doi={10.1007/s11023-021-09579-2},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117768690&doi=10.1007%2fs11023-021-09579-2&partnerID=40&md5=b49d3e648f5e265cafd2230daa175eab},
affiliation={Department of Philosophy & Centre for Agency, Values and Ethics, Macquarie University, North Ryde, Australia},
abstract={Social robots are robots that can interact socially with humans. As social robots and the artificial intelligenceÂ (AI) that powers them becomes more advanced, they will likely take on more social and work roles. This has many important ethical implications. In this paper, we focus on one of the most central of these, the impacts that social robots can have on human autonomy. We argue that, due to their physical presence and social capacities, there is a strong potential for social robots to enhance human autonomy as well as several ways they can inhibit and disrespect it. We argue that social robots could improve human autonomy by helping us to achieve more valuable ends, make more authentic choices, and improve our autonomy competencies. We also argue that social robots have the potential to harm human autonomy by instead leading us to achieve fewer valuable ends ourselves, make less authentic choices, decrease our autonomy competencies, make our autonomy more vulnerable, and disrespect our autonomy. Whether the impacts of social robots on human autonomy are positive or negative overall will depend on the design, regulation, and use we make of social robots in the future. Â© 2021, The Author(s).},
author_keywords={Artificial intelligence (AI);  Artificial moral agents;  Autonomy;  Machine ethics;  Respect;  Social robots},
keywords={Autonomous agents;  Economic; social effects;  Intelligent robots;  Machine design, Artificial intelligence;  Artificial moral agent;  Autonomy;  Ethical implications;  Machine ethic;  Moral agents;  Power;  Respect;  Robot autonomy;  Social robots, Philosophical aspects},
references={Anderson, J., Christman, J., Anderson, J., (2005) Autonomy and the challenges to liberalism, , Cambridge University Press; Asaro, P., What should we want from a robot ethic? (2006) International Review of Information Ethics, 6, pp. 9-16; Bankins, S., Formosa, P., When AI meets PC: Exploring the implications of workplace social robots and a human-robot psychological contract (2020) European Journal of Work and Organizational Psychology, 29 (2), pp. 215-229; Beauchamp, T.L., Childress, J.F., (2001) Principles of biomedical ethics, , Oxford University Press; Beauchamp, T.L., DeGrazia, D., Principles and principlism (2004) Handbook of bioethics, pp. 55-74. , Khushf G, (ed), Springer; Begon, J., What are adaptive preferences? (2015) Journal of Applied Philosophy, 32 (3), pp. 241-257; Belpaeme, T., Kennedy, J., Ramachandran, A., Scassellati, B., Tanaka, F., Social robots for education: A review (2018) Science Robotics; Benson, P., Autonomy and oppressive socialization (1991) Social Theory and Practice, XVI, 1 (3), pp. 385-408; Benson, P., Free agency and self-worth (1994) Journal of Philosophy, 91 (12), pp. 650-658; Borenstein, J., Arkin, R., Robotic nudges: The ethics of engineering a more socially just human being (2016) Science and Engineering Ethics, 22 (1), pp. 31-46; Bostrom, N., (2014) Superintelligence, , Oxford University Press; Breazeal, C., Toward sociable robots (2003) Robotics and Autonomous Systems, 42, pp. 167-175; Christman, J., Relational autonomy, liberal individualism and the social constitution of selves (2004) Philosophical Studies, 117, pp. 143-164; Christman, J., (2009) The politics of persons: Individual autonomy and socio-historical selves, , Cambridge University Press; Cohen, S., Nudging and informed consent (2013) The American Journal of Bioethics, 13 (6), pp. 3-11; Darling, K., Extending legal protection to social robots (2016) Robot law, , Calo R, Froomkin A, Kerr I, (eds), Edward Elgar; Darling, K., Whoâ€™s Johnny?â€™ anthropomorphic framing in human-robot interaction, integration, and policy (2018) Robot ethics 2.0 (p. 22), , Lin P, Bekey G, Abney K, Jenkins R, (eds), Oxford University Press; Darwall, S., The value of autonomy and autonomy of the will (2006) Ethics, 116, pp. 263-284; Epley, N., Waytz, A., Cacioppo, J.T., On seeing human: A three-factor theory of anthropomorphism (2007) Psychological Review, 114 (4), pp. 864-886; Etzioni, A., Etzioni, O., AI assisted ethics (2016) Ethics and Information Technology, 18 (2), pp. 149-156; Evans, J.S.B.T., Dual-processing accounts of reasoning, judgement, and social cognition (2008) Annual Review of Psychology, 2008 (59), pp. 255-278; Ferreira, M.I.A., Sequeira, J.S., Tokhi, M.O., Kadar, E.E., Virk, G.S., (2017) A World with Robots: International Conference on Robot Ethics: ICRE, p. 2015. , Eds, Springer; Fink, J., Anthropomorphism and human likeness in the design of robots and humanâ€“robot interaction (2012) Social robotics (Vol. 7621, pp. 199â€“208), , Ge SS, Khatib O, Cabibihan J-J, Simmons R, Williams M-A, (eds), Springer; Floridi, L., Cowls, J., A Unified Framework of Five Principles for AI in Society (2019) Harvard Data Science Review; Floridi, L., AI4Peopleâ€”An ethical framework for a good AI society (2018) Minds and Machines, 28 (4), pp. 689-707; Formosa, P., Kantâ€™s conception of personal autonomy (2013) Journal of Social Philosophy, 44 (3), pp. 193-212; Formosa, P., (2017) Kantian ethics, , Cambridge University Press; Formosa, P., Ryan, M., Making moral machines (2020) AI & Society; Fosch-Villaronga, E., Lutz, C., TamÃ²-Larrieux, A., Gathering expert opinions for social robotsâ€™ ethical, legal, and societal concerns (2020) International Journal of Social Robotics, 12 (2), pp. 441-458; Fossa, F., Artificial moral agents: Moral mentors or sensible tools? (2018) Ethics and Information Technology, 20 (2), pp. 1-12; Frankfurt, H.G., Freedom of the will and the concept of a person (1971) The Journal of Philosophy, 68 (1), pp. 5-20; Friedman, M., Autonomy and the split-level self (1986) Southern Journal of Philosophy, 24 (1), pp. 19-35; Gambino, A., Fox, J., Ratan, R., Building a stronger CASA: Extending the computers are social actors paradigm (2020) Human-Machine Communication, 1, pp. 71-86; Gehman, S., RealToxicityPrompts: Evaluating neural toxic degeneration in language models (2020) Findings of the Association for Computational Linguistics. Association for Computational Linguistics.; Goddard, K., Roudsari, A., Wyatt, J., Automation bias (2012) Journal of the American Medical Informatics Association, 19 (1), pp. 121-127; Gunkel, D.J., Mind the gap: Responsible robotics and the problem of responsibility (2020) Ethics and Information Technology, 22 (4), pp. 307-320; Gurkaynak, G., Yilmaz, I., Haksever, G., Stifling artificial intelligence (2016) Computer Law & Security Review, 32 (5), pp. 749-758; Hagendorff, T., The ethics of Ai ethics: An evaluation of guidelines (2020) Minds and Machines; Hansen, P., Jespersen, A., Nudge and the manipulation of choice (2013) European Journal of Risk Regulation, 4 (1), pp. 3-28; Jeong, S., A Social Robot to Mitigate Stress, Anxiety, and Pain in Hospital Pediatric Care (2015) In Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction, pp. 103-104; Jobin, A., Lenca, M., Vayena, E., The global landscape of AI ethics guidelines (2019) Nature Machine Intelligence, 1 (9), pp. 389-399; Kahneman, D., (2011) Thinking, fast and slow, , Macmillan; Kanero, J., GeÃ§kin, V., OranÃ§, C., Mamus, E., KÃ¼ntay, A.C., GÃ¶ksun, T., Social robots for early language learning: Current evidence and future directions (2018) Child Development Perspectives, 12 (3), pp. 146-151; Kant, I., Groundwork of the metaphysics of morals (1996) Practical philosophy, pp. 37-108. , Gregor MJ, (ed), Cambridge University Press; Kittay, E.F., Human dependency and Rawlsian equality (1997) Feminists rethink the self, , Meyers D, (ed), Westview Press; Korsgaard, C.M., (1996) The sources of normativity, , Cambridge University Press; Li, J., The nature of the bots (2013) Proceedings of the 15Th ACM on International Conference on Multimodal Interactionâ€”ICMI â€™13, pp. 337-340; Lin, P., Abney, K., Bekey, G.A., (2012) Robot ethics. MIT Press, , Eds; Lucidi, P.B., Nardi, D., Companion Robots (2018) Proceedings of The, pp. 17-22. , AAAI/ACM Conference on AI, Ethics, and Society; Lutz, C., SchÃ¶ttler, M., Hoffmann, C., The privacy implications of social robots (2019) Mobile Media & Communication, 7 (3), pp. 412-434; Lyell, D., Coiera, E., Chen, J., Shah, P., Magrabi, F., How machine learning is embedded to support clinician decision making: An analysis of FDA-approved medical devices (2021) BMJ Health & Care Informatics, 28 (1); Mackenzie, C., Relational autonomy, normative authority and perfectionism (2008) Journal of Social Philosophy, 39 (4), pp. 512-533; Mackenzie, C., Stoljar, N., (2000) Relational Autonomy: Feminist Perspectives on Autonomy, Agency, and the Social Self, , Eds, Oxford University Press; Mackenzie, R., Sexbots: sex slaves, vulnerable others or perfect partners? (2018) International Journal of Technoethics, 9 (1), pp. 1-17; Meyers, D., Personal autonomy and the paradox of feminine socialization (1987) Journal of Philosophy, 84 (11), pp. 619-628; Molitorisz, S., Net privacy (2020) Newsouth Publishing; Moshkina, L., Park, S., Arkin, R.C., Lee, J.K., Jung, H., TAME: Time-varying affective response for humanoid robots (2011) International Journal of Social Robotics, 3 (3), pp. 207-221; Nash, K., Lea, J.M., Davies, T., Yogeeswaran, K., The bionic blues: Robot rejection lowers self-esteem (2018) Computers in Human Behavior, 78, pp. 59-63; Nissenbaum, H., A contextual approach to privacy online (2011) Daedalus, 140 (4), pp. 32-48; Oâ€™Neill, O., (2002) Autonomy and Trust in Bioethics, , Cambridge University Press; Pashevich, E., Can communication with social robots influence how children develop empathy? (2021) AI & SOCIETY; Petit, N., Law and regulation of artificial intelligence and robots (2017) SSRN Electronic Journal; Pirhonen, J., Melkas, H., Laitinen, A., Pekkarinen, S., Could robots strengthen the sense of autonomy of older people residing in assisted living facilities? (2020) Ethics and Information Technology, 22 (2), pp. 151-162; Pu, L., Moyle, W., Jones, C., Todorovic, M., The Effectiveness of social robots for older adults (2019) The Gerontologist, 59 (1), pp. e37-e51; Quigley, M., Nudging for health (2013) Medical Law Review, 21 (4), pp. 588-621; Rahwan, I., Society-in-the-loop (2018) Ethics and Information Technology, 20 (1), pp. 5-14; Raz, J., (1986) The morality of freedom, , Clarendon Press; Reeves, B., Nass, C.I., (1996) The media equation: How people treat computers, television, and new media like real people and places, , Cambridge University Press; Robbins, S., AI and the path to envelopment (2019) AI & SOCIETY; Rogers, W.A., Draper, H., Carter, S.M., Evaluation of artificial intelligence clinical applications: Detailed case analyses show value of healthcare ethics approach in identifying patient care issues (2021) Bioethics, 35 (7), pp. 623-633; Ryan, R.M., Rigby, C.S., Przybylski, A., The motivational pull of video games: A self-determination theory approach (2006) Motivation and Emotion, 30 (4), pp. 344-360; Ryan, R.M., Deci, E.L., Self-Determination Theory (2017) Guilford Publications; Schmidt, A.T., Engelen, B., The ethics of nudging (2020) Philosophy Compass; Schmitt, M.N., Thurnher, J.S., Out of the loopâ€: Autonomous weapon systems and the law of armed conflict (2013) Harvard National Security Journal, 4, pp. 231-281; Schneewind, J.B., (1998) The invention of autonomy, , Cambridge University Press; Scoccia, D., Paternalism and respect for autonomy (1990) Ethics, 100 (2), pp. 318-334; SeÃ¯ler, N.R., Craig, P., Empathetic technology (2016) Emotions and technology, emotions, technology, and design, pp. 55-81. , Tettegah S, Sharon S, (eds), Academic Press; Sharkey, A., Sharkey, N., Granny and the robots (2012) Ethics and Information Technology, 14 (1), pp. 27-40; Shea, M., Forty years of the four principles (2020) The Journal of Medicine and Philosophy, 45 (4-5), pp. 387-395; Sparrow, R., Can machines be people? (2012) Robot ethics, pp. 301-316. , Lin P, Abney K, Bekey G, (eds), MIT Press; Sparrow, R., Robots and respect: Assessing the case against autonomous weapon systems (2016) Ethics and International Affairs, 30 (1), pp. 93-116; Sparrow, R., Robots, rape, and representation (2017) International Journal of Social Robotics, 9 (4), pp. 465-477; Stoljar, N., Autonomy and the FEMINIST INTUITion (2000) Relational autonomy, , Mackenzie C, Stoljar N, (eds), Oxford University Press; Susser, D., Roessler, B., Nissenbaum, H., Technology, autonomy, and manipulation (2019) Internet Policy Review; Thaler, R.H., Sunstein, C.R., (2008) Nudge, , Yale University Press; Turkle, S., (2012) Alone together, , Basic Books; Turkle, S., Targgart, W., Kidd, C., Daste, O., Relational artifacts with children and elders (2006) Connection Science, 18 (4), pp. 347-361; Iâ€™d blush if I could: Closing gender divides in digital skills through education (2019) UNESCO, , https://unesdoc.unesco.org/ark:/48223/pf0000367416.page=1; Vallor, S., moral deskilling and upskilling in a new machine age (2015) Philosophy & Technology, 28 (1), pp. 107-124; van Wynsberghe, A., Robbins, S., Critiquing the reasons for making artificial moral agents (2019) Science and Engineering Ethics, 25, pp. 719-735; von Hippel, W., Trivers, R., The evolution and psychology of self-deception (2011) Behavioral and Brain Sciences, 34 (1), pp. 1-16; Wakefield, J., Fear detector exposes peopleâ€™s emotions (2018) BBC, , https://www.bbc.com/news/technology-43653649; Walker, M.J., Mackenzie, C., Neurotechnologies, Relational autonomy, and authenticity (2020) International Journal of Feminist Approaches to Bioethics, 13 (1), pp. 98-119; Walsh, T., Levy, N., Bell, G., Elliott, A., Maclaurin, J., Mareels, I., Wood, F., The Effective and ethical development of Artificial Intelligence (P. 250) (2019) ACOLA, p. 10; Wang, P., On defining artificial intelligence (2019) Journal of Artificial General Intelligence, 10 (2), pp. 1-37; Watson, G., Free agency (1975) Journal of Philosophy, 72 (8), pp. 205-220; Woiceshyn, L., Wang, Y., Nejat, G., Benhabib, B., Personalized clothing recommendation by a social robot (2017) IEEE International Symposium on Robotics and Intelligent Sensors (IRIS), 2017, pp. 179-185; Wolf, S., (1990) Freedom within reason, , Oxford University Press},
correspondence_address1={Formosa, P.; Department of Philosophy & Centre for Agency, Australia; email: Paul.Formosa@mq.edu.au},
editor={NA},
publisher={Springer Science and Business Media B.V.},
issn={09246495},
isbn={NA},
language={English},
abbrev_source_title={Minds Mach},
document_type={Article},
source={Scopus},
number={4},
art_number={NA},
funding_details={Macquarie UniversityMacquarie University funding_textÂ 1={Open Access funding provided by the Macquarie University Research Centre for Agency, Values and Ethics (CAVE).},
coden={MMACE},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Taddeo20211707,
type={ARTICLE},
author={Taddeo, M. and McNeish, D. and Blanchard, A. and Edgar, E.},
title={Ethical Principles for Artificial Intelligence in National Defence},
journal={Philosophy and Technology},
year={2021},
volume={34},
pages={1707-1729},
doi={10.1007/s13347-021-00482-3},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117036067&doi=10.1007%2fs13347-021-00482-3&partnerID=40&md5=3a31c1534d4933c0dcd4eca7558eff76},
affiliation={Oxford Internet Institute, University of Oxford, 1, St Giles, Oxford, OX1 3JS, United Kingdom; Alan Turing Institute, British Library, 96 Euston Rd, London, NW1 2DB, United Kingdom; Defence Science Technology Laboratory (Dstl), Salisbury, United Kingdom},
abstract={Defence agencies across the globe identify artificial intelligence (AI) as a key technology to maintain an edge over adversaries. As a result, efforts to develop or acquire AI capabilities for defence are growing on a global scale. Unfortunately, they remain unmatched by efforts to define ethical frameworks to guide the use of AI in the defence domain. This article provides one such framework. It identifies five principlesâ€”justified and overridable uses, just and transparent systems and processes, human moral responsibility, meaningful human control and reliable AI systemsâ€”and related recommendations to foster ethically sound uses of AI for national defence purposes. Â© 2021, The Author(s).},
author_keywords={Artificial intelligence;  Control;  Defence;  Digital ethics;  Ethical principles;  Fairness;  Just war theory;  Reliability;  Responsibility},
keywords={NA},
references={(2019) Acalvio. 2019, , https://www.acalvio.com/; Asaro, P., On banning autonomous weapon systems: Human rights, automation, and the dehumanization of lethal decision-making (2012) International Review of the Red Cross, 94 (886), pp. 687-709; Continuous Authentication Through Behavioral Biometrics. 2019 Behaviosec. 2019, , https://www.behaviosec.com/; Boardman, M., Butcher, F., An exploration of maintaining human control in AI enabled systems and the challenges of achieving it (2019) STO-MP-IST-178; Boulanin, V., PeldÃ¡n Carlsson, N., Goussacdavidson, D., (2020) Limits on Autonomy in Weapon Systems: Identifying Practical Elements of Human Control. Stockholm International Peace Research Institute and the International Committee of the Red Cross, , https://www.sipri.org/publications/2020/other-publications/limits-autonomy-weapon-systems-identifying-practical-elements-human-control-0; Brundage, M., Avin, J., Clark, H., Toner, P., Eckersley, B., Garfinkeldafoe, A., (2018) The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation., , http://arxiv.org/abs/1802.07228; Brunstetter, D., Braun, M., From Jus Ad Bellum to Jus Ad Vim: Recalibrating our understanding of the moral use of force (2013) Ethics & International Affairs, 27 (1), pp. 87-106; (2017) Business Wire, , https://www.businesswire.com/news/home/20170726005117/en/DarkLight-Offers-Kind-Artificial-Intelligence-Enhance-Cybersecurity; How AI Can Power a Stealthy New Breed of Malware. 2018 (2018) Security Intelligence (Blog), p. 8. , https://securityintelligence.com/deeplocker-how-ai-can-power-a-stealthy-new-breed-of-malware/; (2018) Data Ethics Framework., , https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework; (2020) AI Principles: Recommendations on the Ethical Use of Artificial Intelligence by the Department of Defense, , https://media.defense.gov/2019/Oct/31/2002204458/-1/-1/0/DIB_AI_PRINCIPLES_PRIMARY_DOCUMENT.PDF; AI principles: Recommendations on the ethical use of artificial intelligence by the Department of Defense - Supporting document (2020) Defence Innovation Board (DIB), , https://media.defense.gov/2019/Oct/31/2002204459/-1/-1/0/DIB_AI_PRINCIPLES_SUPPORTING_DOCUMENT.PDF; Docherty, B., Shaking the foundations: The human rights implications of killer robots (2014) Human Rights Watch, , https://www.hrw.org/report/2014/05/12/shaking-foundations/human-rights-implications-killer-robots; Ekelhof, M., Moving beyond semantics on autonomous weapons: Meaningful human control in operation (2019) Global Policy, 10 (3), pp. 343-348; Enemark, C., Drones over Pakistan: Secrecy, ethics, and counterinsurgency (2011) Asian Security, 7 (3), pp. 218-237; Floridi, L., The method of levels of abstraction (2008) Minds and Machines, 18 (3), pp. 303-329; Floridi, L., Mature information societiesâ€”A matter of expectations (2016) Philosophy & Technology, 29 (1), pp. 1-4; Floridi, L., Faultless responsibility: On the nature and allocation of moral responsibility for distributed moral actions (2016) Philosophical Transactions of the Royal Society a: Mathematical, Physical and Engineering Sciences, 374 (2083), p. 20160112; Floridi, L., Soft ethics and the governance of the digital (2018) Philosophy & Technology, 31 (1), pp. 1-8; Floridicowls, L., A unified framework of five principles for AI in society (2019) Harvard Data Science Review, , https://doi.org/10.1162/99608f92.8cd550d1; Floridi, L., Cowls, J., King, T.C., Taddeo, M., How to design AI for social good: Seven essential factors (2020) Science and Engineering Ethics, 26 (3), pp. 1771-1796; Floridi, L., Sanders, J.W., On the morality of artificial agents (2004) Minds and Machines, 14 (3), pp. 349-379; Fraga-Lamas, P.M., FernÃ¡ndez-CaramÃ©s, M., SuÃ¡rez-Albela, L., CastedogonzÃ¡lez-LÃ³pez, M., A review on Internet of things for defense and public safety (2016) Sensors (Basel, Switzerland), 16 (10). , https://doi.org/10.3390/s16101644; Gavaghan, C., Knott, J., Maclaurin, J., Zerilliliddicoat, J., Government use of artificial intelligence in New Zealand, final report on phase 1 of the Law Foundationâ€™s Artificial Intelligence and Law in New Zealand Project (2019) In. New Zealand Law Foundation: Wellington., , https://www.cs.otago.ac.nz/research/ai/AI-Law/NZLF%20report.pdf; (2017) Minimum requirements related to technical performance for IMT-2020 radio interface(s), 2017. , https://www.itu.int/pub/R-REP-M.2410-2017; (2017) Ethical Guidelines, , http://ai-elsi.org/wp-content/uploads/2017/05/JSAI-Ethical-Guidelines-1.pdf; Jobin, A., Ienca, M., Vayena, E., The global landscape of AI ethics guidelines (2019) Nature Machine Intelligence, 1 (9), pp. 389-399; Johnson, A.M., Axinn, S., The morality of autonomous robots (2013) Journal of Military Ethics, 12 (2), pp. 129-141; King, T.M., Arbon, D., Santiago, D., Adamo, W., Chinshanmugam, R., AI for testing today and tomorrow: Industry perspectives (2019) 2019 IEEE International Conference on Artificial Intelligence Testing (Aitest), pp. 81-88. , https://doi.org/10.1109/AITest.2019.000-3, Newark, CA, USA, IEEE; Kott, A., Swamiwest, B.J., (2017) The Internet of battle things., , http://arxiv.org/abs/1712.08980; Lysaght, R.J., Harriskelly, W., Artificial intelligence for command and control (1988) ANALYTICS INC WILLOW GROVE PA, , https://apps.dtic.mil/docs/citations/ADA229342; McMahan, J., (2013) Forward. in Who Should Die? The Ethics of Killing in War, , edited by Ryan Jenkins, Michael Robillard, and B J Strawser, ix-xiv. Oxford New York, NY: Oxford University Press; Mirsky, Y., Mahler, I., Shelefelovici, Y., CT-GAN: Malicious tampering of 3D medical imagery using deep learning (2019) Researchgate, , https://www.researchgate.net/publication/330357848_CT-GAN_Malicious_Tampering_of_3D_Medical_Imagery_using_Deep_Learning/figures?lo=1; MÃ¶kanderfloridi, J., (2021) Ethics-Based Auditing to Develop Trustworthy AI, , https://doi.org/10.1007/s11023-021-09557-8, Minds and Machines, February; Nagel, T., â€œWar and Massacre.â€ Philosophy and Public Affairs, 1972, 1 (Winter): 123-144 (1972) American Behavioral Scientist, 15 (6), p. 951. , https://doi.org/10.1177/000276427201500678; (2020) NATO 2030: United for a New Era. Brussels, , https://www.nato.int/nato_static_fl2014/assets/pdf/2020/12/pdf/201201-Reflection-Group-Final-Report-Uni.pdf; Oâ€™Connell, M.E., The American way of bombing: How legal and ethical norms change (2014) In, Edited by Matthew Evangelista and Henry Shue. Ithaca: Cornel University Press.; Rigakielragal, M., (2017) Adversarial Deep Learning against Intrusion Detection Classifiers. In, p. 14; Roberts, H., Cowls, J., Morley, M., Taddeo, V., Wangfloridi, L., The Chinese approach to artificial intelligence: An analysis of policy, ethics, and regulation (2020) AI & SOCIETY, June, , https://doi.org/10.1007/s00146-020-00992-2; Schubert, J., Brynielsson, M., Nilssonsvenmarck, P., (2018) Artificial Intelligence for Decision Support in Command and Control Systems, p. 15; Sharkey, A., Autonomous weapons systems, killer robots and human dignity (2019) Ethics and Information Technology, 21 (2), pp. 75-87; Sharkey, N., Saying â€œNo!â€ to lethal autonomous targeting (2010) Journal of Military Ethics, 9 (4), pp. 369-383; Sharkey, N., Killing made easy: From joysticks to politics (2012) Robot ethics: The ethical and social implications of robotics, pp. 111-128. , Lin P, Abney K, Bekey G, (eds), MIT Press; Sharkey, N.E., The evitability of autonomous robot warfare (2012) International Review of the Red Cross, 94 (886), pp. 787-799; Sparrow, R., Killer robots (2007) Journal of Applied Philosophy, 24 (1), pp. 62-77; Sparrow, R., Robots and respect: Assessing the case against autonomous weapon systems (2016) Ethics & International Affairs, 30 (1), pp. 93-116; Taddeo, M., Information warfare: A philosophical perspective (2012) Philosophy and Technology, 25 (1), pp. 105-120; Taddeo, M., An analysis for a just cyber warfare (2012) Fourth International Conference of Cyber Conflict. NATO CCD COE and IEEE Publication; Taddeo, M., Cyber security and individual rights, striking the right balance (2013) Philosophy & Technology, 26 (4), pp. 353-356; Taddeo, M., (2014) Just Information Warfare, pp. 1-12. , https://doi.org/10.1007/s11245-014-9245-8, Topoi, April; Taddeo, M., The struggle between liberties and authorities in the information age (2014) Science and Engineering Ethics, pp. 1-14. , https://doi.org/10.1007/s11948-014-9586-0; Taddeo, M., The limits of deterrence theory in cyberspace (2017) Philosophy & Technology; Taddeo, M., The challenges of cyber deterrence (2019) The 2018 Yearbook of the Digital Ethics Lab, pp. 85-103. , https://doi.org/10.1007/978-3-030-17152-0_7, Carl Ã–hman, David Watson, Cham, Springer International Publishing; Taddeo, M., Three ethical challenges of applications of artificial intelligence in cybersecurity (2019) Minds and Machines, 29 (2), pp. 187-191; Taddeo, M., Floridi, L., Regulate artificial intelligence to avert cyber arms race (2018) Nature, 556 (7701), pp. 296-298; Taddeo, M., McCutcheon, T., Floridi, L., Trusting artificial intelligence in cybersecurity is a double-edged sword (2019) Nature Machine Intelligence, 1 (12), pp. 557-560; Tamburrini, G., On banning autonomous weapons systems: From deontological to wide consequentialist reasons (2016) Autonomous weapons systems: Law, ethics, policy, pp. 122-142. , Nehal B, Beck S, GeiÎ² R, Liu H-Y, KreÎ² C, (eds), Cambridge University Press; (2018), https://www.gov.uk/government/publications/international-humanitarian-law-and-the-uk-government/uk-and-international-humanitarian-law-2018; (2017) Robotic and Autonomous Systems Strategy, , https://www.tradoc.army.mil/Portals/14/Documents/RAS_Strategy.pdf; Yang, G.-Z., Bellingham, P.E., Dupont, P., Fischer, L., Floridi, R., Fulljacobstein, N., The grand challenges of Science Robotics (2018) Science Robotics, 3 (14), p. eaar7650. , https://doi.org/10.1126/scirobotics.aar7650; Zhuge, J., Holz, X., Han, C., Songzou, W., Collecting autonomous spreading malware using high-interaction honeypots (2007) In Information and Communications Security, Edited by Sihan Qing, Hideki Imai, and Guilin Wang, pp. 438-451. , Lecture Notes in Computer Science. Springer Berlin Heidelberg},
correspondence_address1={Taddeo, M.; Oxford Internet Institute, 1, St Giles, United Kingdom; email: mariarosaria.taddeo@oii.ox.ac.uk},
editor={NA},
publisher={Springer Science and Business Media B.V.},
issn={22105433},
isbn={NA},
language={English},
abbrev_source_title={Philos. Technol.},
document_type={Article},
source={Scopus},
number={4},
art_number={NA},
funding_details={R-DST-TFS/D026 funding_textÂ 1={Mariarosaria Taddeo and Alexander Blanchardâ€™s work on this article has been funded by the Dstl Ethics Fellowship held at the Alan Turing Institute. The research underpinning this work was funded by the UK Defence Chief Scientific Advisorâ€™s Science and Technology Portfolio, through the Dstl Autonomy Programme, grant number R-DST-TFS/D026. This paper is an overview of UK Ministry of Defence (MOD)â€“sponsored research and is released for informational purposes only. The contents of this paper should not be interpreted as representing the views of the UK MOD, nor should it be assumed that they reflect any current or future UK MOD policy. The information contained in this paper cannot supersede any statutory or contractual requirements or liabilities and is offered without prejudice or commitment.},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Mabaso20217,
type={ARTICLE},
author={Mabaso, B.A.},
title={Artificial Moral Agents Within an Ethos of AI4SG},
journal={Philosophy and Technology},
year={2021},
volume={34},
pages={7-21},
doi={10.1007/s13347-020-00400-z},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083990530&doi=10.1007%2fs13347-020-00400-z&partnerID=40&md5=7ce3954aa27b1868e5b46ea7ed511b39},
affiliation={University of Pretoria, Pretoria, South Africa},
abstract={As artificial intelligence (AI) continues to proliferate into every area of modern life, there is no doubt that society has to think deeply about the potential impact, whether negative or positive, that it will have. Whilst scholars recognise that AI can usher in a new era of personal, social and economic prosperity, they also warn of the potential for it to be misused towards the detriment of society. Deliberate strategies are therefore required to ensure that AI can be safely integrated into society in a manner that would maximise the good for as many people as possible, whilst minimising the bad. One of the most urgent societal expectations of artificial agents is the need for them to behave in a manner that is morally relevant, i.e. to become artificial moral agents (AMAs). In this article, I will argue that exemplarism, an ethical theory based on virtue ethics, can be employed in the building of computationally rational AMAs with weak machine ethics. I further argue that three features of exemplarism, namely grounding in moral exemplars, meeting community expectations and practical simplicity, are crucial to its uniqueness and suitability for application in building AMAs that fit the ethos of AI4SG. Â© 2020, Springer Nature B.V.},
author_keywords={AI4SG;  Artificial moral agency;  Exemplarism;  Machine ethics},
keywords={NA},
references={Abel, D., Macglashan, J., Littman, M.L., Reinforcement learning as a framework for ethical decision making (2016) In AAAI Workshop - Technical Report, pp. 54-61. , www.aaai.org; Abney, K., Robotics, ethical theory, and metaethics: A guide for the perplexed (2012) Robot Ethics, the Ethical and Social Implications of Robotics, 3, pp. 35-52. , Lin, P, Abney, K, & Bekey, G, The MIT Press, chap; Aghion, P.J., Benjamin, F., Jones, C.I., (2017) Artificial Intelligence and Economic Growth; Allen, C., Wallach, W., Moral machines: Contradiction in terms, or abdication of human responsibility? (2011) Robot Ethics, p. 4. , In Lin, P., Abney, K., & Bekey, G.A. (Eds.), The MIT Press, chap; Alzahrani, H., Artificial intelligence: uses and misuses (2016) Global Journal of Computer Science and Technology, 16 (1s); Anderson, M., Anderson, S.L., Machine ethics: creating an ethical intelligent agent (2007) AI Magazine, 28 (4), p. 15. , https://doi.org/10.1609/aimag.v28i4.2065 http://www.aaai.org/ojs/index.php/aimagazine/article/view/2065; Anderson, S.L., Anderson, M., A prima facie duty approach to machine ethics and its application to elder care (2011) In Workshops at the Twenty-Fffth AAAI Conference on Artificial Intelligence; Annas, J., (2011) Intelligent virtue, , Oxford University Press, Oxford; Argall, B.D., Chernova, S., Veloso, M., Browning, B., A survey of robot learning from demonstration (2009) Robotics and autonomous systems, 57 (5), pp. 469-483; Baum, S.D., Social choice ethics in artificial intelligence (2017) AI and Society (October), pp. 1-12. , https://doi.org/10.1007/s00146-017-0760-1; Brys, T., Harutyunyan, A., Suay, H.B., Chernova, S., Taylor, M.E., NowÃ©, A., Reinforcement learning from demonstration through shaping (2015) In Twenty-Fourth International Joint Conference on Artificial Intelligence; Churchland, P.S., The neurobiological platform for moral values (2014) Behaviour, 151 (2-3), pp. 283-296; Cloos, C., The utilibot project: An autonomous mobile robot based on utilitarianism (2005) Machine Ethics: Papers from the 2005 AAAI Fall Symposium, pp. 38-45. , http://philpapers.org/archive/CLOTUP.2.pdf; Conitzer, V., Sinnott-Armstrong, W., Borg, J.S., Deng, Y., Kramer, M., Moral decision making frameworks for artificial intelligence (2017) In ISAIM, pp. 4831-4835. , www.aaai.org; Cowls, J., Floridi, L., Prolegomena to a white paper on an ethical framework for a good AI society (2018) SSRN Electronic Journal, , https://doi.org/10.2139/ssrn.3198732.https://ssrn.com/abstract=3198732; Dameski, A., A comprehensive ethical framework for AI entities: Foundations (2018) International Conference on Artificial General Intelligence, pp. 42-51. , https://doi.org/10.1007/978-3-319-97676-1, IklÃ©, M., Franz, A., Rzepka, R., & Goertzel, B, July, Springer; Dignum, V., Responsible autonomy (2017) Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17)., , https://doi.org/10.24963/ijcai.2017/655, arXiv: 1706.02513; Duan, Y., Andrychowicz, M., Stadie, B., Jonathan, H., Open, A.I., Schneider, J., Sutskever, I., Zaremba, W., Advances in Neural Information Processing Systems (2017) One-Shot Imitation Learning, 30, pp. 1087-1098. , http://papers.nips.cc/paper/6709-one-shot-imitation-learning.pdf, I. Guyon, U.V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, & R. Garnett, Curran Associates, Inc; Floridi, L., Sanders, J.W., On the morality of artificial agents (2004) Minds and machines, 14 (3), pp. 349-379; Gershman, S.J., Horvitz, E.J., Tenenbaum, J.B., Computational rationality: a converging paradigm for intelligence in brains, minds, and machines (2015) Science, 349 (6245), pp. 273-278; Gips, J., Towards the ethical robot (1995) Android Epistemology, pp. 243-252. , MIT Press; Himma, K.E., Artificial agency, consciousness, and the criteria for moral agency: what properties must an artificial agent have to be a moral agent? (2008) Ethics and Information Technology, 11 (1), pp. 19-29; Horvitz, E.J., Reasoning about beliefs and actions under computational resource constraints (1987) Proceedings of the Third Workshop on Uncertainty in Artificial Intelligence, AAAI and Association for Uncertainty in Artificial Intelligence, July., pp. 429-444. , http://erichorvitz.com/u87.htm; Howard, D., Muntean, I., A minimalist model of the artificial autonomous moral agent (AAMA) (2016) AAAI Spring Symposium Series.; Hursthouse, R., Pettigrove, G., Virtue ethics (2018) The Stanford Encyclopedia of Philosophy, Winter 2018 Edn, , Zalta, E N, Metaphysics Research Lab, Stanford University; Johnson, D.G., Computer systems: moral entities but not moral agents (2006) Machine Ethics, pp. 168-183; Kiela, D., Deep embodiment: Grounding semantics in perceptual modalities (2017) Tech. Rep., , http://www.cl.cam.ac.uk/, University of Cambridge, Computer Laboratory; Kuipers, B., (2016) Human-Like Morality and Ethics for Robots; van Lent, M., Laird, J.E., Learning procedural knowledge through observation (2001) K-CAP, pp. 179-186. , to appear in print; Levinson, M., Fay, J., (2016) Dilemmas of educational ethics: cases and commentaries, , Harvard Education Press, Cambridge; Liao, S.M., The basis of human moral status (2010) Journal of Moral Philosophy, 7 (2), pp. 1-31; Mayo, M.J., Symbol grounding and its implications for artificial intelligence (2003) In Proceedings of the 26Th Australasian Computer Science Conference, 16, pp. 55-60. , http://portal.acm.org/citation.cfm?id=783106.783113&type=series, Darlinghurst: Australian Computer Society, Inc; Miller, F.D., Aristotle on rationality in action (1984) The Review of Metaphysics, 37 (3), pp. 499-520. , https://www.jstor.org/stable/20128047; Moor, J.H., The nature, importance, and difficulty of machine ethics (2006) IEEE intelligent systems, 21 (4), pp. 18-21; Parthemore, J., Whitby, B., What makes any agent a moral agent? Reflections on machine consciousness and moral agency (2013) International Journal of Machine Consciousness, 5 (2), pp. 105-129. , https://pdfs.semanticscholar.org/3ff2/49fe3c8b3a2c94ae762b76b2dd0203f1f789.pdf; Parthemore, J., Whitby, B., Moral agency, moral responsibility, and artifacts: what existing artifacts fail to achieve (and why), and why they, nevertheless, can (and do!) make moral claims upon us (2014) International Journal of Machine Consciousness, 6 (2), pp. 141-161; Peterson, M., An introduction to decision theory (2009) Cambridge Introductions to Philosophy, , Cambridge, Cambridge University Press, to appear in print; Pontier, M., Hoorn, J., (2012) Toward Machines that Behave Ethically Better than Humans Do, p. 34. , In Proceedings of the annual meeting of the cognitive science society, Vol; Prasad, M., Social choice and the value alignment problem (2018) Artificial Intelligence Safety and Security, pp. 291-314. , London, Chapman and Hall/CRC; Rottschaefer, W.A., Naturalizing ethics: the biology and psychology of moral agency (2000) ZygonÂ®;, 35 (2), pp. 253-286; Russell, S.J., Norvig, P., (2009) Artifical intelligence: a modern approach, , 3rd edn., Prentice Hall, Upper Saddle River: https://doi.org/10.1017/S0269888900007724. arXiv: http://arxiv.org/abs/1707.02286, arXiv: http://arxiv.org/abs/1011.1669v3; Scheutz, M., Malle, B.F., Moral robots (2017) The Routledge Handbook of Neuroethics, p. 24. , https://doi.org/10.4324/9781315708652.ch24, Johnson, L.S.M., & Rommelfanger, K.S. (Eds, Routledge, chap; Simon, H.A., A behavioral model of rational choice (1955) The Quarterly Journal of Economics, 69 (1), pp. 99-118; Slote, M., Agent-based virtue ethics (1995) Midwest Studies in Philosophy, 20 (1), pp. 83-101; Sullins, J.P., When is a robot a moral agent (2006) IRIE: International Review of Information Ethics, , http://sonoma-dspace.calstate.edu/handle/10211.1/427; Szutta, N., Exemplarist moral theoryâ€“some pros and cons (2019) Journal of Moral Education, 48 (3), pp. 280-290; Torrance, S., Ethics and consciousness in artificial agents (2008) AI and Society, 22 (4), pp. 495-521; Vamplew, P., Dazeley, R., Foale, C., Firmin, S., Mummery, J., Human-aligned artificial intelligence is a multiobjective problem (2018) Ethics and Information Technology, 20 (1), pp. 27-40; Wallach, W., Franklin, S., Allen, C., A conceptual and computational model of moral decision making in human and artificial agents (2010) Topics in Cognitive Science, 2 (3), pp. 454-485; Zagzebski, L., Exemplarist virtue theory (2010) Metaphilosophy, 41 (1-2), pp. 41-57},
correspondence_address1={Mabaso, B.A.; University of PretoriaSouth Africa; email: bamabaso@gmail.com},
editor={NA},
publisher={Springer Science and Business Media B.V.},
issn={22105433},
isbn={NA},
language={English},
abbrev_source_title={Philos. Technol.},
document_type={Article},
source={Scopus},
number={NA},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Smith2021483,
type={ARTICLE},
author={Smith, N. and Vickers, D.},
title={Statistically responsible artificial intelligences},
journal={Ethics and Information Technology},
year={2021},
volume={23},
pages={483-493},
doi={10.1007/s10676-021-09591-1},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104116727&doi=10.1007%2fs10676-021-09591-1&partnerID=40&md5=9bc5bf9e4fafea94b5ecf3a2e800dba6},
affiliation={Department of Philosophy, 85 Humanities Instructional Building, University of California, Irvine, Irvine, CA  92697-4555, United States},
abstract={As artificial intelligence (AI) becomes ubiquitous, it will be increasingly involved in novel, morally significant situations. Thus, understanding what it means for a machine to be morally responsible is important for machine ethics. Any method for ascribing moral responsibility to AI must be intelligible and intuitive to the humans who interact with it. We argue that the appropriate approach is to determine how AIs might fare on a standard account of human moral responsibility: a Strawsonian account. We make no claim that our Strawsonian approach is either the only one worthy of consideration or the obviously correct approach, but we think it is preferable to trying to marry fundamentally different ideas of moral responsibility (i.e. one for AI, one for humans) into a single cohesive account. Under a Strawsonian framework, people are morally responsible when they are appropriately subject to a particular set of attitudesâ€”reactive attitudesâ€”and determine under what conditions it might be appropriate to subject machines to this same set of attitudes. Although the Strawsonian account traditionally applies to individual humans, it is plausible that entities that are not individual humans but possess these attitudes are candidates for moral responsibility under a Strawsonian framework. We conclude that weak AI is never morally responsible, while a strong AI with the right emotional capacities may be morally responsible. Â© 2021, The Author(s).},
author_keywords={AI ethics;  Artificial intelligence;  Moral responsibility;  Reactive attitude;  Strawson},
keywords={Turing machines, Moral responsibility, Artificial intelligence funding_textÂ 1={The Authors would like to thank Karl Schafer, Eric Brown, Chris Freiman, Kaite McKenna,; Sara Rodriguez.},
references={Barry, P.B., Saving Strawson: Evil and Strawsonian accounts of moral responsibility (2011) Ethical Theory and Moral Practice, 14 (5), pp. 5-21; Beard, J.M., Autonomous weapons and human responsibilities (2014) Georgetown Journal of International Law, 45 (3), pp. 617-682; Benn, P., Freedom, resentment, and the psychopath (1999) Philosophy, Psychiatry, & Psychology, 6 (1), pp. 29-39; Brown, D., (2017) Origin, , Doubleday; Champagne, M., Tonkens, R., Bridging the responsibility gap in automated warfare (2015) Philosophy and Technology, 28, pp. 125-137; Ciocchetti, C., The responsibility of the psychopathic offender (2003) Philosophy, Psychiatry, and Psychology.; Coeckelbergh, M., The moral standing of machines: Towards a relational and non-Cartesian moral hermeneutics (2014) Philosophy and Technology; Doris, J., (2002) Lack of character, , Cambridge University Press; Floridi, L., Sanders, J.W., On the morality of artificial agents (2004) Machine Ethics, 14, pp. 349-379; Gilbert, M., (2014) Joint commitment: How we make the social world, , Oxford University Press; Godfrey-Smith, P., (2016) Other minds: The Octopus, the sea, and the deep origins of consciousness, , Farrar; Greenspan, P.S., Responsible psychopaths (2003) Philosophical Psychology; Greenspan, P., Responsible psychopaths revisited (2016) Journal of Ethics, 20, pp. 265-278; Gunkel, D.J., Mind the gap: Responsible robotics and the problem of responsibility (2017) Ethics and Information Technology; Hieronymi, P., (2020) Freedom, Resentment, and the Metaphysics of Morals, , Princeton University Press; Himma, K.E., Artificial agency, consciousness, and the criteria for moral agency: What properties must an artificial agent have to be a moral agent? (2009) Ethics and Information Technology, 11, pp. 19-29; Himmelreich, J., Responsibility for killer robots (2019) Ethical Theory and Moral Practice, 22, pp. 731-747; Johnson, D.G., Computer systems: Moral entities but not moral agents (2006) Ethics and Information Technology, 8, pp. 195-204; Johnson, D.G., Miller, K.W., Un-making artificial moral agents (2008) Ethics and Information Technology, 10, pp. 123-133; Kirk, R., (2019) Zombies, , https://plato.stanford.edu/archives/spr2019/entries/zombies/, In The stanford encyclopedia of philosophy, E. N. Zalta (ed.). Retrieved from; Matthias, A., The responsibility gap: Ascribing responsibility for the actions of learning automata (2004) Ethics and Information Technology, 6, pp. 175-183; McKenna, M., (2012) Conversation and responsibility, , Oxford University Press; Ramirez, E., Psychopathy, moral reasons, and responsibility (2013) Ethics and neurodiversity, , Alexandra Perry C, Herrera D, (eds), Cambridge Scholars Publishing; Russell, S., Norvig, P., (2016) Artificial intelligence: A modern approach, , 3, Pearson; Scanlon, T.M., (2008) Moral dimensions: Permissibility, meaning, , Belknap Press of Harvard University Press; Schulzke, M., Autonomous weapons and distributed responsibility (2013) Philosophy & Technology, 26 (2), pp. 203-219; Shoemaker, D., Attributability, answerability, and accountability: Toward a wider theory of moral responsibility (2011) Ethics, 121 (3), pp. 603-632; Shoemaker, D., (2015) Responsibility from the margins, , Oxford University Press; Silver, D., A Strawsonian defense of corporate moral responsibility (2005) American Philosophical Quarterly, 42 (4), pp. 279-293; Smith, A.M., Attributability, answerability, and accountability: In defense of a unified account (2012) Ethics, 122 (3), pp. 575-589; Smith, A.M., Responsibility as answerability (2015) Inquiry: An Interdisciplinary Journal of Philosophy, 58 (2), pp. 99-126; Sparrow, R., Killer robots (2007) Journal of Applied Philosophy, 24, pp. 62-77; Stahl, B.C., Responsible computers? A case for ascribing quasi-responsibility to computers independent of personhood or agency (2006) Ethics and information technology, 8 (4), pp. 205-213; Strawson, P.F., Freedom and resentment (2008) Freedom and Resentment and Other Essays; Sullins, J.P., When is a robot a moral agent? (2006) International Review of Information Ethics, 6, pp. 23-30; Sun, R., Computation, reduction, and teleology of consciousness (2001) Cognitive Systems Research, 1 (4), pp. 241-249; Swoboda, T., Autonomous weapon systemsâ€”An alleged responsibility gap (2018) Philosophy and theory of artificial intelligence 2017. PT-AI 2017. Studies in applied philosophy, epistemology and rational ethics, 44. , MÃ¼ller V, (ed), Springer; Talbert, M., Blame and responsiveness to moral reasons: Are psychopaths blameworthy? (2008) Pacific Philosophical Quarterly; Talbert, M., Moral responsibility (2019) The stanford encyclopedia of philosophy, , Zalta E, (ed), Stanford University; Tollefsen, D., Organizations as true believers (2002) Journal of Social Philosophy, 33 (3), pp. 395-410; Tollefsen, D.P., Participant reactive attitudes and collective responsibility (2003) Philosophical Explorations, 6 (3), pp. 218-234; Tuomela, R., (2013) Social ontology: Collective intentionality and group agents, , Oxford University Press; Watson, G., Two faces of responsibility (1996) Philosophical Topics, 24 (2), pp. 227-248; Williams, B., The self and the future (1970) The Philosophical Review, 79 (2), pp. 161-180. , (,).,., (,)., https://doi.org/10.2307/2183946},
correspondence_address1={Smith, N.; Department of Philosophy, United States; email: nichos1@uci.edu},
editor={NA},
publisher={Springer Science and Business Media B.V.},
issn={13881957},
isbn={NA},
language={English},
abbrev_source_title={Ethics Inf. Technol.},
document_type={Article},
source={Scopus},
number={3},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Sparrow2021685,
type={ARTICLE},
author={Sparrow, R.},
title={Why machines cannot be moral},
journal={AI and Society},
year={2021},
volume={36},
pages={685-693},
doi={10.1007/s00146-020-01132-6},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099800789&doi=10.1007%2fs00146-020-01132-6&partnerID=40&md5=49d03207350e32fcda3f719b85e656d9},
affiliation={Department of Philosophy, Faculty of Arts, Monash University, Melbourne, Australia},
abstract={The fact that real-world decisions made by artificial intelligences (AI) are often ethically loaded has led a number of authorities to advocate the development of â€œmoral machinesâ€. I argue that the project of building â€œethicsâ€ â€œintoâ€ machines presupposes a flawed understanding of the nature of ethics. Drawing on the work of the Australian philosopher, Raimond Gaita, I argue that ethical dilemmas are problems for particular people and not (just) problems for everyone who faces a similar situation. Moreover, the force of an ethical claim depends in part on the life history of the person who is making it. For both these reasons, machines could at best be engineered to provide a shallow simulacrum of ethics, which would have limited utility in confronting the ethical and policy dilemmas associated with AI. Â© 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.},
author_keywords={Artificial intelligence;  Ethics;  Machine ethics;  Moral authority;  Raimond Gaita},
keywords={Artificial intelligence, Ethical dilemma;  Life history;  Real-world, Philosophical aspects funding_textÂ 1={I would like to acknowledge helpful conversations with David Simpson over the course of drafting this manuscript. I am grateful to Joshua Hatherley for his assistance with bibliographic research. This paper also owes an obvious,; large, debt to the work of Raimond Gaita,; also to his personal example when he taught me in several seminars at the University of Melbourne. It is a cause of some discomfort to me that, besides bringing his arguments to bear on the case of AI, I am not sure how much I have added to them. Nevertheless, I hope that, at the very least, by bringing them to the attention of a larger audience; demonstrating the extent to which they illuminate the central questions of AI ethics, I will encourage abler minds to engage with,; perhaps extend, his work on the personal in ethics, the role of remorse, the concept of the person,; the nature of ethics itself.},
references={Anderson, M., Anderson, S.L., Machine ethics: Creating an ethical intelligent agent (2007) AI Mag, 28 (4), pp. 15-26; Anderson, M., Anderson, S.L., (2018) Machine ethics, , Cambridge University Press, New York; Anderson, M., Anderson, S.L., A prima facie duty approach to machine ethics: Machine learning of features of ethical dilemmas, prima facie duties, and decision principles through a dialogue with ethicists (2018) Machine ethics, pp. 476-494. , Anderson M, Anderson SL, (eds), Cambridge University Press, Cambridge; Annas, J., (2011) Intelligent virtue, , Oxford University Press, New York; (1986) Ethics, , Penguin Books, Harmondsworth UK; Arkin, R.C., (2009) Governing lethal behavior in autonomous robots, , CRC Press, Boca Raton; Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., Bonnefon, J.F., Rahwan, I., The moral machine experiment (2018) Nature, 563 (7729), pp. 59-64; Bringsjord, S., Taylor, J., Van Heuveln, B., Arkoudas, K., Clark, M., Wojtowicz, R., Piagetian roboethics via category theory: Moving beyond mere formal operations to engineer robots whose decisions are guaranteed to be ethically correct (2018) Machine Ethics, pp. 361-374. , Anderson M, Anderson SL, (eds), Cambridge University Press, Cambridge; Brundage, M., Limitations and risks of machine ethics (2014) J Exp Theor Artif Intell, 26 (3), pp. 355-372; Cervantes, J.A., RodrÃ­guez, L.F., LÃ³pez, S., Ramos, F., Robles, F., Autonomous agents and ethical decision-making (2016) Cognit Comput, 8 (2), pp. 279-296; Cherry, C., Machines as persons? (1991) Philos Supp, 29, pp. 11-24; Cockburn, D., The mind, the brain and the face (1985) Philosophy, 60 (234), pp. 477-493; Cockburn, D., An attitude towards a soul (1990) Other human beings, pp. 3-12. , Cockbur(ed), Palgrave Macmillan, London; Cockburn, D., Human beings and giant squids (1994) Philosophy, 69 (268), pp. 135-150; Cordner, C., Moral philosophy in the midst of things (2014) A sense for Humanity: The ethical thought of Raimond Gaita, pp. 125-140. , Taylor C, Graefe M, (eds), Monash University Publishing, Clayton; Eubanks, V., (2018) Automating inequality: how high-tech tools profile, police, and punish the poor, , St. Martins Press, New York; Gaita, R., The personal in ethics (1989) Wittgenstein: attention to particulars, pp. 124-150. , Phillips DZ, Winch P, (eds), MacMillan, London; Gaita, R., (2004) Good and evil: an absolute conception, 2. , MacMillan, London; Gaita, R., Truth and truthfulness in narrative (2011) After Romulus, pp. 90-119. , Gaita R, (ed), Text, Melbourne; Giubilini, A., Savulescu, J., The artificial moral advisor. The `ideal observerÂ´ meets artificial intelligence (2017) Philos Technol, 31, pp. 1-20; Gunkel, D., (2012) The machine question: critical perspectives on AI, robots, and ethics, , MIT Press, Cambridge; HellstrÃ¶m, T., On the moral responsibility of military robots (2013) Ethics Inf Technol, 15 (2), pp. 99-107; (2018) Ethics guidelines for trustworthy AI, , European Commission, Brussels; Hursthouse, R., (1999) On virtue ethics, , Oxford University Press, Oxford; Lin, P., Why ethics matters for autonomous cars (2016) Autonomous driving, pp. 69-85. , Maurer M, Gerdes JC, Lenz B, Winner H, (eds), Springer, Berlin, Heidelberg; Miner, A.S., Milstein, A., Schueller, S., Hegde, R., Mangurian, C., Linos, E., Smartphone-based conversational agents and responses to questions about mental health, interpersonal violence, and physical health (2016) JAMA Intern Med, 76 (5), pp. 619-625; Moor, J., The nature, importance, and difficulty of machine ethics (2018) Machine ethics, pp. 13-20. , Anderson M, Anderson SL, (eds), Cambridge University Press, Cambridge; O'Neil, C., (2016) Weapons of math destruction: How big data increases inequality and threatens democracy, , Allen Lane, London; Pereira, L.M., Saptawijaya, A., Modelling morality with prospective logic (2018) Machine ethics, pp. 398-421. , Anderson M, Anderson SL, (eds), Cambridge University Press, Cambridge; Pianalto, P., Speaking for oneself: Wittgenstein on ethics (2011) Inquiry, 54 (3), pp. 252-276; Klibansky, R., Anscombe, E., The Sophist (1961) The sophist; and the statesman, translation and introduction, , (eds), Taylor AE, T. Nelson, London; Powers, T.M., Prospects for a Kantian machine (2006) IEEE Intell Syst, 21 (4), pp. 46-51; Roff, H.M., The strategic robot problem: Lethal autonomous weapons in war (2014) J Mil Ethics, 13 (3), pp. 211-227; Savulescu, J., Maslen, H., Moral enhancement and artificial intelligence: moral AI? (2015) Beyond artificial intelligence, pp. 79-95. , Romportl J, Zackova E, Kelemen J, (eds), The disappearing human-machine divide, Springer, Cham; Shapiro, S.C., Artificial intelligence (1992) Encyclopedia of artificial intelligence, pp. 54-57. , Shapiro SC, (ed), 2, Wiley, New York; Scharre, P., (2018) Army of none, , WW Norton & Co, New York and London; Scheutz, M., The case for explicit ethical agents (2017) AI Mag, 38 (4), pp. 57-64; Smith, M., Realism (1991) A companion to ethics, pp. 399-410. , Singer P, (ed), Blackwell Reference, Cambridge; Skilbeck, A., The personal and impersonal in moral education (2014) New perspectives on philosophy of education: ethics, politics and religion, pp. 59-76. , LewiGuilherme A, White M, (eds), Bloomsbury Academic, London; Sparrow, R., The Turing triage test (2004) Ethic Inf Technol, 6 (4), pp. 203-213; Sparrow, R., Killer robots (2007) J Appl Philos, 24 (1), pp. 62-77; Sparrow, R., Robots and respect: Assessing the case against autonomous weapon systems (2016) Ethics Int Aff, 30 (1), pp. 93-116; Taylor, C., Moral thought and ethical individuality (2014) A sense for humanity: the ethical thought of Raimond Gaita, pp. 141-151. , Taylor C, Graefe M, (eds), Monash University Publishing, Clayton; van den Hoven, J., Lokhorst, G.J., Deontic logic and computer-supported ethics (2002) Metaphilosophy, 33 (3), pp. 376-386; Wallach, W., Allen, C., (2009) Moral machines: Teaching robots right from wrong, , Oxford University Press, Oxford; Walzer, M., (2015) Just and unjust wars: A moral argument with historical illustrations, , 5, Basic Books, New York; Whitby, B., On computable morality: An examination of machines as moral advisors (2018) Machine ethics, pp. 138-150. , Anderson M, Anderson SL, (eds), Cambridge University Press, Cambridge; Winch, P., The Presidential address: â€˜Eine Einstellung zur Seeleâ€™ (1980) Proc Aristot Soci, 81, pp. 1-15; Winfield, A.F.T., Blum, C., Liu, W., Towards an ethical robot: Internal models, consequences and ethical action selection (2014) Advances in autonomous robotics systems, pp. 85-96. , Mistry M, Leonard A, Witkowski M, Melhuish C, (eds), Springer, Cham; Wittgenstein, L., (1989) Philosophical investigations, , 3, Basil Blackwell, Oxford},
correspondence_address1={Sparrow, R.; Department of Philosophy, Australia; email: robert.sparrow@monash.edu},
editor={NA},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={09515666},
isbn={NA},
language={English},
abbrev_source_title={AI Soc.},
document_type={Article},
source={Scopus},
number={3},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Formosa2021839,
type={ARTICLE},
author={Formosa, P. and Ryan, M.},
title={Making moral machines: why we need artificial moral agents},
journal={AI and Society},
year={2021},
volume={36},
pages={839-851},
doi={10.1007/s00146-020-01089-6},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094949433&doi=10.1007%2fs00146-020-01089-6&partnerID=40&md5=f64e21301c32fcf141db0a8543263638},
affiliation={Department of Philosophy, Macquarie University, Sydney, NSW  2109, Australia; Department of Computing, Macquarie University, Sydney, NSW  2109, Australia},
abstract={As robots and Artificial Intelligences become more enmeshed in rich social contexts, it seems inevitable that we will have to make them into moral machines equipped with moral skills. Apart from the technical difficulties of how we could achieve this goal, we can also ask the ethical question of whether we should seek to create such Artificial Moral Agents (AMAs). Recently, several papers have argued that we have strong reasons not to develop AMAs. In response, we develop a comprehensive analysis of the relevant arguments for and against creating AMAs, and we argue that all things considered we have strong reasons to continue to responsibly develop AMAs. The key contributions of this paper are threefold. First, to provide the first comprehensive response to the important arguments made against AMAs by Wynsberghe and Robbins (in â€œCritiquing the Reasons for Making Artificial Moral Agentsâ€, Science and Engineering Ethics 25, 2019) and to introduce several novel lines of argument in the process. Second, to collate and thematise for the first time the key arguments for and against AMAs in a single paper. Third, to recast the debate away from blanket arguments for or against AMAs in general, to a more nuanced discussion about the use of what sort of AMAs, in what sort of contexts, and for what sort of purposes is morally appropriate. Â© 2020, Springer-Verlag London Ltd., part of Springer Nature.},
author_keywords={Artificial intelligence (AI);  Artificial moral agents (AMA);  Autonomous vehicle ethics;  Machine ethics;  Moral machines;  Robot ethics},
keywords={Philosophical aspects, Comprehensive analysis;  Ethical question;  Moral agents;  Science; engineering;  Social context;  Technical difficulties, Artificial intelligence},
references={Addyman, C., French, R., Computational modeling in cognitive science (2012) Top Cogn Sci, 4, pp. 332-341; Allen, C., Smit, I., Wallach, W., Artificial morality (2005) Ethics Inf Technol, 7 (3), pp. 149-155; Anderson, M., Anderson, S., Machine ethics (2007) AI Mag Winter, 28 (4), pp. 15-26; Anderson, S., Anderson, M., How machines can advance ethics (2009) Philos Now, 72, pp. 17-20; Anderson, M., Anderson, S., GenEth (2018) Paladyn J Behav Robot, 9, pp. 337-357; Arkin, R., Ulam, P., Duncan, B., An ethical governor for constraining lethal action in an autonomous system (2009) Technical Report GIT-GVU-09-02; Arkin, R., Ulam, P., Wagner, A., Moral decision making in autonomous systems (2012) Proc IEEE, 100 (3), pp. 571-589; Asaro, P.M., What should we want from a robot ethic? (2006) Int Rev Inform Ethics, 6, pp. 9-16; Bankins, S., Formosa, P., When AI meets PC (2019) Eur J Work Organ Psychol, 26, pp. 1-15; Bekey, G.A., Current trends in robotics (2012) Robot ethics, pp. 17-34. , Lin P, Abney K, Bekey GA, (eds), MIT Press, Cambridge, MA; Boden, M., (2016) AI, , OUP, Oxford; Bonnemains, V., Saurel, C., Tessier, C., Embedded ethics (2018) Ethics Inf Technol, 20 (1), pp. 41-58; Bostrom, N., (2014) Superintelligence, , OUP, Oxford; Broadbent, E., Interactions with robots (2017) Annu Rev Psychol, 68 (1), pp. 627-652; Brundage, M., Limitations and risks of machine ethics (2014) J Exp Theor Artif Intell, 26 (3), pp. 355-372; Bryson, J., Patiency is not a virtue (2018) Ethics Inf Technol, 20 (1), pp. 15-26; Chalmers, D., The singularity (2010) J Conscious Stud, 17 (9), pp. 7-65; Danaher, J., Robots, law and the retribution gap (2016) Ethics Inf Technol, 18 (4), pp. 299-309; Darling, K., Whoâ€™s Johnny? Anthropomorphic framing in humanâ€“robot interaction, integration, and policy (2017) Robot ethics 2.0, pp. 173-188. , Lin P, Abney K, Jenkins R, (eds), OUP, New York; Dietrich, E., Homo Sapiens 2.0 (2001) J Exp Theor Artif Intell, 13 (4), pp. 323-328; Etzioni, A., Etzioni, O., AI assisted ethics (2016) Ethics Inf Technol, 18 (2), pp. 149-156; Floridi, L., Sanders, J.W., On the morality of artificial agents (2004) Mach Ethics, 14, pp. 349-379; Formosa, P., (2017) Kantian ethics, dignity and perfection, , CUP, Cambridge; Gogoll, J., MÃ¼ller, J., Autonomous cars (2017) Sci Eng Ethics, 23 (3), pp. 681-700; Greene, J., An FMRI investigation of emotional engagement in moral judgment (2001) Science, 293, pp. 2105-2108; Gunkel, D., A vindication of the rights of machines (2014) Philos Technol, 27 (1), pp. 113-132; Gunkel, D., Mind the gap (2017) Ethics Inf Technol; Hevelke, A., Nida-RÃ¼melin, J., Responsibility for crashes of autonomous vehicles (2015) Sci Eng Ethics, 21 (3), pp. 619-630; Himma, K., Artificial agency, consciousness, and the criteria for moral agency (2009) Ethics Inf Technol, 11 (1), pp. 19-29; Himmelreich, J., Never mind the trolley (2018) Ethical Theory Moral Pract; Laukyte, M., Artificial agents among us (2017) Ethics Inf Technol, 19 (1), pp. 1-17; Lin, P., Why ethics matters for autonomous cars (2015) Autonomes Fahren, pp. 69-85. , Maurer M, (ed), Springer, Berlin; McCauley, L., AI Armageddon and the three laws of robotics (2007) Ethics Inf Technol, 9 (2), pp. 153-164; Miller, K., Wolf, M., Grodzinsky, F., This â€˜Ethical Trapâ€™ is for roboticists, not robots (2017) Sci Eng Ethics, 23 (2), pp. 389-401; Moor, J., The nature, importance, and difficulty of machine ethics (2006) IEEE Intell Syst, 21 (4), pp. 18-21; Moor, J., Four kinds of ethical robots (2009) Philos Today, 72, pp. 12-14; Nyholm, S., The ethics of crashes with self-driving cars (2018) Philos Compass; Oâ€™Neill, O., (1989) Constructions of reason, , CUP, Cambridge; Peterson, S., Designing people to serve (2012) Robot ethics, pp. 283-298. , Lin P, Abney K, Bekey GA, (eds), MIT Press, Cambridge, MA; Powers, T., Prospects for a Kantian machine (2006) IEEE Intell Syst, 21 (4), pp. 46-51; Robbins, S., AI and the path to envelopment (2020) AI Soc, 35 (2), pp. 391-400; Roff, H.M., Danks, D., Trust but verify (2018) J Mil Ethics, 17 (1), pp. 2-20; Scheutz, M., The need for moral competency in autonomous agent architectures (2016) Fundamental issues of artificial intelligence, pp. 517-527. , MÃ¼ller V, (ed), Springer, Cham; Scheutz, M., The case for explicit ethical agents (2017) AI Mag, 38 (4), pp. 57-64; Sharkey, N., The evitability of autonomous robot warfare (2012) Int Rev Red Cross, 94 (886), pp. 787-799; Sharkey, A., Can robots be responsible moral agents? (2017) Connect Sci, 29 (3), pp. 210-216; Sparrow, R., Can machines be people? (2012) Robot ethics, pp. 301-316. , Lin P, Abney K, Bekey GA, (eds), MIT Press, Cambridge, MA; Sparrow, R., Robots and respect (2016) Ethics Int Aff, 30 (1), pp. 93-116; Staines, D., Formosa, P., Ryan, M., Morality play: a model for developing games of moral expertise (2019) Games Cult, 14 (4), pp. 410-429; Tonkens, R., A challenge for machine ethics (2009) Mind Mach, 19 (3), pp. 421-438; Tonkens, R., Out of character (2012) Ethics Inf Technol, 14 (2), pp. 137-149; Torrance, S., Ethics and consciousness in artificial agents (2008) AI Soc, 22 (4), pp. 495-521; Turkle, S., (2011) Alone together, , Basic Books, New York; Vallor, S., Moral deskilling and upskilling in a new machine age (2015) Philos Technol, 28 (1), pp. 107-124; van Wynsberghe, A., Robbins, S., Critiquing the reasons for making artificial moral agents (2019) Sci Eng Ethics, 25, pp. 719-735; Voiklis, J., Moral judgments of human vs. Robot agents (2016) 25Th IEEE International Symposium on Robot and Human Interactive Communication, pp. 775-780; Wallach, W., Robot minds and human ethics (2010) Ethics Inf Technol, 12 (3), pp. 243-250; Wallach, W., Allen, C., (2009) Moral machines, , OUP, Oxford},
correspondence_address1={Formosa, P.; Department of Philosophy, Australia; email: Paul.Formosa@mq.edu.au},
editor={NA},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={09515666},
isbn={NA},
language={English},
abbrev_source_title={AI Soc.},
document_type={Article},
source={Scopus},
number={3},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Tigard2021455,
type={ARTICLE},
author={Tigard, D.W.},
title={Artificial Agents in Natural Moral Communities: A Brief Clarification},
journal={Cambridge Quarterly of Healthcare Ethics},
year={2021},
volume={30},
pages={455-458},
doi={10.1017/S0963180120001000},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107737565&doi=10.1017%2fS0963180120001000&partnerID=40&md5=81b4d193be2dee3de86e19c4c7f91983},
affiliation={Institute for History and Ethics of Medicine, Technical University of Munich, Munich, 81675, Germany},
abstract={What exactly is it that makes one morally responsible? Is it a set of facts which can be objectively discerned, or is it something more subjective, a reaction to the agent or context-sensitive interaction? This debate gets raised anew when we encounter newfound examples of potentially marginal agency. Accordingly, the emergence of artificial intelligence (AI) and the idea of novel beings represent exciting opportunities to revisit inquiries into the nature of moral responsibility. This paper expands upon my article Artificial Moral Responsibility: How We Can and Cannot Hold Machines Responsible and clarifies my reliance upon two competing views of responsibility. Although AI and novel beings are not close enough to us in kind to be considered candidates for the same sorts of responsibility we ascribe to our fellow human beings, contemporary theories show us the priority and adaptability of our moral attitudes and practices. This allows us to take seriously the social ontology of relationships that tie us together. In other words, moral responsibility is to be found primarily in the natural moral community, even if we admit that those communities now contain artificial agents. Â©},
author_keywords={artificial intelligence;  blame;  human-robot interaction;  machine ethics;  moral agency;  moral responsibility},
keywords={artificial intelligence;  human;  morality;  robotics, Artificial Intelligence;  Humans;  Morals;  Robotics},
references={Shoemaker, D., Responsibility from the Margins (2015) New York: Oxford University Press; (2015) For a Fuller Explanation, See Note 2, Shoemaker, pp. 19-20; Hence my subtitle: How we can and cannot hold machines responsible See Note 1},
correspondence_address1={Tigard, D.W.; Institute for History and Ethics of Medicine, Germany; email: daniel.tigard@tum.de},
editor={NA},
publisher={Cambridge University Press},
issn={09631801},
isbn={NA},
language={English},
abbrev_source_title={Camb. Q. Healthc. Ethics},
document_type={Article},
source={Scopus},
number={3},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={34109922},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Tigard2021435,
type={ARTICLE},
author={Tigard, D.W.},
title={Artificial Moral Responsibility: How We Can and Cannot Hold Machines Responsible},
journal={Cambridge Quarterly of Healthcare Ethics},
year={2021},
volume={30},
pages={435-447},
doi={10.1017/S0963180120000985},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107716321&doi=10.1017%2fS0963180120000985&partnerID=40&md5=5439ea73003444755802aa636d22d407},
affiliation={Institute for History and Ethics of Medicine, Technical University of Munich, Ismaninger Str. 22, Munich, 81675, Germany},
abstract={Our ability to locate moral responsibility is often thought to be a necessary condition for conducting morally permissible medical practice, engaging in a just war, and other high-stakes endeavors. Yet, with increasing reliance upon artificially intelligent systems, we may be facing a widening responsibility gap, which, some argue, cannot be bridged by traditional concepts of responsibility. How then, if at all, can we make use of crucial emerging technologies? According to Colin Allen and Wendell Wallach, the advent of so-called 'artificial moral agents' (AMAs) is inevitable. Still, this notion may seem to push back the problem, leaving those who have an interest in developing autonomous technology with a dilemma. We may need to scale-back our efforts at deploying AMAs (or at least maintain human oversight); otherwise, we must rapidly and drastically update our moral and legal norms in a way that ensures responsibility for potentially avoidable harms. This paper invokes contemporary accounts of responsibility in order to show how artificially intelligent systems might be held responsible. Although many theorists are concerned enough to develop artificial conceptions of agency or to exploit our present inability to regulate valuable innovations, the proposal here highlights the importance of-and outlines a plausible foundation for-a workable notion of artificial moral responsibility. Â©},
author_keywords={AI ethics;  artificial intelligence;  blame;  human-robot interaction;  machine ethics;  moral agency;  moral responsibility},
keywords={artificial intelligence;  human;  intelligence;  morality;  technology, Artificial Intelligence;  Humans;  Intelligence;  Morals;  Technology},
references={See Book Iii of Aristotle?s Nicomachean Ethics; (2012) See Note 7, McKenna, 11; (2009) See Note 6, Allen and Wallach, 4; (2009) See Note 6, Allen and Wallach, 68; Ibid at 5; (2015) See Note 10, Shoemaker, pp. 19-20; See Note 5 for Similar Arguments in Healthcare, Education, and Transportation; (2007) See Note 4, Sparrow, 65; Ibid., at 66; Italics Added; Ibid., at 69; Italics added. Comparable inconsistencies are seen in Floridi and Sanders (2004) Note 24; Ibid; Ibid., at 71; Italics Added; (2007) See Note 4, Sparrow, 71; Ibid., at 72; Ibid; (2004) See Note Note 50, Watson, 274; (2015) See Note 10, Shoemaker, 57; (2015) See Note 10, Shoemaker, pp. 146-182},
correspondence_address1={Tigard, D.W.; Institute for History and Ethics of Medicine, Ismaninger Str. 22, Germany; email: daniel.tigard@tum.de},
editor={NA},
publisher={Cambridge University Press},
issn={09631801},
isbn={NA},
language={English},
abbrev_source_title={Camb. Q. Healthc. Ethics},
document_type={Article},
source={Scopus},
number={3},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={34109925},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Gu2021632,
type={ARTICLE},
author={Gu, T.-L. and Li, L.},
title={Artificial Moral Agents and Their Design Methodology: Retrospect and Prospect [ä¼¦ç†æ™ºèƒ½ä½“åŠå…¶è®¾è®¡: çŽ°çŠ¶å’Œå±•æœ›]},
journal={Jisuanji Xuebao/Chinese Journal of Computers},
year={2021},
volume={44},
pages={632-651},
doi={10.11897/SP.J.1016.2021.00632},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101869141&doi=10.11897%2fSP.J.1016.2021.00632&partnerID=40&md5=e7096c0b15cb08fb6064118214be17dd},
affiliation={College of Information Science and Technology/College of Cyber Security, Jinan University, Guangzhou, 510632, China; Guangxi Key Laboratory of Trusted Software, Guilin University of Electronic Technology, Guilin, 541004, China},
abstract={Artificial agents have always been one of the main research fields of artificial intelligence. Any independent entity that can interact with the environment and make decisions autonomously can be abstracted as an agent. With the development of artificial intelligence from computational intelligence to perceptual intelligence, and then to cognitive intelligence, artificial agents have gradually penetrated into many fields closely related to our human life, such as unmanned driving cars, service robots, smart households, voice assistants, intelligent medical care and war weapons. In these applications, the interactions between agents and among agents and the environment, especially humans and society, are becoming more and more prominent, and the interactions of agents with humans and society inevitably resulting in ethical threats and moral attentions. Issues regarding artificial moral agents are important research contents of ethical artificial intelligence. The ethical risks and challenges in the application of artificial intelligence have aroused more and more public concerns. The ethical issues of artificial intelligence belong to the interdisciplinary research of social science, philosophy, psychology, cognitive science, computer, artificial intelligence, etc. On the one hand, philosophy and humanistic social scientists focus on the moral subjects of artificial intelligence, the effects of artificial intelligence on the human society, the ethical norms and moral codes of artificial intelligence and so on. On the other hand, researchers in the engineering fields such as computer, cognitive science and artificial intelligence are interested in the embedded ethical decision design, the implementation of moral artificial intelligence, the ethical supervision and control of artificial intelligence and others. In the framework of ethically aligned design, realization and deployment of moral artificial intelligence, this paper reviews the design methodology of artificial moral agents in the following aspects. Firstly, the concepts of artificial moral agents are introduced, and in terms of multiple behavioral characteristics, such as autonomy, adaptation, evolution and consciousness, the categories of artificial moral agents are illustrated. Although there is still controversial on the effectiveness of the Turing test, as a promising way to identify and evaluate moral artificial agents, the moral Turing test is addressed briefly. Ethical norms and moral codes are the premises for the design and implementation of artificial moral agents, and many government agencies, industry associations, companies and international organizations have put forward a series of moral codes and ethical norms to manage and supervise artificial intelligence. The open problems, such as how to properly choose and strictly abide by them, and how to translate them to machine or computer codes are discussed, etc. Secondly, the three design paradigms of artificial moral agents, including top-down, bottom-up and hybrid approach are analyzed, and the design of moral artificial agents via logical programming is elaborated, which integrates two different concepts of both logic and programming. Formal verification and validation of artificial moral agents are also outlined. Finally, the ethical dilemma is a common puzzle of philosophy, social science and engineering disciplines, and the potential solution using computational ethics and crowdsourcing technology to defend it is described. More importantly, a comprehensive outlook of the challenges and the further research directions of moral artificial agents is presented in the paper. Â© 2021, Science Press. All right reserved.},
author_keywords={Artificial intelligence ethics;  Artificial moral agents;  Ethically aligned design;  Formal verification;  Logic programming},
keywords={Behavioral research;  Computation theory;  Computer programming;  Design;  Intelligent agents;  Intelligent computing;  Medical robotics;  Philosophical aspects;  Public risks;  Social robots;  Social sciences computing, Behavioral characteristics;  Cognitive intelligence;  Design; implementations;  Industry association;  Interdisciplinary research;  International organizations;  Science; engineering;  Verification-and-validation, Autonomous agents},
references={Wooldridge, M, Jennings, N R., Intelligent agents: Theory and practice (1995) The Knowledge Engineering Review, 10 (2), pp. 115-152; Cervantes, J A, LÃ³pez, S, RodrÃ­guez, L F, Artificial moral agents: A survey of the current status (2020) Science and Engineering Ethics, 26, pp. 501-532; Floridi, L, Sanders, J W., On the morality of artificial agents (2004) Minds and Machines, 14 (3), pp. 349-379; Allen, C, Wallach, W, Smit, I., Why machine ethics? (2006) IEEE Intelligent Systems, 21 (4), pp. 12-17; Anderson, M, Anderson, S L., Machine ethics: Creating an ethical intelligent agent (2007) AI Magazine, 28 (4), pp. 15-26; Yu, H, Shen, Z, Miao, C, Building ethics into artificial intelligence (2018) Proceedings of the 27th International Joint Conference on Artificial Intelligence, pp. 5527-5533. , Stockholm, Sweden; Weiner, N., (1950) The Human Use of Human Beings: Cybernetics and Society, , Boston, USA: Da Capo Press; Allen, C, Varner, G, Zinser, J., Prolegomena to any future artificial moral agent (2000) Journal of Experimental & Theoretical Artificial Intelligence, 12 (3), pp. 251-261; Moor, J H., The nature, importance, and difficulty of machine ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 18-21; Wang, Dong-Hao, A preliminary study on moral conflicts and dilemmas caused by artificial intelligence (2014) Ethics Research, (2), pp. 74-79. , (in Chinese) (çŽ‹ä¸œæµ©. äººå·¥æ™ºèƒ½ä½“å¼•å‘çš„é“å¾·å†²çªå’Œå›°å¢ƒåˆæŽ¢. ä¼¦ç†å­¦ç ”ç©¶, 2014, (2): 74-79); Nagenborg, M., Artificial moral agents: An intercultural perspective (2007) International Review of Information Ethics, 7 (9), pp. 129-133; Chen, Xiao-Ping, Ethical system of artificial intelligence: Infrastructure and key issues Journal of Intelligent Systems, 14 (4), pp. 605-610. , 209, (in Chinese) (é™ˆå°å¹³. äººå·¥æ™ºèƒ½ä¼¦ç†ä½“ç³»: åŸºç¡€æž¶æž„ä¸Žå…³é”®é—®é¢˜. æ™ºèƒ½ç³»ç»Ÿå­¦æŠ¥, 2019, 14(4): 605-610); Allen, C, Smit, I, Wallach, W., Artificial morality: Top-down, bottom-up, and hybrid approaches (2005) Ethics and Information Technology, 7 (3), pp. 149-155; Van Den Hoven, J, Lokhorst, G J., Deontic logic and computer supported computer ethics (2002) Meta Philosophy, 33 (3), pp. 376-386; Mikhail, J., Universal moral grammar: Theory, evidence and the future (2007) Trends in Cognitive Sciences, 11 (4), pp. 143-152; Brundage, M., Limitations and risks of machine ethics (2014) Journal of Experimental & Theoretical Artificial Intelligence, 26 (3), pp. 355-372; Coeckelbergh, M., Moral appearances: Emotions, robots, and human morality (2010) Ethics and Information Technology, 12 (3), pp. 235-241; Malle, B F., Integrating robot ethics and machine morality: The study and design of moral competence in robots (2016) Ethics and Information Technology, 18 (4), pp. 243-256; Turing, A M., Computing machinery and intelligence (1950) Mind, 59 (236), pp. 433-460; Quinn, M J., (2017) Ethics for the Information Age, , 7th Edition. Boston, USA: Pearson Addison Wesley; Gips, J., Towards the ethical robot (1995) Android Epistemology, pp. 243-252. , Ford K, Glymour C, Hayes P, eds. Cambridge, UK: MIT Press; Asimov, I., Runaround (1942) Astounding Science Fiction, 29 (1), pp. 94-103; (2016) Guide to the Ethical Design and Application of Robots and Robotic Systems, , British Standards Institution. BS 8611: 2016 Robots and robotic devices, London, UK; How, J P., Ethically aligned design (2018) IEEE Control Systems Magazine, 38 (3), pp. 3-4; Bryson, J, Winfield, A., Standardizing ethical design for artificial intelligence and autonomous systems (2017) Computer, 50 (5), pp. 116-119; Luetge, C., The German ethics code for automated and connected driving (2017) Philosophy & Technology, 30 (4), pp. 547-558; China Robot Ethics Standardization Perspective(2019), , China National Robot Standardization Group. Beijing: Peking University Press, 2019(in Chinese) (ä¸­å›½å›½å®¶æœºå™¨äººæ ‡å‡†åŒ–æ€»ä½“ç»„. ä¸­å›½æœºå™¨äººä¼¦ç†æ ‡å‡†åŒ–å‰çž»(2019). åŒ—äº¬: åŒ—äº¬å¤§å­¦å‡ºç‰ˆç¤¾, 2019); Floridi, L., Establishing the rules for building trustworthy AI (2019) Nature Machine Intelligence, 1 (6), pp. 261-262; Jobin, A, Ienca, M, Vayena, E., The global landscape of AI ethics guidelines (2019) Nature Machine Intelligence, 1 (9), pp. 389-399; Winfield, A F, Michael, K, Pitt, J, Machine ethics: The design and governance of ethical AI and autonomous systems (2019) Proceedings of the IEEE, 107 (3), pp. 509-517; Morley, J, Floridi, L, Kinsey, L, (2019) From what to how. An overview of AI ethics tools, methods and research to translate principles into practices, , arXiv: 1905.06876; Wallach, W, Allen, C, Smit, I., Machine morality: Bottom-up and top-down approaches for modelling human moral faculties (2008) AI & Society, 22 (4), pp. 565-582; Dehghani, M, Tomai, E, Forbus, K, MoralDM: A computational modal of moral decision-making (2008) Proceedings of the 30th Annual Conference of the Cognitive Science Society, pp. 1410-1415. , Washington, USA; Blass, J A, Forbus, K D., Moral decision-making by analogy: Generalizations versus exemplars (2015) Proceedings of the 29th AAAI Conference on Artificial Intelligence, pp. 501-507. , Austin, USA; Anderson, M, Anderson, S L, Armen, C., An approach to computing ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 56-63; Winfield, A F T, Blum, C, Liu, W., Towards an ethical robot: Internal models, consequences and ethical action selection (2014) Proceedings of the 14th Annual Conference Towards Autonomous Robotic Systems, pp. 85-96. , Birmingham, UK; Briggs, G M, Scheutz, M., "Sorry, I can't do that": Developing mechanisms to appropriately reject directives in human-robot interactions (2015) Proceedings of the 2015 AAAI Fall Symposium: AI for Human-Robot Interaction, pp. 32-36. , Arlington, USA; Tonkens, R., A challenge for machine ethics (2009) Minds and Machines, 19 (3), pp. 421-438; Moor, J H., Is ethics computable? (1995) Meta Philosophy, 26, pp. 1-21. , (1/2); Honarvar, A R, Ghasem-Aghaee, N., Casuist BDI-agent: A new extended BDI architecture with the capability of ethical reasoning (2009) Proceedings of the 2009 International Conference on Artificial Intelligence and Computational Intelligence, pp. 86-95. , Shanghai, China; Rao, A S, Georgeff, M P., BDI agents: From theory to practice (1995) Proceedings of the First International Conference on Multi-Agent Systems, pp. 312-319. , San Francisco, USA; Anderson, M, Anderson, S L., GenEth: A general ethical dilemma analyzer (2014) Proceedings of the 28th AAAI Conference on Artificial Intelligence, pp. 253-261. , QuÃ©bec City, Canada; Anderson, M, Anderson, S L., GenEth: A general ethical dilemma analyzer (2018) Paladyn, Journal of Behavioral Robotics, 9 (1), pp. 337-357; Guarini, M., Particularism and the classification and reclassification of moral cases (2006) IEEE Intelligent Systems, 21 (4), pp. 22-28; Honarvar, A R, Ghasem-Aghaee, N., An artificial neural network approach for creating an ethical artificial agent (2009) Proceedings of the 2009 IEEE International Symposium on Computational Intelligence in Robotics and Automation, pp. 290-295. , Daejeon, South Korea; Abel, D, MacGlashan, J, Littman, M L., Reinforcement learning as a framework for ethical decision making (2016) Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pp. 54-61. , Phoenix, USA; Wu, Y H, Lin, S D., (2018) A low-cost ethics shaping approach for designing reinforcement learning agents, , arXiv: 1712.04172v2; Wallach, W, Franklin, S, Allen, C., A conceptual and computational model of moral decision making in human and artificial agents (2010) Topics in Cognitive Science, 2 (3), pp. 454-485; Franklin, S, Patterson, F G., The LIDA architecture: Adding new modes of learning to an intelligent, autonomous, software agent (2006) Integrated Design and Process Technology, pp. 1-8; Madl, T, Franklin, S., Constrained incrementalist moral decision making for a biologically inspired cognitive architecture (2015) A Construction Manual for Robots' Ethical Systems: Requirements, Methods, Implementations, pp. 137-153. , Robert T, ed. Cham, Swiss: Springer; Cervantes, J A, RodrÃ­guez, L F, LÃ³pez, S, Autonomous agents and ethical decision-making (2016) Cognitive Computation, 8 (2), pp. 278-296; Anderson, M, Anderson, S L., Ethical healthcare agents (2008) Advanced Computational Intelligence Paradigms in Healthcare-3, pp. 233-257. , Margarita S, Sachin Lakhmi C J, eds. Berlin, Germany: Springer; Anderson, M, Anderson, S L, Armen, C., MedEthEx: Toward a medical ethics advisor (2005) Proceedings of the 2005 AAAI Fall Symposium: Caring Machines, pp. 9-16. , Arlington, USA; Arkin, R C, Ulam, P, Wagner, A R., Moral decision making in autonomous systems: Enforcement, moral emotions, dignity, trust, and deception (2011) Proceedings of the IEEE, 100 (3), pp. 571-589; Aleksander, I, Lahnstein, M, Lee, R., Will and emotions: A machine model that shuns illusion (2005) Proceedings of the 2005 Symposium on Next Generation Approaches to Machine Consciousness: Imagination, Development, Intersubjectivity, and Embodiment, pp. 110-117. , Hatfield, UK; Scassellati, B M., (2001) Foundations for a Theory of Mind for a Humanoid Robot, , Cambridge: Massachusetts Institute of Technology; Kowalski, R., Logic programming (2014) Handbook of the History of Logic, pp. 523-569. , Gabbay D M, Woods J, Kanamori A, eds. Boston, USA: Elsevier; Lifschitz, V., Foundations of logic programming (1996) Principles of Knowledge Representation, 3, pp. 69-127; Denecker, M, Kakas, A., Abduction in logic programming (2002) Computational Logic: Logic Programming and Beyond, pp. 402-436. , Antonis C K, Fariba S. Berlin, Germany: Springer; Lukasiewicz, T., Probabilistic logic programming with conditional constraints (2001) ACM Transactions on Computational Logic, 2 (3), pp. 289-339; Poole, D., Probabilistic horn abduction and Bayesian networks (1993) Artificial Intelligence, 64 (1), pp. 81-129; Lifschitz, V., Answer set programming and plan generation (2002) Artificial Intelligence, 138 (1-2), pp. 39-54; Muggleton, S, De Raedt, L., Inductive logic programming: Theory and methods (1994) The Journal of Logic Programming, 19, pp. 629-679; Kowalski, R., (2011) Computational Logic and Human Thinking: How to be Artificially Intelligent, , Cambridge, UK: Cambridge University Press; Kowalski, R, Satoh, K., Obligation as optimal goal satisfaction (2018) Journal of Philosophical Logic, 47 (4), pp. 579-609; Pereira, L M, Saptawijaya, A., Modelling morality with prospective logic (2009) International Journal of Reasoning-Based Intelligent Systems, 1 (3), pp. 209-221. , (/4); Pereira, L M, Saptawijaya, A., Bridging two realms of machine ethics (2016) Programming Machine Ethics, pp. 159-165. , LuÃ­s M P, Ari S, eds. Cham, Swiss: Springer; Pereira, L M, Saptawijaya, A., Counterfactuals, logic programming and agent morality (2017) Applications of Formal Philosophy, pp. 25-53. , RafaÅ‚ U, Gillman P. Cham, Swiss: Springer; Saptawijaya, A, Pereira, L M., From logic programming to machine ethics (2019) Handbuch Maschinenethik, pp. 209-227. , Oliver B, ed. Springer Wiesbaden; Ganascia, J G., Modelling ethical rules of lying with Answer Set Programming (2007) Ethics and Information Technology, 9 (1), pp. 39-47; Berreby, F, Bourgne, G, Ganascia, J G., Modelling moral reasoning and ethical responsibility with logic programming (2015) Proceedings of the 20th International Conference on Logic for Programming Artificial Intelligence and Reasoning, pp. 532-548. , Suva, Fiji; Berreby, F, Bourgne, G, Ganascia, J G., A declarative modular framework for representing and applying ethical principles (2017) Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, pp. 96-104. , SÃ¤o Paulo, Brazil; Cointe, N, Bonnet, G, Boissier, O., Ethical judgment of agents' behaviors in multi-agent systems (2016) Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pp. 1106-1114. , Singapore; Baral, C, Gelfond, M, Rushton, N., Probabilistic reasoning with answer sets (2009) Theory and Practice of Logic Programming, 9 (1), pp. 57-144; Han, T A, Saptawijaya, A, Pereira, L M., Moral reasoning under uncertainty (2012) Proceedings of the 18th International Conference on Logic for Programming Artificial Intelligence and Reasoning, pp. 212-227. , MÃ©rida, Venezuela; Dyoub, A, Costantini, S, Lisi, F A., (2019) Towards ethical machines via logic programming, , arXiv: 1909.08255; Sarlej, M K, Ryan, M., Representing morals in terms of emotion (2012) Proceedings of the 8th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, pp. 69-74. , Palo Alto, Canada; Siau, K, Wang, W., Building trust in artificial intelligence, machine learning, and robotics (2018) Cutter Business Technology Journal, 31 (2), pp. 47-53; Yan, Hong-Xiu, Trustworth: An effective description of the future of artificial intelligence ethics (2019) Theoretical Exploration, (4), pp. 38-42. , (in Chinese) (é—«å®ç§€. å¯ä¿¡ä»»: äººå·¥æ™ºèƒ½ä¼¦ç†æœªæ¥å›¾æ™¯çš„ä¸€ç§æœ‰æ•ˆæç»˜. ç†è®ºæŽ¢ç´¢, 2019, (4): 38-42); Andras, P, Esterle, L, Guckert, M, Trusting intelligent machines: Deepening trust within socio-technical systems (2018) IEEE Technology and Society Magazine, 37 (4), pp. 76-83; Gupta, A., Formal hardware verification methods: A survey (1992) Formal Methods in System Design, 1 (2-3), pp. 151-238; Gu, Tian-Long, (2005) Formal Methods for Software Development, , Beijing: Higher Education Press, 2005(in Chinese) (å¤å¤©é¾™. è½¯ä»¶å¼€å‘çš„å½¢å¼åŒ–æ–¹æ³•. åŒ—äº¬: é«˜ç­‰æ•™è‚²å‡ºç‰ˆç¤¾); Luckcuck, M, Farrell, M, Dennis, L A, Formal specification and verification of autonomous robotic systems: A survey (2019) ACM Computing Surveys, 52 (5), pp. 1-41; Arkoudas, K, Bringsjord, S, Bello, P., Toward ethical robots via mechanized deontic logic (2005) Proceedings of the 2005 AAAI Fall Symposium on Machine Ethics, pp. 17-23. , Menlo Park, Canada; Mermet, B, Simon, G., Formal verification of ethical properties in multiagent systems (2016) Proceedings of the 1st Workshop on Ethics in the Design of Intelligent Agents, pp. 28-33. , La Haye, Netherlands; Dennis, L A, Fisher, M, Webster, M P, Model checking agent programming languages (2012) Automated Software Engineering, 19 (1), pp. 5-63; Dennis, L, Fisher, M, Slavkovik, M, Formal verification of ethical choices in autonomous systems (2016) Robotics and Autonomous Systems, 77, pp. 1-14; Bremner, P, Dennis, L A, Fisher, M, On proactive, transparent, and verifiable ethical reasoning for robots (2019) Proceedings of the IEEE, 107 (3), pp. 541-561; McMillan, K L., Symbolic model checking (1993) Symbolic Model Checking, pp. 25-60. , Kenneth L M, ed. Boston, USA: Springer; Gu, Tian-Long, Xu, Zhou-Bo, (2009) Ordered Binary Decision Graphs and Their Applications, , Beijing: Science Press, 2009(in Chinese) (å¤å¤©é¾™, å¾å‘¨æ³¢. æœ‰åºäºŒå‰å†³ç­–å›¾åŠåº”ç”¨. åŒ—äº¬: ç§‘å­¦å‡ºç‰ˆç¤¾); Deng, B., Machine ethics: The robot's dilemma (2015) Nature News, 523 (7558), p. 24; Cristani, M, Burato, E., Approximate solutions of moral dilemmas in multiple agent system (2009) Knowledge and Information Systems, 2 (18), pp. 157-181; Zambonelli, F, Viroli, M., A survey on nature-inspired metaphors for pervasive service ecosystems (2011) International Journal of Pervasive Computing and Communications, 7 (3), pp. 186-204; Bonnefon, J F, Shariff, A, Rahwan, I., The social dilemma of autonomous vehicles (2016) Science, 352 (6293), pp. 1573-1576; Awad, E, Dsouza, S, Kim, R, The moral machine experiment (2018) Nature, 563 (7729), pp. 59-64; Pereira, L M., (2019) Machine Ethics: From Machine Morals to the Machinery of Morality, , Cham, Swiss: Springer; Cohen, R, Schaekermann, M, Liu, S, Trusted AI and the contribution of trust modeling in multiagent systems (2019) Proceedings of the 18th International Conference on Autonomous Agents and Multi-Agent Systems, pp. 1644-1648. , Montreal, Canada; Henschke, A., Trust and resilient autonomous driving systems (2019) Ethics and Information Technology, 22 (1), pp. 1-12; Sileno, G, Boer, A, van Engers, T., (2018) The role of normware in trustworthy and explainable AI, , arXiv: 1812.02471; Glomsrud, J A, âˆ…degáº£rdstuen, A, Clair, A L S, Trustworthy versus explainable AI in autonomous vessels (2019) Proceedings of the 2nd International Seminar on Safety and Security of Autonomous Vessels, pp. 1-11. , Helsinki, Finland},
correspondence_address1={Li, L.; College of Information Science and Technology/College of Cyber Security, China; email: lilong@guet.edu.cn},
editor={NA},
publisher={Science Press},
issn={02544164},
isbn={NA},
language={Chinese},
abbrev_source_title={Jisuanji Xuebao},
document_type={Review},
source={Scopus},
number={3},
art_number={NA},
funding_details={NA},
coden={JIXUD},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Pinka2021121,
type={ARTICLE},
author={Pinka, R.},
title={Synthetic Deliberation: Can Emulated Imagination Enhance Machine Ethics?},
journal={Minds and Machines},
year={2021},
volume={31},
pages={121-136},
doi={10.1007/s11023-020-09531-w},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087942371&doi=10.1007%2fs11023-020-09531-w&partnerID=40&md5=2ccd2bf08daeb9afaad910d87d439e5e},
affiliation={Department of Philosophy, University of North Carolina at Charlotte, Charlotte, NC, United States},
abstract={Artificial intelligence is becoming increasingly entwined with our daily lives: AIs work as assistants through our phones, control our vehicles, and navigate our vacuums. As these objects become more complex and work within our societies in ways that affect our well-being, there is a growing demand for machine ethicsâ€”we want a guarantee that the various automata in our lives will behave in a way that minimizes the amount of harm they create. Though many technologies exist as moral artifacts (and perhaps moral agents), the development of a truly ethical AI system is highly contentious; theorists have proposed and critiqued countless possibilities for programming these agents to become ethical. Many of these arguments, however, presuppose the possibility that an artificially intelligent system can actually be ethical. In this essay, I will explore a potential path to AI ethics by considering the role of imagination in the deliberative process via the work of John Dewey and his interpreters, showcasing one form of reinforcement learning that mimics imaginative deliberation. With these components in place, I contend that such an artificial agent is capable of something very near ethical behaviorâ€”close enough that we may consider it so. Â© 2020, Springer Nature B.V.},
author_keywords={Artificial intelligence;  Machine ethics;  Machine learning;  Philosophy of technology;  Pragmatism;  STS},
keywords={Intelligent systems;  Reinforcement learning, AI systems;  Artificial agents;  Daily lives;  Deliberative process;  Ethical behavior;  Growing demand;  Moral agents;  Well being, Philosophical aspects},
references={Anderson, M., Anderson, S.L., Guest editorsâ€™ introduction: Machine ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 10-11; Anderson, M., Anderson, S.L., (2011) Machine ethics, , (eds), Cambridge University Press, Cambridge, England; Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., Bonnefon, J.-F., Rahwan, I., The moral machine experiment (2018) Nature, 563 (7729), pp. 59-64; Baum, S.D., Social choice ethics in artificial intelligence (2017) AI and Society; Clark, A., (2003) Natural-born cyborgs: why minds and technologies are made to merge, , Oxford University Press, New York, NY; Dewey, J., (2002) Human nature and conduct, , Dover Publications Inc, Mineola, NY; Dreyfus, H.L., (1992) What computers still canâ€™t do, , MIT Press, Cambridge, MA; Fesmire, S., (2003) John Dewey and moral imagination: Pragmatism in ethics, , Indiana University Press, Bloomington, IN; Johnson, M., (2015) Morality for humans: Ethical understanding from the perspective of cognitive science, , University of Chicago Press, Chicago, IL; Johnson, M., (2020) Deweyâ€™s radical conception of moral cognition, pp. 175-183. , The Oxford Handbook of Dewey, Oxford University Press, New York, NY; McLaren, B.M., Computational models of ethical reasoning: Challenges, initial steps, and future directions (2006) IEEE Intelligent Systems, 21 (4), pp. 29-37; Moor, J.H., The nature, importance, and difficulty of machine ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 18-21; RacaniÃ¨re, S., Weber, T., Reichert, D., Buesing, L., Guez, A., Rezende, D.J., Badia, A.P., Wierstra, D., Imagination-augmented agents for deep reinforcement learning (2017) Advances in Neural Information Processing Systems, , http://papers.nips.cc/paper/7152-imagination-augmented-agents-for-deep-reinforcement-learning.pdf; Rahwan, I., (2017) Ted Talk, , https://www.ted.com/speakers/iyad_rahwan, August, Retrieved from,. Accessed 28 August 2019; Tonkens, R., A challenge for machine ethics (2009) Minds and Machines, 19 (3), pp. 421-438; Verbeek, P.-P., (2011) Moralizing technology: understanding and designing the morality of things, , University of Chicago Press, Chicago, IL},
correspondence_address1={Pinka, R.; Department of Philosophy, United States; email: rpinka@uncc.edu},
editor={NA},
publisher={Springer Science and Business Media B.V.},
issn={09246495},
isbn={NA},
language={English},
abbrev_source_title={Minds Mach},
document_type={Article},
source={Scopus},
number={1},
art_number={NA},
funding_details={NA},
coden={MMACE},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Herzog2021,
type={ARTICLE},
author={Herzog, C.},
title={Three Risks That Caution Against a Premature Implementation of Artificial Moral Agents for Practical and Economical Use},
journal={Science and Engineering Ethics},
year={2021},
volume={27},
pages={NA},
doi={10.1007/s11948-021-00283-z},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099793869&doi=10.1007%2fs11948-021-00283-z&partnerID=40&md5=bcb21ece66d9bbcd56c6b1126eebe7d9},
affiliation={Ethical Innovation Hub, Institute for Electrical Engineering in Medicine, University of LÃ¼beck, Ratzeburger Allee 160, LÃ¼beck, 23562, Germany},
abstract={In the present article, I will advocate caution against developing artificial moral agents (AMAs) based on the notion that the utilization of preliminary forms of AMAs will potentially negatively feed back on the human social system and on human moral thought itself and its valueâ€”e.g., by reinforcing social inequalities, diminishing the breadth of employed ethical arguments and the value of character. While scientific investigations into AMAs pose no direct significant threat, I will argue against their premature utilization for practical and economical use. I will base my arguments on two thought experiments. The first thought experiment deals with the potential to generate a replica of an individualâ€™s moral stances with the purpose to increase, what I term, â€™moral efficiencyâ€™. Hence, as a first risk, an unregulated utilization of premature AMAs in a neoliberal capitalist system is likely to disadvantage those who cannot afford â€™moral replicasâ€™ and further reinforce social inequalities. The second thought experiment deals with the idea of a â€™moral calculatorâ€™. As a second risk, I will argue that, even as a device equally accessible to all and aimed at augmenting human moral deliberation, â€™moral calculatorsâ€™ as preliminary forms of AMAs are likely to diminish the breadth and depth of concepts employed in moral arguments. Again, I base this claim on the idea that the current most dominant economic system rewards increases in productivity. However, increases in efficiency will mostly stem from relying on the outputs of â€™moral calculatorsâ€™ without further scrutiny. Premature AMAs will cover only a limited scope of moral argumentation and, hence, over-reliance on them will narrow human moral thought. In addition and as the third risk, I will argue that an increased disregard of the interior of a moral agent may ensueâ€”a trend that can already be observed in the literature. Â© 2021, The Author(s).},
author_keywords={AI ethics;  Artificial intelligence;  Artificial moral agents;  Machine ethics;  Robot ethics},
keywords={human;  morality, Humans;  Morals},
references={Adamson, G., Havens, J.C., Chatila, R., Designing a value-driven future for ethical autonomous and intelligent systems (2019) Proceedings of the IEEE, 107 (3), pp. 518-525; Allen, C., Varner, G., Zinser, J., Prolegomena to any future artificial moral agent (2000) Journal of Experimental and Theoretical Artificial Intelligence, 12 (3), pp. 251-261; Allen, C., Wallach, W., Smit, I., Why machine ethics? (2006) IEEE Intelligent Systems, 21 (4), pp. 12-17; Anderson, S.L., How machines might help us achieve breakthroughs in ethical theory and inspire us to behave better (2011) Machine Ethics, pp. 524-533. , Anderson M, Anderson SL, chap 30; Angwin, J., Larson, J., Kirchner, L., Mattu, S., Machine Bias: Thereâ€™s software used across the country to predict future criminals (2016) And itâ€™s Biased against Blacks: Propublica.; Arnold, T., Scheutz, M., Against the moral turing test: Accountable design and the moral reasoning of autonomous systems (2016) Ethics and Information Technology, 18 (2), pp. 103-115; Beavers, A.F., Could and should the ought disappear from ethics? (2011) Digital ethics: Research and practice, pp. 197-209. , Heider D, Masanari A, (eds), Peter Lang, New York; Beavers, A.F., Moral machines and the threat of ethical nihilism (2012) Robot ethics: The ethical and social implications of robotics, 2009, pp. 333-344; Bryson, J.J., Patiency is not a virtue: The design of intelligent systems and systems of ethics (2018) Ethics and Information Technology, 20 (1), pp. 15-26; Caliskan, A., Bryson, J.J., Narayanan, A., Semantics derived automatically from language corpora contain human-like biases (2017) Science, 356 (6334), pp. 183-186; Cave, S.J., Nyrup, R., Vold, K., Weller, A., Motivations and risks of machine ethics (2018) Proceedings of the IEEE, 107 (3). , https://doi.org/10.1109/JPROC.2018.2865996; Chen, B.X., Metz, C., (2019) Googleâ€™s Duplex Uses A.I. to Mimic Humans (Sometimes); Floridi, L., Sanders, J., On the morality of artificial agents (2004) Minds and Machines, 14 (3), pp. 349-379; Gerdes, A., Ohrstrom, P., Preliminary Reflections on a Moral Turing Test (2013) ETHICOMP 2013 - the Possibilities of Ethical ICT, Print & Sign University of Southern Denmark, pp. 167-175. , Bynum TW, Fleisman W, Gerdes A, Nielsen GM, Rogersen S; Gips, J., Towards the Ethical Robot (1991) Android Epistemology (May), p. 13; Giubilini, A., Savulescu, J., The Artificial Moral Advisor - The â€œIdeal Observerâ€ Meets Artificial Intelligence (2018) Philosophy & Technology, 31 (2), pp. 169-188; Goddard, K., Roudsari, A., Wyatt, J.C., Automation Bias - A Hidden Issue for Clinical Decision Support System Use (2011) International Perspectives in Health Informatics Studies in Health Technology and Informatics, 164, pp. 17-22; Gordon, J.S., Building Moral Robots: Ethical Pitfalls and Challenges (2020) Science and Engineering Ethics, 26 (1), pp. 141-157; He, E., Bertallee, C., Jones, S., Lyle, L., Schawbel, J., Meister, D., (2019) Ai@Work Global Study 2019: From Fear to Enthusiasm - Artificial Intelligence is Winning More Hearts and Minds in the Workplace, , . Tech. rep., Oracle; Future Workplace; (2019), IBM Watson Website; Koene, A., Algorithmic Bias (2017) IEEE Technology and Society Magazine, pp. 31-32. , (June); Kraut, R., Aristotleâ€™s ethics (2018) The Stanford Encyclopedia of Philosophy, , Zalta EN (ed), 2018th edn; Kruse, K., (2018) 2023 Your Boss Will Be a Robot (And You Will Love Her); Kukita, M., The difference between artificial intelligence and artificial morality (2015) Applied Ethics: Security, pp. 27-37. , Center for Applied Ethics and Philosophy, Hokkaido University, Sapporo, Japan; Lara, F., Deckers, J., Artificial intelligence as a socratic assistant for moral enhancement (2019) Neuroethics, , https://doi.org/10.1007/s12152-019-09401-y; Mason, E., Value pluralism (2018) The Stanford Encyclopedia of Philosophy, Spring, , E. N. Zalta, Metaphysics Research Lab, Stanford University; Moor, J., The Nature, Importance, and Difficulty of Machine Ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 18-21; Novita, N.R., (2019) Can AI Or Robots Replace Managers?; Pizarro, D.A., Bloom, P., The Intelligence of the Moral Intuitions: Comment on Haidt (2001) (2003) Psychological Review, 110 (1), pp. 193-196; Ruthven, K., Calculators in the Mathematics Curriculum: the Scope of Personal Computational Technology (1996) International Handbook of Mathematics Education, Springer, Netherlands, Dordrecht, chap, 12, pp. 435-468; Sauer, H., Educated intuitions. Automaticity and rationality in moral judgement (2012) Philosophical Explorations, 15 (3), pp. 255-275; Scheutz, M., The need for moral competency in autonomous agent architectures (2016) Fundamental Issues of Artificial Intelligence, pp. 517-527. , https://doi.org/10.1007/978-3-319-26485-1_30, MÃ¼ller VC, Springer International Publishing, Cham; Schramowski, P., Turan, C., Jentzsch, S., Rothkopf, C., Kersting, K., The moral choice machine (2020) Frontiers in Artificial Intelligence, 3. , &, (,).,., https://doi.org/10.3389/frai.2020.00036; Simon, J., The entanglement of trust and knowledge on the Web (2010) Ethics and Information Technology, 12 (4), pp. 343-355; Strickland, E., IBM Watson, heal thyself: How IBM overpromised and underdelivered on AI health care (2019) IEEE Spectrum, 56 (4), pp. 24-31; Tonkens, R., A challenge for machine ethics (2009) Minds and Machines, 19 (3), pp. 421-438; Vallor, S., Moral Deskilling and Upskilling in a New Machine Age: Reflections on the Ambiguous Future of Character (2015) Philosophy & Technology, 28 (1), pp. 107-124; Wallach, W., Allen, C., Moral machines: Contradiction in terms, or abdication of human responsibility? (2011) Robot Ethics: The Ethical and Social Implications of Robotics; Weinberg, J., (2020) Philosophers on GPT-3 (Updated with Replies by GPT-3); (2017), P7006-Standard for Personal Data Artificial Intelligence (AI) Agent; Wiegel, V., Building blocks for artificial moral agents (2006) Artificial Life X: Proceedings of the Tenth International Conference on the Simulation and Synthesis of Living Systems, , . In, September; van Wynsberghe, A., Robbins, S., Critiquing the Reasons for Making Artificial Moral Agents (2019) Science and Engineering Ethics, 25 (3), pp. 719-735; Yu, H., Shen, Z., Miao, C., Leung, C., Lesser, V.R., Yang, Q., Building ethics into artificial intelligence (2018) IJCAI International Joint Conference on Artificial Intelligence, pp. 5527-5533. , 2018-July},
correspondence_address1={Herzog, C.; Ethical Innovation Hub, Ratzeburger Allee 160, Germany; email: christian.herzog@uni-luebeck.de},
editor={NA},
publisher={Springer Science and Business Media B.V.},
issn={13533452},
isbn={NA},
language={English},
abbrev_source_title={Sci. Eng. Ethics},
document_type={Article},
source={Scopus},
number={1},
art_number={3},
funding_details={NA},
coden={NA},
pubmed_id={33496885},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Stenseke2021,
type={ARTICLE},
author={Stenseke, J.},
title={Artificial virtuous agents: from theory to machine implementation},
journal={AI and Society},
year={2021},
volume={NA},
pages={NA},
doi={10.1007/s00146-021-01325-7},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121382065&doi=10.1007%2fs00146-021-01325-7&partnerID=40&md5=e92dacd49929e5a1cb3a28ed3433dbca},
affiliation={Department of Philosophy, Lund University, Lund, Sweden},
abstract={Virtue ethics has many times been suggested as a promising recipe for the construction of artificial moral agents due to its emphasis on moral character and learning. However, given the complex nature of the theory, hardly any work has de facto attempted to implement the core tenets of virtue ethics in moral machines. The main goal of this paper is to demonstrate how virtue ethics can be taken all the way from theory to machine implementation. To achieve this goal, we critically explore the possibilities and challenges for virtue ethics from a computational perspective. Drawing on previous conceptual and technical work, we outline a version of artificial virtue based on moral functionalism, connectionist bottomâ€“up learning, and eudaimonic reward. We then describe how core features of the outlined theory can be interpreted in terms of functionality, which in turn informs the design of components necessary for virtuous cognition. Finally, we present a comprehensive framework for the technical development of artificial virtuous agents and discuss how they can be implemented in moral environments. Â© 2021, The Author(s).},
author_keywords={Artificial intelligence;  Artificial moral agent;  Artificial neural networks;  Connectionism;  Machine ethics;  Moral machine;  Virtue ethics},
keywords={Computation theory;  Ethical technology, Artificial moral agent;  Bottom up;  Complex nature;  Connectionism;  Core features;  Machine ethic;  Moral agents;  Moral machine;  Technical work;  Virtue ethics, Neural networks},
references={Abel, D., Macglashan, J., Littman, M.L., Reinforcement learning as a framework for ethical decision making (2016) AAAI Workshop: AI, Ethics, and Society, 2016, p. 02. , Phoenix, AZ; Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., ManÃ©, D., (2016) Concrete Problems in AI Safety Arxiv Preprint, , https://arXiv.org/160606565; Anderson, M., Anderson, S.L., ETHEL: TOWARD a principled ethical eldercare system (2008) AAAI Fall Symposium: AI in Eldercare: New Solutions to Old Problems, p. 02; Anderson, M., Anderson, S.L., (2011) Machine ethics, , Cambridge University Press; Annas, J., (2011) Intelligent virtue, , Oxford University Press; Anscombe, G.E.M., Modern moral philosophy (1958) Philosophy, 33, pp. 1-19; Arkin, R.C., Governing lethal behavior: Embedding ethics in a hybrid deliberative/hybrid robot architecture (2007) Report GIT-GVU-07-11, , Georgia Institute of Technologyâ€™s GVU, Atlanta; Arnold, T., Scheutz, M., Against the moral Turing test: accountable design and the moral reasoning of autonomous systems (2016) Ethics Inf Technol, 18, pp. 103-115; Axelrod, R., Hamilton, W.D., The evolution of cooperation (1981) Science, 211, pp. 1390-1396; BÃ¤ck, T., Fogel, D.B., Michalewicz, Z., Handbook of evolutionary computation (1997) Release, 97, p. B1; Bansal, T., Pachocki, J., Sidor, S., Sutskever, I., Mordatch, I., (2017) . Arxiv Preprint, , https://arXiv.org/171003748, Emergent complexity via multi-agent competition; Bauer, W.A., Virtuous vs utilitarian artificial moral agents (2020) AI Soc, 35, pp. 263-271; Behdadi, D., Munthe, C., A normative approach to artificial moral agency (2020) Mind Mach, 30, pp. 195-218; Bejczy, I.P., (2011) The cardinal virtues in the middle ages: a study in moral thought from the fourth to the fourteenth century, 202. , Brill; Berberich, N., Diepold, K., (2018) The Virtuous Machine-Old Ethics for New Technology? Arxiv Preprint, , https://arXiv.org/180610322; Berner, C., (2019) Dota 2 with Large Scale Deep Reinforcement Learning Arxiv Preprint, , https://arXiv.org/191206680; Besold, T.R., Zaadnoordijk, L., Vernon, D., Feeling functional: a formal account of artificial phenomenology (2021) J Artif Intell Conscious, 8, pp. 147-160; Blackburn, S., (1992) Through Thick and Thin, pp. 284-299. , In, Proceedings of the Aristotelian Society, vol suppl; Blackburn, S., (1998) Ruling passions, , Oxford University Press, Oxford; Bostrom, N., (2014) Superintelligence: paths, dangers, , Oxford University Press, Strategies; Bostrom, N., (2020) Ethical issues in advanced artificial intelligence, , Routledge; Bringsjord, S., Ethical robots: the future can heed us (2008) AI Soc, 22, pp. 539-550; Bryson, J.J., Robots should be slaves close engagements with artificial companions: Key social (2010) Psychological, Ethical and Design Issues, 8, pp. 63-74; Cammarata, N., Carter, S., Goh, G., Olah, C., Petrov, M., Schubert, L., Thread: circuits (2020) Distill, 5; Casebeer, W.D., Moral cognition and its neural constituents (2003) Nat Rev Neurosci, 4, pp. 840-846; Cave, S., The problem with intelligence: its value-laden history and the future of AI (2020) Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pp. 29-35; Cervantes, J.-A., RodrÃ­guez, L.-F., LÃ³pez, S., Ramos, F., Robles, F., Autonomous agents and ethical decision-making (2016) Cogn Comput, 8, pp. 278-296; Cervantes, J.-A., LÃ³pez, S., RodrÃ­guez, L.-F., Cervantes, S., Cervantes, F., Ramos, F., Artificial moral agents: a survey of the current status (2020) Sci Eng Ethics, 26, pp. 501-532; Champagne, M., Tonkens, R., Bridging the responsibility gap in automated warfare (2015) Philos Technol, 28, pp. 125-137; Churchland, P.S., Feeling reasons (1996) Neurobiology of decision-making, pp. 181-199. , Springer; Coeckelbergh, M., Moral appearances: emotions, robots, and human morality (2010) Ethics Inform Technol, 12, pp. 235-241; Coleman, K.G., Android arete: toward a virtue ethic for computational agents (2001) Ethics Inf Technol, 3, pp. 247-265; Crisp, R., Slote, M.A., (1997) Virtue ethics, , Oxford University Press, Oxford; Danaher, J., Welcoming robots into the moral circle: a defence of ethical behaviourism (2020) Sci Eng Ethics, 26, pp. 2023-2049; Danielson, P., (2002) Artificial morality: virtuous robots for virtual games, , Routledge; Dehghani, M., Tomai, E., Forbus, K.D., Klenk, M., (2008) An Integrated Reasoning Approach to Moral Decision-Making, pp. 1280-1286. , AAAI; Demoss, D., Aristotle, connectionism, and the morally excellent brain (1998) The Paideia Archive: Twentieth World Congress of Philosophy, pp. 13-20; Deng, L., Yu, D., Deep learning: methods and applications (2014) Found Trends Signal Process, 7, pp. 197-387; Dennett, D.C., (1989) The intentional stance, , MIT Press; Devettere, R.J., (2002) Introduction to virtue ethics: insights of the ancient Greeks, , Georgetown University Press; Dreyfus, S.E., The five-stage model of adult skill acquisition (2004) Bull Sci Technol Soc, 24, pp. 177-181; Edmonds, B., Meyer, R., (2015) Simulating social complexity, , Springer; Feldmanhall, O., Mobbs, D., A neural network for moral decision making (2015) Brain mapping: an encyclopedic reference, , Toga AW, Lieberman MD, (eds), Elsevier, Oxford; Flanagan, O., (2009) The really hard problem: meaning in a material world, , MIT Press; Flanagan, O., It takes a metaphysics: raising virtuous buddhists (2015) Snow, 2015, pp. 171-196; Floridi, L., Cowls, J., A unified framework of five principles for AI in society Issue 11 (2019) Summer, 2019, p. 1; Floridi, L., Sanders, J.W., On the morality of artificial agents (2004) Mind Mach, 14, pp. 349-379; Frankfurt, H.G., Alternate possibilities and moral responsibility (1969) J Philos, 66, pp. 829-839; Gamez, P., Shank, D.B., Arnold, C., North, M., Artificial virtue: the machine question and perceptions of moral character in artificial moral agents (2020) AI Soc, 35, pp. 795-809; Geertz, C., (1973) The interpretation of cultures, 5019. , Basic books; George, M.I., What moral character is and is not (2017) Linacre Quar, 84, pp. 261-274; Gerdes, A., Ã˜hrstrÃ¸m, P., Issues in robot ethics seen through the lens of a moral Turing test (2015) J Inform Commun Ethics Soc, 13, pp. 98-109; Gilligan, C., (1993) In a different voice: psychological theory and womenâ€™s development, , Harvard University Press; Gips, J., Towards the Ethical Robot (1995) Android Epistemology, pp. 243-252. , Ford K, Glymour C, Hayes P, (eds), MIT Press, Cambridge MA; Goldman, A.I., The psychology of folk (1993) Psychol Behav Brain Sci, 16, pp. 15-28; Govindarajulu, N.S., Bringsjord, S., Ghosh, R., Sarathy, V., Toward the engineering of virtuous machines. In: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and (2019) Society, pp. 29-35; Guarini, M., Particularism and the classification and reclassification of moral cases (2006) IEEE Intell Syst, 21, pp. 22-28; Guarini, M., Case classification, similarities, spaces of reasons, and coherences (2013) Coherence: Insights from Philosophy, Jurisprudence and Artificial Intelligence, pp. 187-201. , Springer; Guarini, M., Moral case classification and the nonlocality of reasons (2013) Topoi, 32, pp. 267-289; Gunkel, D.J., A vindication of the rights of machines (2014) Philos Technol, 27, pp. 113-132; Gunkel, D.J., (2018) Robot rights, , MIT Press, London; Guthrie, W.K.C., (1990) A history of Greek philosophy: Aristotle: an encounter, 6. , Cambridge University Press; Hagendorff, T., The ethics of AI ethics: an evaluation of guidelines (2020) Mind Mach, 30, pp. 99-120; Hare, R.M., Moral thinking: Its levels, method, and point. Clarendon Press (1981) Oxford, , Oxford University Press, New York; Hare, R.M., (1991) The language of morals, 77. , Oxford Paperbacks, Oxford; Haybron, D.M., Moral monsters and saints (2002) Monist, 85, pp. 260-284; HellstrÃ¶m, T., On the moral responsibility of military robots (2013) Ethics Inf Technol, 15, pp. 99-107; Himma, K.E., Artificial agency, consciousness, and the criteria for moral agency: what properties must an artificial agent have to be a moral agent? (2009) Ethics Inf Technol, 11, pp. 19-29; Hooker, B., (2002) Ideal code, real world: a rule-consequentialist theory of morality, , Oxford University Press; Howard, D., Muntean, I., Artificial moral cognition: moral functionalism and autonomous moral agency (2017) Philosophy and computing, pp. 121-159. , Springer; Hursthouse, R., (1999) On virtue ethics, , OUP Oxford; Jackson, F., Pettit, P., Moral functionalism and moral motivation (1995) Philos Quar, 45, pp. 20-40; Johnson, M., There is no moral faculty (2012) Philos Psychol, 25, pp. 409-432; Johnson, D.G., Miller, K.W., Un-making artificial moral agents (2008) Ethics Inf Technol, 10, pp. 123-133; Kant, I., (2008) Groundwork for the metaphysics of morals, , Yale University Press; Kitcher, P., (2011) The ethical project, , Harvard University Press; Kohlberg, L., Hersh, R.H., Moral development: a review of the theory (1977) Theory Pract, 16, pp. 53-59; Leben, D., (2018) Ethics for robots: how to design a moral algorithm, , Routledge; Linda, J., The functional morality of robots (2010) Int J Technoeth (IJT), 1, pp. 65-73; MacIntyre, A., (2013) After virtue, , A&C Black; McCulloch, W.S., Pitts, W., A logical calculus of the ideas immanent in nervous activity (1943) Bull Math Biophys, 5, pp. 115-133; McDowell, J., Virtue and reason (1979) Monist, 62, pp. 331-350; McLaren, B., Lessons in machine ethics from the perspective of two computational models of ethical reasoning (2005) In: 2005 AAAI Fall Symposium on Machine Ethics; McLaren, B.M., Computational models of ethical reasoning: challenges, initial steps, and future directions (2006) IEEE Intell Syst, 21, pp. 29-37; Medler, D.A., A brief history of connectionism Neural (1998) Comput Surv, 1, pp. 18-72; Metzinger, T., Artificial suffering: an argument for a global moratorium on synthetic phenomenology (2021) J Artif Intell Conscious, 8, pp. 43-66; Miikkulainen, R., Evolving deep neural networks (2019) Artificial intelligence in the age of neural networks and brain computing, pp. 293-312. , Elsevier; Mittelstadt, B., Principles alone cannot guarantee ethical AI (2019) Nat Mach Intell, 1, pp. 501-507; Mostafa, S.A., Ahmad, M.S., Mustapha, A., Adjustable autonomy: a systematic literature review (2019) Artif Intell Rev, 51, pp. 149-186; Ng, A.Y., Russell, S.J., (2000) Algorithms for Inverse Reinforcement Learning, p. 2. , In, Icml; Nussbaum, M.C., Non-relative virtues: an Aristotelian approach (1988) Midwest Stud Philos, 13, pp. 32-53; Olden, J.D., Jackson, D.A., Illuminating the â€œblack boxâ€: a randomization approach for understanding variable contributions in artificial neural networks (2002) Ecol Model, 154, pp. 135-150; Perrett, R.W., Pettigrove, G., Hindu virtue ethics (2015) The Routledge companion to virtue ethics, pp. 75-86. , Routledge; Piaget, J., (1965) The moral development, p. 1. , Free Press, New York; Purves, D., Jenkins, R., Strawser, B.J., Autonomous machines, moral judgment, and acting for the right reasons (2015) Ethical Theory Moral Pract, 18, pp. 851-872; Putnam, H., (2002) The collapse of the fact/value dichotomy and other essays, , Harvard University Press; Radtke, R.R., Role morality in the accounting professionâ€”how do we compare to physicians and attorneys? (2008) J Bus Ethics, 79, pp. 279-297; Rest, J.R., Narvaez, D., Thoma, S.J., Bebeau, M.J., DIT2: devising and testing a revised instrument of moral judgment (1999) J Educ Psychol, 91, p. 644; Russell, S., Norvig, P., (2020) Artificial Intelligence: a Modern Introduction, , http://aima.cs.berkeley.edu/newchap00.pdf, Pearson; Russell, S., (2019) Human compatible: artificial intelligence and the problem of control, , Penguin; Senior, A.W., Improved protein structure prediction using potentials from deep learning (2020) Nature, 577, pp. 706-710; Sharkey, A., Can robots be responsible moral agents? And why should we care? (2017) Connect Sci, 29, pp. 210-216; Shen, S., The curious case of human-robot morality (2011) Proceedings of the 6Th International Conference on Human-Robot Interaction, , Paper presented at the, Lausanne, Switzerland; Silver, D., A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play (2018) Science, 362, pp. 1140-1144; Singer, P., (2011) Practical ethics, , Cambridge University Press; Smart, R.N., Negative utilitarianism (1958) Mind, 67, pp. 542-543; Sparrow, R., Killer robots (2007) J Appl Philos, 24, pp. 62-77; Sparrow, R., Why machines cannot be moral (2021) AI Soc, 36, pp. 1-9; Stanley, K.O., Miikkulainen, R., Efficient evolution of neural network topologies (2002) Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No. 02TH8600). IEEE, pp. 1757-1762; Tangney, J.P., Stuewig, J., Mashek, D.J., Moral emotions and moral behavior (2007) Annu Rev Psychol, 58, pp. 345-372; Tegmark, M., (2017) Life 3.0: being human in the age of artificial intelligence, , Knopf; Teper, R., Inzlicht, M., Page-Gould, E., Are we more moral than we think? Exploring the role of affect in moral behavior and moral forecasting (2011) Psychol Sci, 22, pp. 553-558; Thornton, S.M., Pan, S., Erlien, S.M., Gerdes, J.C., Incorporating ethical considerations into automated vehicle control (2016) IEEE Trans Intell Transp Syst, 18, pp. 1429-1439; Tolmeijer, S., Kneer, M., Sarasua, C., Christen, M., Bernstein, A., Implementations in machine ethics: a survey (2020) ACM Comput Surv (CSUR), 53, pp. 1-38; Tonkens, R., A challenge for machine ethics (2009) Mind Mach, 19, p. 421; Tonkens, R., Out of character: on the creation of virtuous machines (2012) Ethics Inf Technol, 14, pp. 137-149; Vallor, S., (2016) Technology and the virtues: a philosophical guide to a future worth wanting, , Oxford University Press; Van Wynsberghe, A., Robbins, S., Ethicist as designer: a pragmatic approach to ethics in the lab (2014) Sci Eng Ethics, 20, pp. 947-961; Van Wynsberghe, A., Robbins, S., Critiquing the reasons for making artificial moral agents (2019) Sci Eng Ethics, 25, pp. 719-735; Wallach, W., Allen, C., (2008) Moral machines: Teaching robots right from wrong, , Oxford University Press; Wang, J.X., Hughes, E., Fernando, C., Czarnecki, W.M., DuÃ©Ã±ez-GuzmÃ¡n, E.A., Leibo, J.Z., (2018) Evolving Intrinsic Motivations for Altruistic Behavior Arxiv Preprint, , https://arXiv.org/181105931; Williams, B., (1981) Moral luck: philosophical papers 1973â€“1980, , Cambridge University Press; Williams, B., (2006) Ethics and the limits of philosophy, , Routledge; Winfield, A.F., Blum, C., Liu, W., Towards an ethical robot: internal models, consequences and ethical action selection (2014) Conference towards autonomous robotic systems, pp. 85-96. , Springer; Yampolskiy, R.V., Artificial intelligence safety engineering: why machine ethics is a wrong approach (2013) Philosophy and theory of artificial intelligence, pp. 389-396. , Springer; Yu, J., (2013) The ethics of confucius and Aristotle: mirrors of virtue, 7. , Routledge; Zagzebski, L., Exemplarist virtue theory (2010) Metaphilosophy, 41, pp. 41-57},
correspondence_address1={Stenseke, J.; Department of Philosophy, Sweden; email: jakob.stenseke@fil.lu.se},
editor={NA},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={09515666},
isbn={NA},
language={English},
abbrev_source_title={AI Soc.},
document_type={Article},
source={Scopus},
number={NA},
art_number={NA},
funding_details={Marcus och Amalia Wallenbergs minnesfondMarcus och Amalia Wallenbergs minnesfond,Â MAW funding_textÂ 1={This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Programâ€”Humanities and Society (WASP-HS) funded by the Marianne and Marcus Wallenberg Foundation and the Marcus and Amalia Wallenberg Foundation. The author is grateful to his colleagues at the Department of Philosophy and the Department Cognitive Science at Lund University for insightful discussions and feedback on previous versions of the paper, in particular Frits GÃ¥vertsson for useful input on virtue ethics, Amandus Krantz and Christian Balkenius for technical advice, and BjÃ¶rn Petersson, Ylva von Gerber, and Jiwon Kim for reviewing an early draft of the paper.},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Anderson2021177,
type={ARTICLE},
author={Anderson, M. and Anderson, S.L. and Gounaris, G. and Kosteletos, G.},
title={Towards moral machines: A discussion with michael anderson and susan leigh anderson},
journal={Conatus - Journal of Philosophy},
year={2021},
volume={6},
pages={177-202},
doi={10.12681/cjp.26832},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116451995&doi=10.12681%2fcjp.26832&partnerID=40&md5=61e1e5ef8f96b5c6c75f0785aeb343fc},
affiliation={University of Hartford, United States; University of Connecticut, United States; National and Kapodistrian, University of Athens, Greece},
abstract={At the turn of the 21st century, Susan Leigh Anderson and Michael Anderson conceived and introduced the Machine Ethics research program, that aimed to highlight the requirements under which autonomous artificial intelligence (AI) systems could demonstrate ethical behavior guided by moral values, and at the same time to show that these values, as well as ethics in general, can be representable and computable. Today, the interaction between humans and AI entities is already part of our everyday lives, in the near future it is expected to play a key role in scientific research, medical practice, public administration, education and other fields of civic life. In view of this, the debate over the ethical behavior of machines is more crucial than ever and the search for answers, directions and regulations is imperative at an academic, institutional as well as at a technical level. Our discussion with the two inspirers and originators of Machine Ethics highlights the epistemological, metaphysical and ethical questions arising by this project, as well as the realistic and pragmatic demands that dominate artificial intelligence and robotics research programs. Most of all, however, it sheds light upon the contribution of Susan and Michael Anderson regarding the introduction and undertaking of a main objective related to the creation of ethical autonomous agents, that will not be based on the â€œimperfectâ€ patterns of human behavior, or on preloaded hierarchical laws and human-centric values. Â© 2021 George Kosteletos, Alkis Gounaris, Michael Anderson, Susan Leigh Anderson.},
author_keywords={AI Ethics;  Artificial Moral Agents;  Computation of Bio-Medical Ethics;  Ethical Machines;  Machine Ethics;  Moral Status of Robots;  Philosophy of Artificial Intelligence},
keywords={NA},
references={Aldinhas, Ferreira Maria, Sequeira, Joao Silva, Virk, Gurvinder Singh, Osman, Mohammad Tokhi, Kadar, Ender E., (2019) Robotics and WellÂ¬being, , eds. Cham: Springer; Allen, Colin, Gary, Varner, Jason, Zinser, Prolegomena to Any Future Artificial Moral Agent (2000) Journal of Experimental and Theoretical Artificial Intelligence, 12, pp. 151-261; Anderson, Michael, Anderson, Suzan Leigh, (2011) Machine Ethics, , eds. New York and Cambridge: Cambridge University Press; Anderson, Michael, Anderson, Suzan Leigh, A Prima Facie Duty Approach to Machine Ethics: Machine Learning of Features of Ethical Dilemmas, Prima Facie Duties, and Decision Principles through a Dialogue with Ethicists (2011) Machine Ethics, pp. 476-492. , edited by Michael Anderson, and Suzan Leigh Anderson, New York and Cambridge: Cambridge University Press; Anderson, Michael, Anderson, Suzan Leigh, ETHEL: Toward a Principled Ethical Eldercare System (2008) Proceedings of the AAAI Fall Symposium: New Solutions to Old Problems, , Technical Report FS-08-02. Arlington, VA; Anderson, Michael, Anderson, Suzan Leigh, Guest Editorsâ€™ Introduction: Machine Ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 10-11; Anderson, Michael, Anderson, Suzan Leigh, Machine Ethics: Creating an Ethical Intelligent Agent (2007) AI Magazine, 28 (4), pp. 15-26; Anderson, Michael, Anderson, Suzan Leigh, MedEthEx: A Prototype Medical Ethics Advisor (2006) Proceedings of the 21st National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference, pp. 1759-1765. , Boston, MA: AAAI Press; Anderson, Michael, Anderson, Suzan Leigh, Robot Be Good (2010) Scientific American, 303 (4), pp. 72-77; Anderson, Michael, Anderson, Suzan Leigh, The Status of Machine Ethics: A Report from the AAAI Symposium (2007) Minds & Machines, 17, pp. 1-10; Anderson, Michael, Anderson, Suzan Leigh, Toward Ensuring Ethical Behavior from Autonomous Systems: A Case-supported Principle-based Paradigm (2015) Industrial Robot, 42 (4), pp. 324-331; Anderson, Michael, Anderson, Suzan Leigh, Armen, Chris, (2005) Machine Ethics: Papers form AAAI Fall Symposium, , https://www.aaai.org/Library/Symposia/Fall/fs05-06.php, eds. 2005. Technical Report FS-05-06. Menlo Park, CA: Association for the Advancement of Artificial Intelligence; Anderson, Michael, Anderson, Suzan Leigh, Armen, Chris, An Approach to Computing Ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 65-63; Anderson, Michael, Anderson, Suzan Leigh, Armen, Chris, Toward Machine Ethics: Implementing Two Action-Based Ethical Theories (2005) Machine Ethics, Papers form AAAI Fall Symposium, 2005, , edited by Michael Anderson, Suzan Leigh Anderson, and Chris Armen, Technical Report FS 05-06. Menlo Park, CA: Association for the Advancement of Artificial Intelligence; Anderson, Michael, Anderson, Suzan Leigh, Armen, Chris, Towards Machine Ethics (2004) Proceedings of the AAAI-04 Workshop on Agent Organizations: Theory and Practice, pp. 53-59. , San Jose, CA; Anderson, Suzan Leigh, Anderson, Michael, Towards a Principle-Based Healthcare Agent (2015) Machine Medical Ethics, pp. 67-77. , edited by S. van Rysewyk, and M. Pontier, Cham: Springer; Anderson, Suzan Leigh, Asimovâ€™s â€˜Three Laws of Roboticsâ€™ and Machine Metaethics (2007) AI and Society, 22, pp. 477-493; Anderson, Suzan Leigh, Machine Metaethics (2011) Machine Ethics, pp. 21-27. , edited by Michael Anderson, and Suzan Leigh Anderson, New York and Cambridge: Cambridge University Press; Anderson, Suzan Leigh, Philosophical Concerns with Machine Ethics (2011) Machine Ethics, pp. 162-167. , edited by Michael Anderson, and Suzan Leigh Anderson, New York and Cambridge: Cambridge University Press; Anderson, Suzan Leigh, The Unacceptability of Asimovâ€™s Three Laws of Robotics as a Basis for Machine Ethics (2011) Machine Ethics, pp. 285-296. , edited by Michael Anderson, and Suzan Leigh Anderson, New York and Cambridge: Cambridge University Press; Asimov, Isaac, The Bicentennial Man (1984) Philosophy and Science Fiction, pp. 183-216. , edited by Michael Phillips, Buffalo, NY: Prometheus Books; Awad, Edmond, Anderson, Michael, Anderson, Suzan Leigh, Liao, Beishui, An Approach for Combining Ethical Principles with Public Opinion to Guide Public Policy (2020) Artificial Intelligence, 287, p. 103349. , Article; Awad, Edmond, Dsouza, Sohan, Shariff, Azim, Rahwan, Iyad, Bonnefon, Jean-Francois, Universals and Variations in Moral Decisions Made in 42 Countries by 70,000 Participants (2020) Proceedings of the National Academy of Sciences, 117 (5), pp. 2332-2337; Awad, Edmond, Dsouza, Sohan, Kim, Richard, Schulz, Jonathan, Henrich, Joseph, Shariff, Azim, Bonnefon, Jean-Frangois, Rahwan, Iyad, The Moral Machine Experiment (2018) Nature, 563 (7729), pp. 59-64; Beauchamp, Tom Lamar, Childress, James Franklin, (1979) Principles of Biomedical Ethics, , Oxford, UK: Oxford University Press; Bentham, Jeremy, (1789) An Introduction to the Principles of Morals and Legislation, , Edited by J. Burns, and H. Hart. Oxford: Clarendon Press; Bonnefon, Jean-Francois, Shariff, Azim, Rahwan, Iyad, The Social Dilemma of Autonomous Vehicles (2016) Science, 352 (6293), pp. 1573-1576; Bostrom, Nick, Existential Risk Prevention as Global Priority (2013) Global Policy, 4 (1), pp. 15-31; Bostrom, Nick, (2014) Superintelligence: Paths, Dangers, Strategies, , Oxford: Oxford University Press; Clarke, Roger, Asimovâ€™s Laws of Robotics: Implications for Information Technology. Part I (1993) Computer, 26 (12), pp. 53-61; Clarke, Roger, Asimovâ€™s Laws of Robotics: Implications for Information Technology. Part II (1994) Computer, 27 (1), pp. 57-66; Clifford, Catherine, Musk, Elon, Mark my Words - A.I. is far more Dangerous than Nukes (2018) CNBC, , https://www.cnbc.com/2018/03/13/elon-musk-at-sxsw-a-i-is-more-dangerous-than-nuclear-weapons.html, March 13; Davis, Martin, (1958) Computability and Unsolvability, , New York: McGraw-Hill; Dennett, Daniel, Cognitive Wheels: The Frame Problem of AI (1984) Minds, Machines and Evolution: Philosophical Studies, pp. 129-152. , edited by C. Hoockway, Cambridge: Cambridge University Press; Dennett, Daniel, When Hal Kills, Whoâ€™s to Blame? Computer Ethics (1997) Halâ€™s Legacy: 2001â€™s Computer as Dream and Reality, pp. 351-365. , edited by David G. Stork, Cambridge, MA: MIT Press; Dennett, Daniel, (1978) Brainstorms: Philosophical Essays on Mind and Psychology, , Cambridge, MA: MIT Press; Dreyfus, Hubert Lederer, (1992) What Computers Still Canâ€™t Do: A Critique of Artificial Reason, , Cambridge, MA: MIT Press; Feenberg, Andrew, Subversive Rationalization: Technology, Power, and Democracy (1995) Technology and the Politics of Knowledge, pp. 3-11. , edited by Andrew Feenberg, and Alastair Hannay, Bloomington and Indianapolis: Indiana University Press; Feenberg, Andrew, (1999) Questioning Technology, , London, New York: Routledge; Floridi, Luciano, Sanders, J. W., On the Morality of Artificial Agents (2004) Minds and Machines, 14, pp. 349-379; Fodor, Jerry Alan, (1983) The Modularity of Mind, , Cambridge, MA: MIT Press; Gounaris, Alkis, Kosteletos, George, Licensed to Kill: Autonomous Weapons as Persons and Moral Agents (2020) Personhood, 2, pp. 137-189. , edited by Dragan Prole, and Goran Rujievic, Hellenic-Serbian Philosophical Dialogue Series, Novi Sad: The NKUA Applied Philosophy Research Lab Press; Gunkel, David, (2012) The Machine Question: Critical Perspectives on AI, Robots and Ethics, , Cambridge, MA: MIT Press; Hao, Karen, Should a Self-driving Car Kill the Baby or the Grandma? Depends on where Youâ€™re from (2018) MIT Technology Review, , https://www.technologyreview.com/2018/10/24/139313/a-global-ethics-study-aims-to-help-ai-solve-the-self-driving-trolley-problem/, October 14; Harari, Yuval Noah, (2018) 21 Lessons for the 21st Century, , New York: Spiegel & Grau; Harris, Ricki, Elon Musk: Humanity Is a Kind of â€˜Biological Boot Loaderâ€™ for AI (2019) Wired, , https://www.wired.com/story/elon-musk-humanity-biological-boot-loader-ai/, January 9; Hobbes, Thomas, (1904) Leviathan, or The Matter, Forme and Power of a Commonwealth Ecclesiastical and Civil, , Edited by A. R. Waller. Cambridge: Cambridge University Press; Hoffmann, Christian Hugo, Hahn, Benjamin, Decentered Ethics in the Machine Era and Guidance for AI Regulation (2009) AI & Society, 35 (3), pp. 635-644; Kant, Immanuel, (2015) Critique of Practical Reason, , Translated by Mary Gregor. Cambridge: Cambridge University Press; Kant, Immanuel, (1963) Lectures on Ethics, , Translated by L. Infield. New York: Harper & Row; Kant, Immanuel, (2002) The Groundwork for the Metaphysics of Morals, , Translated by Allen W. Wood. New Haven and London: Yale University Press; Kleene, Stephen Cole, (1952) Introduction to Metamathematics, , Amsterdam: North Holland; Leibniz, Gottfried Wilhelm, Principles of Nature and Grace, Based on Reason (1989) Gottfried Wilhelm Leibniz, Philosophical Papers and Letters, , edited by Leroy E. Loemker. Dordrecht: Springer; Leibniz, Gottfried Wilhelm, (2018) Dissertatio de arte combinatoria, , Paris: Hachette Livre-BNF; Levy, David, The Ethical Treatment of Artificially Conscious Robots (2009) International Journal of Social Robotics, 1 (3), pp. 209-216; Malle, Bertram F., Magar, Thapa Stuti, Scheutz, Matthias, AI in the Sky: How People Morally Evaluate Human and Machine Decisions in a Lethal Strike Dilemma (2019) Robotics and Well-Being, pp. 111-133. , edited by Maria Aldinhas Ferreira, Joao Silva Sequeira, Gurvinder Singh Virk, Mohammad Tokhi Osman, and Ender E. Kadar, Cham: Springer; McCarthy, John, Hayes, Patrick J., Some Philosophical Problems from the Standpoint of Artificial Intelligence (1969) Machine Intelligence, 4, pp. 463-502. , edited by Bernard Meltzer, and Donald M. Michie, Edinburgh: Edinburgh University Press; McCulloch, Warren S., Pitts, Walter H., A Logical Calculus of the Ideas Immanent in Nervous Activity (1943) Bulletin of Mathematical Biophysics, 5, pp. 115-133; Minsky, Marvin, (1967) Computation: Finite and Infinite Machines, , New Jersey: Prentice-Hall; Mitcham, Carl, (1994) Thinking through Technology: The Path between Engineering and Philosophy, , Chicago: The University of Chicago Press; Moor, James H., The Nature, Importance, and Difficulty of Machine Ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 18-21; Newell, Allen, Simon, Herbert Alexander, Computing Science as Empirical Enquiry: Symbols and Search (1976) Communications of the Association for Computing Machinery, 19, pp. 113-126; Newell, Allen, Simon, Herbert Alexander, GPS-A Program that Simulates Human Thought (1961) Lernende Automaten, pp. 109-124. , edited by Heinz Billing, Munich: Oldenburg; Newell, Allen, Simon, Herbert Alexander, The Logic Theory Machine: A Complex Information-Processing System (1956) IRE Transactions on Information Theory, 2 (3), pp. 61-79; Newell, Allen, Simon, Herbert Alexander, (1956) Current Developments in Complex Information Processing: Technical Report P-850, , Santa Monica, CA: Rand Corporation; Newell, Allen, Shaw, John Crosley, Programming the Logic Theory Machine IRE-AIEE-ACM â€˜57 (Western): Papers Presented at the February 26-28, 1957, Western Joint Computer Conference: Techniques for Reliability, pp. 230-240. , New York: Association for Computing Machinery, 1957; Newell, Allen, Shaw, John Crosley, Simon, Herbert Alexander, Element of a Theory of Human Problem Solving (1958) Psychological Review, 65, pp. 151-166; Newell, Allen, Physical Symbol Systems (1980) Cognitive Science, 4, pp. 135-183; Owen, Jonathan, Osley, Richard, Bill of Rights for Abused Robots: Experts Draw up an Ethical Charter to Prevent Humans Exploiting Machines (2011) The Independent, , https://www.independent.co.uk/news/science/bill-of-rights-for-abused-robots-5332596.html, September 17; Pylyshyn, Zenon W., (1987) The Robotâ€™s Dilemma: The Frame Problem in Artificial Intelligence, , ed. Norwood, NJ: Ablex; Rawls, John, Justice as Fairness: Political not Metaphysical (1985) Philosophy and Public Affairs, 14, pp. 223-251; Rawls, John, (1999) A Theory of Justice, , 2nd edition. Cambridge, MA: The Belknap Press of Harvard University Press; Rawls, John, (1971) A Theory of Justice, , Cambridge, MA: The Belknap Press of Harvard University Press; Rawls, John, (2001) Justice as Fairness: A Restatement, , Cambridge, MA: Harvard University Press; Ross, William David, (1930) The Right and the Good, , Oxford: Clarendon Press; Russell, Stuart, Tegmark, Max, Autonomous Weapons: An Open Letter from AI & Robotics Researchers Future of Life Institute, , https://futureoflife.org/open-letter-autonomous-weapons/; Singer, Peter, All Animals Are Equal (2012) Animal Ethics: Past and Present Perspectives, pp. 163-178. , edited by Evangelos D. Protopapadakis, Berlin: Logos Verlag; Singer, Peter, (1975) Animal Liberation: A New Ethics for our Treatment of Animals, , New York: New York Review of Books; Singer, Peter, (1993) Practical Ethics, , 2nd edition. Cambridge: Cambridge University Press; Soares, Nate, The Value Learning Problem (2019) Artificial Intelligence, Safety and Security, pp. 89-97. , edited by Roman Yampolskiy, Boca Raton, FL: CRC Press; Sparrow, Robert, Killer Robots (2007) Journal of Applied Philosophy, 24 (1), pp. 62-77; Sparrow, Robert, The Turing Triage Test (2004) Ethics and Information Technology, 6, pp. 201-213; Taylor, Steve, Pickering, Brian, Boniface, Michael, Anderson, Michael, Danks, David, Felstad, Asbj0rn, Leese, Matthias, Woollard, Fiona, Responsible AI - Key Themes, Concerns & Recommendations for European Research and Innovation (2018) Zenodo, , July 2; Tegmark, Mark, Benefit and Risks of Artificial Intelligence Future of Life Institute, , https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/; Tegmark, Mark, (2017) Life 3.0: Being Human in the Age of Artificial Intelligence, , New York: Knopf; Thompson, Nicholas, Will Artificial Intelligence Enhance or Hack Humanity? (2019) Wired, , https://www.wired.com/story/will-artificial-intelligence-enhance-hack-humanity/, April 20; Tooley, Michael, In Defense of Abortion and Infanticide (1994) The Abortion Controversy: A Reader, pp. 186-213. , edited by Luis P. Pojman, and Francis J. Beckwith, Boston, MA: Jones & Bartlett; Turing, Alan Mathison, Computing, Machinery and Intelligence (1950) Mind, 59, pp. 433-460; Turing, Alan Mathison, Intelligent Machinery (1969) Machine Intelligence, 5, pp. 3-23. , edited by B. Meltzer, and D. M. Michie, Edinburgh: Edinburgh University Press; Turing, Alan Mathison, On Computable Numbers, with an Application to the Entscheidungsproblem (2004) The Essential Turing: Seminal Writings in Computing, Logic, Philosophy, Artificial Intelligence, and Artificial Life, pp. 58-90. , edited by Jack B. Copeland, Oxford: Oxford University Press; Turing, Alan Mathison, On Computable Numbers, with an Application to the Entscheidungsproblem. A Correction (1938) Proceedings of the London Mathematical Society, 43, pp. 544-546; Wallace, Gregory, Elon Musk Warns against Unleashing Artificial Intelligence â€˜Demon (2014) CNN Business, , https://money.cnn.com/2014/10/26/technology/elon-musk-artificial-intelligence-demon/, October 26; Warren, Mary Anne, On the Moral and Legal Status of Abortion (2003) Contemporary Moral Problems, pp. 144-155. , edited by J. White, Belmont, CA: Wadsworth/Thompson Learning; Wheeler, Michael, Cognition in Context: Phenomenology, Situated Robotics, and the Frame Problem (2008) International Journal of Philosophical Studies, 16 (3), pp. 323-349; Wheeler, Michael, (2005) Reconstructing the Cognitive World: The Next Step, , Cambridge, MA: MIT Press; Winner, Langdon, Citizen Virtues in a Technological Order (1992) Inquiry, 35 (3-4), pp. 341-361. , nos; Winner, Langdon, Techne and Politeia: The Technical Constitution of Society (1983) Philosophy of Technology, pp. 97-111. , edited by Paul T. Dubrin, and Friedrich Rapp, Dordrecht, Boston, Lancaster: D. Reidel; Yampolskiy, Roman, Artificial Intelligence Safety Engineering: Why Machine Ethics is a Wrong Approach (2013) Philosophy and Theory of Artificial Intelligence. Studies in Applied Philosophy, Epistemology and Rational Ethics, pp. 389-396. , edited by Vincent Muller, Berlin, Heidelberg: Springer; Yudkowsky, Eliezer, Complex Value Systems in Friendly AI (2011) Artificial General Intelligence, pp. 388-393. , edited by Jurgen Schmidhuber, Kristinn R. Thorisson, and Moshe Looks, Berlin, Heidelberg: Springer; Yudkowsky, Eliezer, The Value Loading Problem EDGE, p. 2021. , https://www.edge.org/response-detail/26198, July 12},
correspondence_address1={Anderson, M.; University of HartfordUnited States; email: anderson@hartford.edu},
editor={NA},
publisher={NKUA Applied Philosophy Research Laboratory},
issn={26539373},
isbn={NA},
language={English},
abbrev_source_title={Conatus J. Philos.},
document_type={Article},
source={Scopus},
number={1},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Tafani2021377,
type={ARTICLE},
author={Tafani, D.},
title={L'IMPERATIVO CATEGORICO COME ALGORITMO. KANT E L'ETICA DELLE MACCHINE},
journal={Sistemi Intelligenti},
year={2021},
volume={33},
pages={377-393},
doi={10.1422/101195},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112807816&doi=10.1422%2f101195&partnerID=40&md5=b785c781bca6ddc4cdddb2f6aae55e9b},
affiliation={Dipartimento di Scienze Politiche, UniversitÃ  di Pisa, Via Serafini 3, Pisa, 56126},
abstract={Machine ethics can be defined as the attempt to identify a method to construct artificial moral agents. Such an attempt is based on the metaethical assumption that ethics can be translated into computational terms. Kant's moral doctrine was very early identified as one of the reference models for this attempt, due to three characteristics: a cognitive and rationalist metaethics, the denial of the existence of mutually alternative ethics and of moral dilemmas and the elaboration of a logical criterion for moral judgment. With the universalisation test, Kant presents a formulation of the categorical imperative which is based on the principle of non-contradiction. It thus seems that this test could be translated into an algorithm and transformed into a procedure that can be executed by a computer system. The paper aims to show that the Kantian test cannot be translated into an algorithm and cannot therefore provide machine ethics with a criterion for moral judgment. Although the categorical imperative is untranslatable in computational terms, it is nevertheless appropriate for machine ethics to start from Kant, from a normative, a metaethical and a methodological point of view: Kant's idea of man as an end in himself is incorporated in the constitutional laws of many contemporary States and constitutes a constraint on the programming of intelligent systems. Kantian doctrine can also be a warning against the tendency to think of machines as agents in the same sense that human beings, rather than as things with the ability to follow programmed or learned rules. Finally, Kant's moral doctrine should also be considered a reminder that a moral agent has, above all, the ability to set himself goals and to represent a norm, to respect it or to violate it. Ongoing research on these issues can make an important contribution to governing those systems in which artificial agents operate, and make decisions, alongside human beings. Â© 2021. All Rights Reserved.},
author_keywords={AI ethics;  artificial morality;  ethics of artificial intelligence;  Kant and AI ethics;  Kant and machine ethics;  Kant's universalizability test;  machine ethics},
keywords={NA},
references={(2020) AI Ethics Guidelines Global Inventory, , https://algo-rithmwatch.org/en/project/ai-ethics-guidelines-global-inventory, AlgorithmWatch (ultima consultazione, di questo e degli altri indirizzi web citati, 1.6.2020); Allen, C., Varner, G., Zinzer, J., Prolegomena to any future artificial moral agent (2000) Journal of Experimental & Theoretical Artificial Intelligence, 12 (3), pp. 251-261; Anderson, M., Anderson, S.L., (2011) Machine Ethics, , (a cura di) Cambridge: Cambridge University Press; Andrighetto, G., Governatori, G., Noriega, P., van der Torre, L., (2013) Normative Multi-Agent Systems, 4. , https://drops.dagstuhl.de/opus/portals/dfu/index.php?semnr=13003, (a cura di) (Dagstuhl Follow-Ups, Wadern, Germany: Dagstuhl Publishing; Awad, E., Dsouza, S., Bonnefon, J.-F., Shariff, A., Rahwan, I., Crowdsourcing Moral Machines (2020) Communications of the ACM, 63 (3), pp. 48-55; Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., Bonnefon, J.-F., Rahwan, I., The Moral Machine experiment (2018) Nature, 563, pp. 59-64; Beavers, A.F., Between Angels and Animals: The Question of Robot Ethics, or Is Kantian Moral Agency Desirable? (2009) Association for Practical and Professional Ethics, Eighteenth Annual Meeting, , Cincinnati, Ohio, March 5th-8th, 2009; Beavers, A.F., Moral Machines and the Threat of Ethical Nihilism (2012) Robot Ethics: The Ethical and Social, Implication on Robotics, pp. 333-344. , P. Lin, G. Bekey e K. Abney (a cura di), Cambridge, MA: MIT Press; Bendel, O., Schwegler, K., Richards, B., Towards Kant Machines (2017) The AAAI 2017 Spring Symposium on Artificial Intelligence for the Social Good Technical Report SS-17-01, , https://www.aaai.org/ocs/index.php/SSS/SSS17/paper/view/15278/14507; Blaes, S., Pogancic, M.V., Zhu, J., Martius, G., Control What You Can: Intrinsically Motivated Task-Planning Agent (2019) Advances in Neural Information Processing Systems, 32, pp. 12541-12552. , H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox e R. Garnett (a cura di); Brieger, J., Ãœber die UnmÃ¶glichkeit einer kantisch handelnden Maschine (2019) Maschinenethik. Ethik in mediatisierten Welten, pp. 107-119. , M. Rath, F. Krotz e M. Karmasin (a cura di), Wiesbaden: Springer; (2006) Urteil des Ersten Senats vom 15, pp. 1-156. , http://www.bverfg.de/e/rs20060215_1b-vr035705.html, Bundesverfassungsgericht Februar 2006 1 BvR 357/05 Rn; Castelfranchi, C., Dignum, F., Jonker, C.M., Treur, J., Deliberative Normative Agents: Principles and Architecture (2000) Intelligent Agents VI. Agent Theories, Architectures, and Languages. ATAL 1999. Lecture Notes in Computer Science, 1757, pp. 364-378. , N.R. Jennings e Y. Le-sperance (a cura di), Berlin, Heidelberg: Springer, doi.org; (2020) Libro bianco sull'intelligenza artificiale. Un approccio europeo all'eccellenza e alla fiducia, , https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence-feb2020_it.pdf, Commissione Europea; Conte, R., Castelfranchi, C., The Mental Path of Norms (2006) Ratio Juris, 19 (4), pp. 501-517; (2017) Automatisiertes und Vernetztes Fahren. Eingesetzt durch den Bundesminister fÃ¼r Verkehr und digitale Infrastruktur, , https://www.bmvi.de/SharedDocs/DE/Publikationen/DG/bericht-der-ethik-kommission.html, Ethik-Kommission Bericht, Juni 2017; Gips, E., Towards the ethical robot (1995) Android epistemology, pp. 243-252. , K.M. Ford, C. Glymour e P.J. Hayes (a cura di), Cambridge, MA: MIT Press; Goble, L., Meyer, J.-J.Ch., (2006) Deontic Logic and Artificial Normative Systems. 8th International Workshop on Deontic Logic in Computer Science, DEON 2006, , (a cura di). Utrecht, The Netherlands, July 12-14, 2006. Proceedings. Berlin, Heidelberg: Springer, doi.org; Goodall, N.J., More than Trolleys: Plausible, Ethically Ambiguous Scenarios likely to Be Encountered by Automated Vehicles (2019) Transfers: Interdisciplinary Journal of Mobility Studies, 9 (2), pp. 45-58; Hagendorff, T., The Ethics of AI Ethics. An Evaluation of Guidelines (2020) Minds and Machines, 30, pp. 99-120; Herman, B., (1993) The Practice of Moral Judgement, , Cambridge and London: Harvard University Press; Hinske, N., Zwischen Begriff und Metapher zur Verwendung des Stichworts â€˜Maschine' bei Kant (2005) Machina. XI Colloquio Internazionale del Lessico Intellettuale Europeo, pp. 477-488. , M. Veneziani (a cura di), (Roma, 8-10 gennaio 2004). Atti. Firenze: Olschki; (2018) Ethically Aligned Design. A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems, , https://standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/ead_v2.pdf, IEEE; (2018) When Computers Decide: European Recommendations on Machine-Learned Automated Decision Making, , https://www.acm.org/binaries/content/assets/public-policy/ie-euacm-adm-report-2018.pdf, Informatics Europe & EUACM; Kant, I., Beantwortung der Frage: Was ist AufklÃ¤rung? (1784) Accademia Prussiana delle Scienze (a cura di) (1900), Kant's gesammelte Schriften, pp. 33-42. , https://korpora.zim.uni-duisburg-essen.de/kant/verzeichnisse-gesamt.html, Berlin: W. de Gruyter, VIII; Kant, I., Grundlegung zur Metaphysik der Sitten (1785) Kant's gesammelte Schriften, IV, pp. 385-463. , cit; Kant, I., Kritik der praktischen Vernunft (1788) Kant's gesammelte Schriften, pp. 1-163. , cit; Kant, I., Zum ewigen Frieden. Ein philosophischer Entwurf von Immanuel Kant (1795) Kant's gesammelte Schriften, VIII, pp. 341-386. , cit; Kant, I., Die Metaphysik der Sitten (1797) Kant's gesammelte Schriften, VI, pp. 203-493. , cit; Kant, I., Reflexionen zur Rechtsphilosohie Kant's gesammelte Schriften, XIX, pp. 442-613. , cit; Kersting, W., Das starke Gesetz der Schuldigkeit und das schwÃ¤chere der GÃ¼tigkeit. Kant und die Pflichtenlehre des 18. Jahrhunderts (1982) Studia Leibnitiana, 14, pp. 184-220; Kleingeld, P., Contradiction and Kant's Formula of Universal Law (2017) Kant-Studien, 108 (1), pp. 89-115; Klincewicz, M., Challenges to engineering moral reasoners. Time and context (2017) Robot Ethics 2.0: From Autonomous Cars to Artificial Intelligence, pp. 244-259. , P. Lin, K. Abney e R. Jenkins (a cura di), New York: Oxford University Press; Kochetkova, T., An Overview of Machine Medical Ethics (2015) Machine Medical Ethics, pp. 3-15. , S.P. van Rysewyk e M. Pontier (a cura di), Cham: Springer; Korsgaard, C.M., Kant's Formula of Universal Law (1985) Pacific Philosophical Quarterly, 66 (1-2), pp. 24-47; Landucci, S., (1994) Sull'etica di Kant, , Milano: Guerini; Lindner, F., Benzen, M.M., The Hybrid Ethical Reasoning Agent IMMANUEL (2017) HRI '17: Proceedings of the Companion of the 2017 ACM/ IEEE International Conference on Human-Robot Interaction, pp. 187-188. , http://gki.informatik.uni-freiburg.de/papers/lindner-bentzen-hri2017.pdf; Lindner, F., Benzen, M.M., A Formalization of Kant's Second Formulation of the Categorical Imperative (2018) ISAIM 2018, International Symposium on Artificial Intelligence and Mathematics, , https://arxiv.org/pdf/1801.03160.pdf, Fort Lauderdale, FL. January 3-5, 2018; Malle, B.F., Bello, P., Scheutz, M., Requirements for an Artificial Agent with Norm Competence (2019) AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. 21-27. , https://doi.org/10.1145/3306618.3314252; McDermott, D., Why Ethics is a High Hurdle for AI (2008) North American Conference on Computing and Philosophy, , http://www.cs.yale.edu/homes/dvm/papers/ethical-machine.pdf, Bloomington, Indiana; Misselhorn, K., (2018) Grundfragen der Machinenethik, , Ditzingen: Reclam; Moor, J.H., The Nature, Importance, and Difficulty of Machine Ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 18-21; Noothigattu, R., Gaikwad, S.S., Awad, E., Dsouza, S., Rahwan, I., Ravikumar, P., Procaccia, A.D., (2017) A Voting-Based System for Ethical Decision Making, , https://arxiv.org/pdf/1709.06692.pdf; Nowak, E., Can human and artificial agents share an autonomy categorical imperative-based ethics and moral selfhood? (2017) Filozofia Publiczna i Edukacja Demokratyczna, 6 (2), pp. 169-208; Nyholm, S., The Ethics of Crashes with Self-Driving Cars: A Roadmap, I (2018) Philosophy Compass, 13, p. 7; Numerico, T., Social network e algoritmi di machine learning: problemi cognitivi e propagazione dei pregiudizi (2019) Sistemi intelligenti, 31 (3), pp. 469-494; O'Neill, O., (1993) Acting on Principle: An essay on Kantian ethics, , (2). New York: Cambridge University Press; Oâ€™Neil, C., (2016) Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy, , New York: Crown; Powers, T.M., Prospects for a Kantian Machine (2006) IEEE Intelligent Systems, 21 (4), pp. 46-51; Ruffolo, U., (2020) Intelligenza Artificiale - Il diritto, i diritti, l'etica, , (a cura di) Milano: Giuffre; (2018) Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles, , https://www.sae.org/standards/content/j3016_201806/, SAE International; Santucci, V.G., Baldassarre, G., Mirolli, M., GRAIL: A Goal-Discovering Robotic Architecture for Intrinsically-Motivated Learning (2016) IEEE Transactions on Cognitive and Developmental Systems, 8 (3), pp. 214-231; Santucci, V.G., Oudeyer, P.-Y., Barto, A., Baldassarre, G., Intrinsically motivated open-ended learning in autonomous robots (2020) Frontiers in Neurorobotics, 13, p. 115. , doi.org; SchÃ¤ffner, V., Wenn Ethik zum Programm wird: Eine risikoethische Analyse moralischer Dilemmata des autonomen Fahrens (2020) Zeitschrift fÃ¼r Ethik und Moralphilosophie, , https://doi.org/10.1007/s42048-020-00061-9; Singer, M.G., (1963) Generalization in Ethics. An Essay in the Logic of Ethics, with the Rudiments of a System of Moral Philosophy, , London: Eyre & Spottiswoode; Stahl, B.C., Information, Ethics and Computers: The Problem of Autonomous Moral Agents (2004) Mind and Machines, 14, pp. 67-83; Storrs Hall, J., Ethic for Machines (2011) Machine Ethics, pp. 28-44. , M. Anderson e S.L. Anderson (a cura di), Cambridge: Cambridge University Press; Tafani, D., Sulla moralitÃ  artificiale. Le decisioni delle macchine tra etica e diritto (2020) Rivista di filosofia, 111 (1), pp. 81-103; Tonkens, R., A Challenge for Machine Ethics (2009) Minds & Machines, 19, pp. 421-438; Turing, A., Computing Machinery and Intelligence (1950) Mind and Language, 59, pp. 434-460; Wallach, W., Allen, C., (2009) Moral Machines: Teaching Robots Right from Wrong, , Oxford: Oxford University Press; Weber, K., Autonomie und MoralitÃ¤t als Zuschreibung: Ãœber die begriffliche und inhaltliche Sinnlosigkeit einer Maschinenethik (2019) Maschinenethik. Normative Grenzen autonomer Systeme, pp. 193-208. , M. Rath, F. Krotz e M. Karmasin (a cura di), Wiesbaden: Springer; Wood, A., (2017) Formulas of the Moral Law (Elements in the Philosophy of Immanuel Kant), , Cambridge: Cambridge University Press; Zuboff, S., (2018) The age of surveillance capitalism: The fight for a human future at the new frontier of power, , New York: PublicAffairs},
correspondence_address1={Tafani, D.; Dipartimento di Scienze Politiche, Via Serafini 3},
editor={NA},
publisher={Societa Editrice Il Mulino},
issn={11209550},
isbn={NA},
language={English},
abbrev_source_title={Sist. Intelligenti},
document_type={Article},
source={Scopus},
number={2},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Serafimova2020,
type={ARTICLE},
author={Serafimova, S.},
title={Whose morality? Which rationality? Challenging artificial intelligence as a remedy for the lack of moral enhancement},
journal={Humanities and Social Sciences Communications},
year={2020},
volume={7},
pages={NA},
doi={10.1057/s41599-020-00614-8},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092361491&doi=10.1057%2fs41599-020-00614-8&partnerID=40&md5=6fd0e54360ae1197e342fb4feb398cae},
affiliation={Bulgarian Academy of Sciences, Sofia, Bulgaria},
abstract={Moral implications of the decision-making process based on algorithms require special attention within the field of machine ethics. Specifically, research focuses on clarifying why even if one assumes the existence of well-working ethical intelligent agents in epistemic terms, it does not necessarily mean that they meet the requirements of autonomous moral agents, such as human beings. For the purposes of exemplifying some of the difficulties in arguing for implicit and explicit ethical agents in Moorâ€™s sense, three first-order normative theories in the field of machine ethics are put to test. Those are Powersâ€™ prospect for a Kantian machine, Anderson and Andersonâ€™s reinterpretation of act utilitarianism and Howard and Munteanâ€™s prospect for a moral machine based on a virtue ethical approach. By comparing and contrasting the three first-order normative theories, and by clarifying the gist of the differences between the processes of calculation and moral estimation, the possibility for building whatâ€”one might call strong â€œmoralâ€ AI scenariosâ€”is questioned. The possibility of weak â€œmoralâ€ AI scenarios is likewise discussed critically. Â© 2020, The Author(s).},
author_keywords={NA},
keywords={NA},
references={Anderson, M., Anderson, S.L., Machine ethics: creating an ethical intelligent agent (2007) AI Magazine, 28 (4), pp. 15-26; Anderson, S.L., Asimovâ€™s â€œthree laws of roboticsâ€ and machine metaethics (2008) AI & Society, 22 (4), pp. 477-493; Amoore, L., (2020) Cloud Ethics: Algorithms and the Attributes of Ourselves and Others, , Duke University Press Books; Beer, D., The social power of algorithms (2016) Information, Commun Soc, , https://doi.org/10.1080/1369118X.2016.1216147; Feldman, J., Condorcet et la mathÃ©matique sociale. Enthousiasmes et bÃ©mols (2005) MathÃ©matiques Sci. Hum., 172 (4), pp. 7-41; Howard, D., Muntean, I., Artificial moral cognition: Moral functionalism and autonomous moral agency (2017) Philosophy and Computing: Essays in Epistemology, Philosophy of Mind, Logic, and Ethics. Springer, pp. 121-159; Kitchin, R., Thinking critically about and researching algorithms (2017) Inform Commun Soc, 20 (1), pp. 14-29; Klincewicz, M., Artificial intelligence as a means to moral enhancement (2016) Stud Log Gramm Rhetor, 48 (61), pp. 171-187; Klincewicz, M., Challenges to engineering moral reasoners: time and context (2017) Robot ethics 2.0: from autonomous cars to artificial intelligence, pp. 244-257. , Lin P, Abney K, Jenkins R, (eds), Oxford University Press, Oxford; Lara, F., Deckers, J., Artificial intelligence as a Socratic assistant for moral enhancement (2019) Neuroethics, , https://doi.org/10.1007/s12152-019-09401-y; Markham, A.N., Tiidenberg, K., Herman, A., Ethics as methods: Doing ethics in the era of big data researchâ€“Introduction (2018) Soc Media+Soc, , https://journals.sagepub.com/doi/full/10.1177/2056305118784502, Julyâ€“September: 1â€“9; Metcalf, J., Keller, E.F., Boyd, D., Perspectives on big data, ethics, and society. The Council for Big Data (2016) Ethics, and Society, , http://bdes.datasociety.net/council-output/perspectives-on-big-data-ethics-and-society/; Mittelstadt, B.D., Allo, P., Taddeo, M., The ethics of algorithms: Mapping the debate (2016) Big Data Soc, 3 (2), pp. 1-21; Moor, J.H., The nature, importance, and difficulty of machine ethics (2006) IEEE Intell Syst, 21 (4), pp. 18-21; Moor, J.H., (2009) Four Kinds of Ethical Robots. Philosophy Now. a Magazine of Ideas 72, , https://philosophynow.org/issues/72/Four_Kinds_of_Ethical_Robots; Neff, G., From bad users and failed uses to responsible technologies: A call to expand the AI ethics toolkit (2020) Proceedings of the AAAI/ACM Conference on AI, Ethics and Society, pp. 5-6. , https://doi.org/10.1145/3375627.3377141; Nickel, P.J., Trust in technological systems (2013) Norms in technology: philosophy of engineering and technology, pp. 223-237. , de Vries MJ, Hansson SO, Meijers AWM, (eds), Springer, Dordrecht; Powers, T.M., Prospects for a Kantian machine (2006) Intell Syst, IEEE, 21 (4), pp. 46-51; Savulescu, J., Maslen, H., Moral enhancement and artificial intelligence. Moral AI? (2015) Beyond Artificial Intelligence. the Disappearing humanâ€”machine Divide, pp. 79-95. , Romportl J, Zackova E, Kelemen J, Springer; Stahl, B.C., Information, ethics, and computers: the problem of autonomous moral agents (2004) Minds Machines, 14, pp. 67-83; Wallach, W., Allen, C., (2010) Moral machines: teaching robots right from wrong, , Oxford University Press, Oxford},
correspondence_address1={Serafimova, S.; Bulgarian Academy of SciencesBulgaria; email: silvija_serafimova@yahoo.com},
editor={NA},
publisher={Springer Nature},
issn={26629992},
isbn={NA},
language={English},
abbrev_source_title={Hum. Soc. Sci. Comm},
document_type={Article},
source={Scopus},
number={1},
art_number={119},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Cervantes2020117,
type={ARTICLE},
author={Cervantes, S. and LÃ³pez, S. and Cervantes, J.-A.},
title={Toward ethical cognitive architectures for the development of artificial moral agents},
journal={Cognitive Systems Research},
year={2020},
volume={64},
pages={117-125},
doi={10.1016/j.cogsys.2020.08.010},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090423654&doi=10.1016%2fj.cogsys.2020.08.010&partnerID=40&md5=2028e70c09943a434b6e6ce2049b4fda},
affiliation={Department of Computer Science and Engineering, Universidad de Guadalajara, Ameca, P.C.  46600, Mexico},
abstract={New technologies based on artificial agents promise to change the next generation of autonomous systems and therefore our interaction with them. Systems based on artificial agents such as self-driving cars and social robots are examples of this technology that is seeking to improve the quality of people's life. Cognitive architectures aim to create some of the most challenging artificial agents commonly known as bio-inspired cognitive agents. This type of artificial agent seeks to embody human-like intelligence in order to operate and solve problems in the real world as humans do. Moreover, some cognitive architectures such as Soar, LIDA, ACT-R, and iCub try to be fundamental architectures for the Artificial General Intelligence model of human cognition. Therefore, researchers in the machine ethics field face ethical questions related to what mechanisms an artificial agent must have for making moral decisions in order to ensure that their actions are always ethically right. This paper aims to identify some challenges that researchers need to solve in order to create ethical cognitive architectures. These cognitive architectures are characterized by the capacity to endow artificial agents with appropriate mechanisms to exhibit explicit ethical behavior. Additionally, we offer some reasons to develop ethical cognitive architectures. We hope that this study can be useful to guide future research on ethical cognitive architectures. Â© 2020 Elsevier B.V.},
author_keywords={Artificial agents;  Artificial moral agents;  Cognitive functions;  Ethical cognitive architectures;  Machine ethics},
keywords={Architecture;  Artificial intelligence;  Biomimetics;  Philosophical aspects, Artificial agents;  Artificial general intelligences;  Autonomous systems;  Cognitive agents;  Cognitive architectures;  Ethical behavior;  Ethical question;  Human-like intelligence, Autonomous agents, article;  ethics;  human;  human experiment;  intelligence;  morality},
references={Aaltola, E., Varieties of empathy and moral agency (2014) Topoi, 33 (1), pp. 243-253; Abbass, H.A., Petraki, E., Merrick, K., Harvey, J., Barlow, M., Trusted autonomy and cognitive cyber symbiosis: Open challenges (2016) Cognitive Computation, 8 (3), pp. 385-408; Arkin, R., (2018), Lethal autonomous systems and the plight of the non-combatant. In The political economy of robots (pp. 317â€“326); Bach, J., A motivational system for cognitive ai (2011) International conference on artificial general intelligence, pp. 232-242; Barbosa, J., LeitÃ£o, P., Adam, E., Trentesaux, D., Dynamic self-organization in holonic multi-agent manufacturing systems: The ADACOR evolution (2015) Computers in Industry, 66, pp. 99-111; Batty, M., Axhausen, K.W., Giannotti, F., Pozdnoukhov, A., Bazzani, A., Wachowicz, M., Ouzounis, G., Portugali, Y., Smart cities of the future (2012) The European Physical Journal Special Topics, 214 (1), pp. 481-518; Bauman, C.W., McGraw, A.P., Bartels, D.M., Warren, C., Revisiting external validity: Concerns about trolley problems and other sacrificial dilemmas in moral psychology (2014) Social and Personality Psychology Compass, 8 (9), pp. 536-554; Bemelmans, R., Gelderblom, G.J., Jonker, P., De Witte, L., Socially assistive robots in elderly care: A systematic review into effects and effectiveness (2012) Journal of the American Medical Directors Association, 13 (2), pp. 114-120; Borst, J.P., Anderson, J., (2015), R. Using the ACT-R cognitive architecture in combination with fMRI data. In An introduction to model-based cognitive neuroscience (pp. 339â€“352); Cervantes, J.-A., LÃ³pez, S., RodrÃ­guez, L.-F., Cervantes, S., Cervantes, F., Ramos, F., Artificial moral agents: A survey of the current status (2019) Science and Engineering Ethics, pp. 1-32; Cervantes, J.-A., RodrÃ­guez, L.-F., LÃ³pez, S., Ramos, F., Robles, F., Autonomous agents and ethical decision-making (2016) Cognitive Computation, 8 (2), pp. 278-296; Contissa, G., Lagioia, F., Sartor, G., The ethical knob: ethically-customisable automated vehicles and the law (2017) Artificial Intelligence and Law, 25 (3), pp. 365-378; Cosmides, L., Tooby, J., Evolutionary psychology: New perspectives on cognition and motivation (2013) Annual Review of Psychology, 64, pp. 201-229; De Sio, F.S., Killing by autonomous vehicles and the legal doctrine of necessity (2017) Ethical Theory and Moral Practice, 20 (2), pp. 411-429; Etzioni, A., Etzioni, O., Incorporating ethics into artificial intelligence (2017) The Journal of Ethics, 21 (4), pp. 403-418; Faghihi, U., Franklin, S., (2012), The LIDA model as a foundational architecture for AGI. In Theoretical Foundations of Artificial General Intelligence (pp. 103â€“121); Feng, S., Setoodeh, P., Haykin, S., Smart home: Cognitive interactive people-centric internet of things (2017) IEEE Communications Magazine, 55 (2), pp. 34-39; Franklin, S., Madl, T., D'mello, S., Snaider, J., LIDA: A systems-level architecture for cognition, emotion, and learning (2013) IEEE Transactions on Autonomous Mental Development, 6 (1), pp. 19-41; Gallese, V., Keysers, C., Rizzolatti, G., A unifying view of the basis of social cognition (2004) Trends in Cognitive Sciences, 8 (9), pp. 396-403; Gogoll, J., MÃ¼ller, J.F., Autonomous cars: In favor of a mandatory ethics setting (2017) Science and Engineering Ethics, 23 (3), pp. 681-700; Goodall, N., (2014), J. Machine ethics and automated vehicles. In Road vehicle automation (pp. 93â€“102); Greczek, J., Kaszubski, E., Atrash, A., MatariÄ‡, M., Graded cueing feedback in robot-mediated imitation practice for children with autism spectrum disorders (2014) The 23rd IEEE international symposium on robot and human interactive communication, pp. 561-566; Gutierrez-Garcia, J.O., RodrÃ­guez, L.-F., Corruptible social agents (2016) Computer Animation and Virtual Worlds, 27 (2), pp. 89-102; Haboucha, C.J., Ishaq, R., Shiftan, Y., User preferences regarding autonomous vehicles (2017) Transportation Research Part C: Emerging Technologies, 78, pp. 37-49; Hagendorff, T., The ethics of ai ethics: An evaluation of guidelines (2020) Minds and Machines, pp. 1-22; <https://standards.ieee.org/project/7000.html>, IEEE, 2016a. P7000 - model process for addressing ethical concerns during system design. Accessed 10 April 2020. URL; <https://standards.ieee.org/project/7001.html>, IEEE, 2016b. P7001 - transparency of autonomous systems. Accessed 10 April 2020. URL; <https://standards.ieee.org/project/7002.html>, IEEE, 2016c. P7002 - data privacy process. Accessed 10 April 2020. URL; <https://standards.ieee.org/industry-connections/ec/ead-v1.html>, IEEE, 2017a. Aligned design: A vision for prioritizing human well-being with autonomous and intelligent systems, version 2. Tech. rep., IEEE, accessed 10 April 2020. URL; <https://standards.ieee.org/project/7003.html>, IEEE, 2017b. P7003 - algorithmic bias considerations. Accessed 11 April 2020. URL; <https://standards.ieee.org/project/7004.html>, IEEE, 2017c. P7004 - standard for child and student data governance. Accessed 11 April 2020. URL; <https://standards.ieee.org/project/7005.html>, IEEE, 2017d. P7005 - standard for transparent employer data governance. Accessed 11 April 2020. URL; <https://standards.ieee.org/project/7006.html>, IEEE, 2017e. P7006 - standard for personal data artificial intelligence (ai) agent. Accessed 11 April 2020. URL; <https://standards.ieee.org/project/7007.html>, IEEE, 2017f. P7007 - ontological standard for ethically driven robotics and automation systems. Accessed 12 April 2020. URL; <https://standards.ieee.org/project/7008.html>, IEEE, 2017g. P7008 - standard for ethically driven nudging for robotic, intelligent and autonomous systems. Accessed 12 April 2020. URL; https://standards.ieee.org/project/7009.html, IEEE, 2017h. P7009 - standard for fail-safe design of autonomous and semi-autonomous systems. Accessed 12 April 2020. URL; (2020), https://standards.ieee.org/content/ieee-standards/en/standard/7010-2020.html, IEEE P7010 well-being metrics standard for ethical artificial intelligence and autonomous systems. Accessed 12 April 2020. URL; Ingrand, F., Ghallab, M., Deliberation for autonomous robots: A survey (2017) Artificial Intelligence, 247, pp. 10-44; Jobin, A., Ienca, M., Vayena, E., The global landscape of ai ethics guidelines (2019) Nature Machine Intelligence, 1 (9), pp. 389-399; Kahane, G., Sidetracked by trolleys: Why sacrificial moral dilemmas tell us little (or nothing) about utilitarian judgment (2015) Social Neuroscience, 10 (5), pp. 551-560; Kotseruba, I., Tsotsos, J.K., 40 years of cognitive architectures: Core cognitive abilities and practical applications (2018) Artificial Intelligence Review, pp. 1-78; Krettenauer, T., Colasante, T., Buchmann, M., Malti, T., The development of moral emotions and decision-making from adolescence to early adulthood: A 6-year longitudinal study (2014) Journal of Youth and Adolescence, 43 (4), pp. 583-596; Laird, J.E., Extending the soar cognitive architecture (2008) Frontiers in Artificial Intelligence and Applications, 171, p. 224; Leite, I., Martinho, C., Paiva, A., Social robots for long-term interaction: A survey (2013) International Journal of Social Robotics, 5 (2), pp. 291-308; Li, R., Lu, B., McDonald-Maier, K.D., Cognitive assisted living ambient system: A survey (2015) Digital Communications and Networks, 1 (4), pp. 229-252; Lieto, A., Bhatt, M., Oltramari, A., Vernon, D., The role of cognitive architectures in general artificial intelligence (2018) Cognitive Systems Research, 48, pp. 1-3; Malle, B.F., Integrating robot ethics and machine morality: The study and design of moral competence in robots (2016) Ethics and Information Technology, 18 (4), pp. 243-256; Malti, T., Latzko, B., Children's moral emotions and moral cognition: Towards an integrative perspective (2010) New Directions for Child and Adolescent Development, 2010 (129), pp. 1-10; Metta, G., Natale, L., Nori, F., Sandini, G., Vernon, D., Fadiga, L., Von Hofsten, C., Santos-Victor, J., The iCub humanoid robot: An open-systems platform for research in cognitive development (2010) Neural Networks, 23 (8-9), pp. 1125-1134; Miller, T., Explanation in artificial intelligence: Insights from the social sciences (2019) Artificial Intelligence, 267, pp. 1-38; Moor, J., Four kinds of ethical robots (2009) Philosophy Now, 72, pp. 12-14; Moor, J.H., The nature, importance, and difficulty of machine ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 18-21; Mostafa, S.A., Ahmad, M.S., Mustapha, A., Adjustable autonomy: A systematic literature review (2017) Artificial Intelligence Review, 51 (2), pp. 149-186; Nahmias, E., (2007), Autonomous agency and social psychology. In Cartographies of the Mind (pp. 169â€“185); Natale, L., Nori, F., Metta, G., Fumagalli, M., Ivaldi, S., Pattacini, U., Randazzo, M., Sandini, G., (2013), The iCub platform: A tool for studying intrinsically motivated learning. In Intrinsically motivated learning in natural and artificial systems (pp. 433â€“458); Omohundro, S., (2012), Rational artificial intelligence for the greater good. In Singularity Hypotheses (pp. 161â€“179); Qureshi, M.O., Syed, R.S., The impact of robotics on employment and motivation of employees in the service sector, with special reference to health care (2014) Safety and Health at Work, 5 (4), pp. 198-202; Romero, O.J., Cognitively-inspired agent-based service composition for mobile and pervasive computing (2019) International Conference on AI and Mobile Services, pp. 101-117; Schreurs, M.A., Steuwer, S., (2015), D. Autonomous driving-political, legal, social, and sustainability dimensions. In Autonomes Fahren (pp. 151â€“173); Schwesinger, D., Shariati, A., Montella, C., Spletzer, J., A smart wheelchair ecosystem for autonomous navigation in urban environments (2017) Autonomous Robots, 41 (3), pp. 519-538; Stahl, B.C., Coeckelbergh, M., Ethics of healthcare robotics: Towards responsible research and innovation (2016) Robotics and Autonomous Systems, 86, pp. 152-161; Taeihagh, A., Lim, H.S.M., Governing autonomous vehicles: Emerging responses for safety, liability, privacy, cybersecurity, and industry risks (2019) Transport Reviews, 39 (1), pp. 103-128; Taormina, R.J., Gao, J.H., Maslow and the motivation hierarchy: Measuring satisfaction of the needs (2013) The American Journal of Psychology, 126 (2), pp. 155-177; ThÃ³risson, K., Helgasson, H., Cognitive architectures and autonomy: A comparative review (2012) Journal of Artificial General Intelligence, 3 (2), pp. 1-30; van Wynsberghe, A., Robbins, S., Critiquing the reasons for making artificial moral agents (2019) Science and Engineering Ethics, 25 (3), pp. 719-735; Wallach, W., Allen, C., Smit, I., Machine morality: Bottom-up and top-down approaches for modelling human moral faculties (2008) Ai & Society, 22 (4), pp. 565-582; Wallach, W., Franklin, S., Allen, C., A conceptual and computational model of moral decision making in human and artificial agents (2010) Topics in Cognitive Science, 2 (3), pp. 454-485; Wykowska, A., Chaminade, T., Cheng, G., Embodied artificial agents for understanding human social cognition (2016) Philosophical Transactions of the Royal Society B: Biological Sciences, 371 (1693), p. 20150375; Zhang, X., Zhou, M., Liu, H., Hussain, A., A cognitively inspired system architecture for the mengshi cognitive vehicle (2019) Cognitive Computation, pp. 1-10},
correspondence_address1={Cervantes, J.-A.; Department of Computer Science and Engineering, Mexico; email: antoniocervantes@valles.udg.mx},
editor={NA},
publisher={Elsevier B.V.},
issn={13890417},
isbn={NA},
language={English},
abbrev_source_title={Cogn. Sys. Res.},
document_type={Article},
source={Scopus},
number={NA},
art_number={NA},
funding_details={NA},
coden={CSROA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Hauer2020425,
type={ARTICLE},
author={Hauer, T.},
title={Machine Ethics, Allostery and Philosophical Anti-Dualism: Will AI Ever Make Ethically Autonomous Decisions?},
journal={Society},
year={2020},
volume={57},
pages={425-433},
doi={10.1007/s12115-020-00506-2},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088995369&doi=10.1007%2fs12115-020-00506-2&partnerID=40&md5=2719ac7bd465e34c1c8ce41ee267e5a8},
affiliation={Department of Philosophy, Faculty of Philosophy and Arts, Trnava University in Trnava, Hornopotocna street 23, Trnava, 918 43, Slovakia},
abstract={Essentially, the area of â€‹â€‹research into the ethics of artificial intelligence is divided into two main areas. One part deals with creating and applying ethical rules and standards. This area formulates recommendations that should respect fundamental rights, applicable regulations and the main principles and values, ensuring the ethical purpose of AI while ensuring its technical robustness and reliability. The second strand of research into AI ethics addresses the question of whether and how robots and AI platforms can behave ethically autonomously. The question of whether ethics can be â€œalgorithizedâ€ depends on how AI developers understand ethics and on the adequacy of their understanding of ethical issues and methodological challenges in this area. There are four basic problem areas that developers of machines and platforms containing advanced AI algorithms are confronted with â€“ lack of ethical knowledge, pluralism of ethical methods, cases of ethical dilemmas, and machine distortion. Knowledge of these and similar problems can help programmers and researchers avoid pitfalls and build better moral machines. Unfortunately, discussions in areas that should help to research the field of AI ethics, such as philosophy of mind or general ethics, are now hopelessly infused with a number of autotelic philosophical distinctions and thought experiments. When asked whether machines could become fully ethically autonomous in the near future, most philosophers and ethicists answer that they could not, because AI has no free will and is unable to realize phenomenal consciousness. Therefore, the main proposition of this text is that questions about the ethics of autonomous intelligent systems and AI platforms evolving over time through learning from data (Machine Ethics) cannot be answered by the concepts and thought experiments of the philosophy of mind and general ethics. These instruments are closed to the possibility of empirical falsification, use special sci-fi tools, are based on faulty analogies, transfer the burden of proof to the counterparty without justification, and usually end in an epistemological fiasco. Therefore, they do not bring any added value. Finally, let us stop analysing and overcoming these infertile philosophical distinctions and leave them at their own mercy. Â© 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Allostery;  Autonomous machines;  Machine ethics;  Philosophical anti-dualism;  Thought experiments},
keywords={NA},
references={Allen, C., Wallach, W., Smit, I., Why machine ethics? (2006) IEEE Intelligent Systems, 21 (4), pp. 12-17; Allen, C., Smit, I., Wallach, W., Artificial morality: Top-down, bottom-up, and hybrid approaches (2005) Ethics and Information Technology, 7 (3), pp. 149-155; Allen, C., Varner, G., Zinser, J., Prolegomena to any future artificial moral agent (2000) Journal of Experimental & Theoretical Artificial Intelligence, 12 (3), pp. 251-261; Allen, C., Wallach, W., Moral machines: Contradition in terms of abdication of human responsibility? (2011) Robot ethics: The ethical and social implications of robotics, pp. 55-68. , Lin P, Abney K, Bekey GA, (eds), MIT Press, Cambridge; Anderson, M., Anderson, S., (2011) Machine ethics, , Cambridge University Press, Cambridge; Anderson, M., Anderson, S., Machine ethics: Creating an ethical intelligent agent (2007) AI Magazine, 28 (4), pp. 15-26; Anderson, M., Anderson, S., Robot be good: A call for ethical autonomous machines (2010) Scientific American, 303 (4), pp. 15-24. , https://franz.com/success/customer_apps/artificial_intelligence/EthEl/robot-be-good.PDF, https://doi.org/10.1038/scientificamerican1010-72; Boddington, P., (2017) Towards a Code of Ethics for Artificial Intelligence, , (Artificial Intelligence: Foundations, Theory, and Algorithms), Springer; 1st ed; Boden, A.M., (2016) AI: Its Nature and Future, , Oxford University Press; Bor, D., (2012) The Ravenous Brain: How the New Science of Consciousness Explains Our Insatiable Search for Meaning, , Basic Books, New York; Bryson, J., Robots should be slaves (2008) Close Engagements with Artificial Companions: Key Social, Psychological, Ethical and Design Issue, pp. 63-74. , https://books.google.nl/books?id=EPznZHeG89cC, Y. Wilks, Amsterdam: John Benjamins Publishing. Retrieved from, Accessed 7 Mar 2017; Dennett, D., (1991) Consciousness Explained, , New York â€“ Boston â€“ London, Little, Brown & Co; Dennett, D., Quining Qualia (1988) Consciousness in Contemporary Science, , Marcel, A. â€“ Bisiach, E. (eds.), New York, Oxford University Press; Dennett, D., (1998) Animal Consciousness: What Matters and Why. Brainstorms, , Cambridge, Mass, The MIT Press; Dignum, V., Responsible Artificial intelligence: Designing AI for human values (2017) ITU Journal, 1. , https://www.itu.int/en/journal/001/Documents/itu2017-1.pdf, ICT Discoveries, Special Issue, 25 Sept. 2017; Domingos, P., (2015) The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World, , Basic Books; Gunkel, D.J., A vindication of the rights of machines (2014) Philosophy & Technology, 27 (1), pp. 113-132; Motlagh, H.N., Wrabl, J.O., Li, J., Hilser, V.J., The ensemble nature of allostery (2014) Nature, 508 (7496), pp. 331-339; Chalmers, D., Facing Up to the Problem of Consciousness (1995) Journal of Consciousness Studies, 2 (3), pp. 200-219; Chalmers, D., (1996) The Conscious Mind, , Oxford University Press, Oxford; Koch, C., The Quest for Consciousness: A Neurobiological Approach (2004) Roberts & Company Publishers; Lin, P., Abney, K., Bekey, G., (2012) Robot ethics: the ethical and social implications of robotics, , MIT Press, Cambridge; Lore, T., Galen, M., Reich, X.Z., Wang, G.E., Smith, Z., Tao, R.S.A., Bin, R., Kishantoniou, M., (2017) Mouth-Clicks Used by Blind Expert Human Echolocators â€“ Signal Description and Model Based Signal Synthesis, PLOS Computational Biology, Published: August 31, , https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005670; Moor, J.H., The nature, importance, and difficulty of machine ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 18-21; Moor, J., Four kinds of ethical robots (2009) Philosophy Now, (72), pp. 12-14. , https://philosophynow.org/issues/72/Four_Kinds_of_Ethical_Robots, Retrieved from; Nagel, T., What Is It Like to Be a Bat? (1974) Philosophical Review, 4, pp. 435-450; Norton, J.D., On Th ought Experiments: Is Th ere More to the Argument? (2004) Philosophy of Science, 71 (5), pp. 1139-1151. , http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.591.7268&rep=rep1&type=pdf, a,., n., http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.591.7268&rep=rep1&type=pdf; Norton, J.D., Why Thought Experiments Do Not Transcend Empiricism (2004) Contemporary Debates in Philosophy of Science, pp. 44-66. , Hitchcock C, (ed), Blackwell, Oxford; Ramachandran, V.S., (2004) A Brief Tour of Human Consciousness: From Impostor Poodles to Purple Numbers, , Pi Press; Ramachandran, V.S., The Tell-Tale Brain: A Neuroscientistâ€™s Quest for What Makes Us Human, W. W (2011) Norton & Company; 1 Edition; Wallach, W., Implementing moral decision making faculties in computers and robots (2007) AI & Society, 22 (4), pp. 463-475},
correspondence_address1={Hauer, T.; Department of Philosophy, Hornopotocna street 23, Slovakia; email: tomas.hauer@truni.sk},
editor={NA},
publisher={Springer},
issn={01472011},
isbn={NA},
language={English},
abbrev_source_title={Society},
document_type={Article},
source={Scopus},
number={4},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Behdadi2020195,
type={ARTICLE},
author={Behdadi, D. and Munthe, C.},
title={A Normative Approach to Artificial Moral Agency},
journal={Minds and Machines},
year={2020},
volume={30},
pages={195-218},
doi={10.1007/s11023-020-09525-8},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085308640&doi=10.1007%2fs11023-020-09525-8&partnerID=40&md5=37802c7642e162b5f4afbaa3e941953c},
affiliation={Department of Philosophy, Linguistics and Theory of Science, University of Gothenburg, Box 200, GÃ¶teborg, 40530, Sweden},
abstract={This paper proposes a methodological redirection of the philosophical debate on artificial moral agency (AMA) in view of increasingly pressing practical needs due to technological development. This â€œnormative approachâ€ suggests abandoning theoretical discussions about what conditions may hold for moral agency and to what extent these may be met by artificial entities such as AI systems and robots. Instead, the debate should focus on how and to what extent such entities should be included in human practices normally assuming moral agency and responsibility of participants. The proposal is backed up by an analysis of the AMA debate, which is found to be overly caught in the opposition between so-called standard and functionalist conceptions of moral agency, conceptually confused and practically inert. Additionally, we outline some main themes of research in need of attention in light of the suggested normative approach to AMA. Â© 2020, The Author(s).},
author_keywords={Artificial agency;  Artificial intelligence;  Artificial moral agent;  Consciousness;  Demarcation problem;  Machine consciousness;  Machine ethics;  Moral agency;  Moral machine;  Moral responsibility;  Moral status},
keywords={Artificial intelligence, AI systems;  Human practices;  Technological development, Philosophical aspects},
references={Adams, T.K., Future warfare and the decline of human decisionmaking (2001) Parameters, 31 (4), pp. 57-71; Allen, C., Varner, G., Zinser, J., Prolegomena to any future artificial moral agent (2000) Journal of Experimental & Theoretical Artificial Intelligence, 12 (3), pp. 251-261; Anderson, S.L., Asimovâ€™s â€œthree laws of roboticsâ€ and machine metaethics (2008) AI & SOCIETY, 22 (4), pp. 477-493; Anderson, M., Anderson, S.L., Machine ethics: Creating an ethical intelligent agent (2007) AI Magazine, 28 (4), p. 15; Anderson, M., Anderson, S.L., Armen, C., (2004) Towards machine ethics, , Proceedings of AAAI; Annas, J., (2011) Intelligent virtue, , Oxford University Press, Oxford; Arkin, R.C., The case for ethical autonomy in unmanned systems (2010) Journal of Military Ethics, 9 (4), pp. 332-341; Asaro, P.M., What should we want from a robot ethic? (2006) International Review of Information Ethics, 6 (12), pp. 9-16; Asimov, I., Runaround (1942) Astounding Science Fiction, 29 (1), pp. 94-103; Bahrammirzaee, A., A comparative survey of artificial intelligence applications in finance: artificial neural networks, expert system and hybrid intelligent systems (2010) Neural Computing and Applications, 19 (8), pp. 1165-1195; Beavers, A.F., (2011) Moral Machines and the Threat of Ethical Nihilism, p. 333. , The ethical and social implications of robotics, Robot ethics; BjÃ¶rnsson, G., Persson, K., The explanatory component of moral responsibility (2012) NoÃ»s, 46 (2), pp. 326-354; BjÃ¶rnsson, G., Persson, K., A unified empirical account of responsibility judgments (2013) Philosophy and Phenomenological Research, 87 (3), pp. 611-639; Bringsjord, S., (1992) What Robots can and canâ€™t be, , Kluwer Academic, NEW York; Bringsjord, S., Ethical robots: the future can heed us (2007) AI & Society, 22 (4), pp. 539-550; Bryson, J.J., (2010) Robots should be slaves, pp. 63-74. , Key social, psychological, ethical and design issues, Close Engagements with Artificial Companions; Champagne, M., Tonkens, R., Bridging the responsibility gap in automated warfare (2013) Philosophy & Technology, 28 (1), pp. 125-137; Christman, J., Autonomy in moral and political philosophy (2015) The Stanford Encyclopedia of Philosophy, , http://plato.stanford.edu/archives/spr2015/entries/autonomy-moral, E. N. Zalta; Coeckelbergh, M., Virtual moral agency, virtual moral responsibility: on the moral significance of the appearance, perception, and performance of artificial agents (2009) AI & Society, 24 (2), pp. 181-189; Coeckelbergh, M., Moral appearances: emotions, robots, and human morality (2010) Ethics and Information Technology, 12 (3), pp. 235-241; Danaher, J., (2019) Automation and Utopia, , Harvard University Press, Cambridge, Mass; Davis, M., Ainâ€™t no one here but us social forcesâ€: constructing the professional responsibility of engineers (2012) Science and Engineering Ethics, 18 (1), pp. 13-34; Dennett, D.C., Mechanism and responsibility (1973) Essays on freedom of action, pp. 157-184. , Honderich Ed, (ed), Routledge and Kegan Paul, Abingdon; Dennett, D.C., Three kinds of intentional psychology (1987) The intentional stance, pp. 43-68. , Dennett DC, (ed), The MIT Press, Cambridge; Dodig-Crnkovic, G., Ã‡Ã¼rÃ¼klÃ¼, B., Robots: ethical by design (2011) Ethics and Information Technology, 14 (1), pp. 61-71; Dodig-Crnkovic, G., Persson, D., Sharing moral responsibility with robots: A pragmatic approach (2008) Frontiers in Artificial Intelligence And Applications, 173, p. 165; Dreyfus, H.L., Hubert, L., (1992) What computers still canâ€™t do: A critique of artificial reason, , MIT press, Cambridge; Eshleman, A., (2014) Moral Responsibility. In E. N. Zalta (Ed.), The stanford encyclopedia of philosophy (Summer 2014 ed.), , http://plato.stanford.edu/archives/sum2014/entries/moral-responsibility/; Etzioni, A., Pros and cons of autonomous weapons systems (with Oren Etzioni) (2018) Happiness is the wrong metric, pp. 253-263. , Springer, Cham; Floridi, L., Sanders, J.W., On the morality of artificial agents (2004) Minds and Machines, 14 (3), pp. 349-379; Friedman, B., Kahn, P.H., Human agency and responsible computing: Implications for computer system design (1992) Journal of Systems and Software, 17 (1), pp. 7-14; Gerdes, A., Ã˜hrstrÃ¸m, P., Issues in robot ethics seen through the lens of a moral Turing test (2015) Journal of Information, Communication and Ethics in Society, 13 (2), pp. 98-109; Gladden, M.E., The diffuse intelligent other: An ontology of nonlocalizable robots as moral and legal actors (2016) Social robots: Boundaries, potential, challenges, pp. 177-198. , NÃ¸rskov M, (ed), Ashgate, Burlington, VT; Grodzinsky, F.S., Miller, K.W., Wolf, M.J., The ethics of designing artificial agents (2008) Ethics and Information Technology, 10 (2-3), pp. 115-121; Gunkel, D.J., A vindication of the rights of machines (2014) Philosophy & Technology, 27 (1), pp. 113-132; HÃ¤ggstrÃ¶m, H., (2016) Here be dragons: science, technology and the future of humanity, , Oxford University Press, Oxford; HellstrÃ¶m, T., On the moral responsibility of military robots (2012) Ethics and Information Technology, 15 (2), pp. 99-107; (2019) Ethics guidelines for trustworthy AI. European Commission, , https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai, Retrieved 2020-04-05 from; Himma, K.E., Artificial agency, consciousness, and the criteria for moral agency: what properties must an artificial agent have to be a moral agent? (2009) Ethics and Information Technology, 11 (1), pp. 19-29; Holroyd, J., Two ways of socializing moral responsibility: Circumstantialism versus scaffolded-responsiveness (2018) Social Dimensions of Moral Responsibility, pp. 137-162. , Hutchison K, Mackenzie C, Oshana M, (eds), Oxford University Press, Oxford; Irrgang, B., Ethical acts in robotics (2006) Ubiquity, 7, p. 34; Johansson, L., The functional morality of robots (2010) International Journal of Technoethics, 1 (4), pp. 65-73; Johnson, D., Computer systems: Moral entities but not moral agents (2006) Ethics and Information Technology, 8 (4), pp. 195-204; Johnson, D.G., Miller, K.W., Un-making artificial moral agents (2008) Ethics and Information Technology, 10 (2-3), pp. 123-133; Johnson, D.G., Powers, T.M., Computer systems and responsibility: A normative look at technological complexity (2005) Ethics and Information Technology, 7 (2), pp. 99-107; Johnson, D., Powers, T.M., Computers as surrogate agents (2008) Information technology and moral philosophy, 2008, pp. 251-269; Kolodny, N., John, B., A (2016) Stanford Encyclopedia of Philosophy., , http://plato.stanford.edu/archives/spr2016/entries/rationality-instrumental/; Korsgaard, C.M., Fellow creatures: Kantian ethics and our duties to animals (2004) Tanner Lectures on Human Values, 25, p. 77; Lin, P., Bekey, G., Abney, K., (2008) Autonomous military robotics: Risk, ethics, and design, , California Polytechnic State Univ San Luis Obispo, DTIC Document; Lokhorst, G.-J., van den Hoven, J., Responsibility for military robots (2012) Robot ethics: The ethical and social implications of robotics, pp. 145-156. , Lin IP, Bekey GA, Abney K, (eds), MIT Press, Cambridge; Macnamara, C., (2015) Blame, communication, and morally responsible agency, p. 211. , New Essays, The Nature of Moral Responsibility; Matheson, B., (2012) Manipulation, moral responsibility, and machines, p. 11. , AI, Ethics and Moral Responsibility, The Machine Question; Matthias, A., The responsibility gap: Ascribing responsibility for the actions of learning automata (2004) Ethics and Information Technology, 6 (3), pp. 175-183; McDermott, D., (2008) Why Ethics is a High Hurdle for AI, , Citeseer; McGeer, V., Mind-making practices: the social infrastructure of self-knowing agency and responsibility (2015) Philosophical Explorations, 18 (2), pp. 259-281; McKennajustin, M.A.C.D., Compatibilism (2015) The Stanford Encyclopedia of Philosophy, , http://plato.stanford.edu/archives/sum2015/entries/compatibilism/, E. N. Zalta; Moor, J.M., The nature, importance, and difficulty of machine ethics (2006) Intelligent Systems, IEEE, 21 (4), pp. 18-21; Moor, J., Four kinds of ethical robots (2009) Philosophy Now, 72, pp. 12-14; Musen, M.A., Middleton, B., Greenes, R.A., Clinical decision-support systems (2014) Biomedical informatics, pp. 643-674. , Springer, Berlin; Nadeau, J.E., (2006) Only androids can be ethical (Thinking about android epistemology), , (ed), MIT Press, Cambridge; Nagel, T., What is it like to be a bat? (1974) The philosophical review, pp. 435-450; Nagenborg, M., Artificial moral agents: an intercultural perspective (2007) International Review of Information Ethics, 7 (9), pp. 129-133; Noone, G.P., Noone, D.C., The debate over autonomous weapons systems (2015) Case W. Res. J. Intâ€™l L., 47, p. 25; Noorman, M., Responsibility practices and unmanned military technologies (2014) Science and Engineering Ethics, 20 (3), pp. 809-826; Noorman, M., Johnson, D.G., Negotiating autonomy and responsibility in military robots (2014) Ethics and Information Technology, 16 (1), pp. 51-62; Nyholm, S., Attributing Agency to Automated Systems: Reflections on Human-Robot Collaborations and Responsibility-Loci (2018) Science and Engineering Ethics, 24 (4), pp. 1201-1219; Nyhom, S., (2020) Humans and robots: Ethics, agency, and anthropomorphism, , Rowman & Littlefield, New York; Oâ€™Connor, T., (2016) Free Will, , http://plato.stanford.edu/archives/sum2016/entries/freewill, E. N. Zalta, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University; Parthemore, J., Whitby, B., What makes any agent a moral agent? Reflections on machine consciousness and moral agency (2013) International Journal of Machine Consciousness, 5 (2), pp. 105-129; Picard, R.W., (1997) Affective computing, 252. , MIT Press, Cambridge; Pontier, M., Hoorn, J., (2012) Toward machines that behave ethically better than humans do. In Proceedings of the Annual Meeting of the Cognitive Science Society (Vol. 34, No. 34); Powers, T.M., Prospects for a Kantian machine (2006) Intelligent Systems, IEEE, 21 (4), pp. 46-51; Powers, T.M., On the moral agency of computers (2013) Topoi, 32 (2), pp. 227-236; Purves, D., Jenkins, R., Strawser, B.J., Autonomous machines, moral judgment, and acting for the right reasons (2015) Ethical Theory and Moral Practice, 18 (4), pp. 851-872; Samuelsson, L., On the demarcation problem and the possibility of environmental ethics: A refutation of â€œa refutation of environmental ethics (2010) Environmental Ethics, 32 (3), pp. 247-265; Schulzke, M., Autonomous weapons and distributed responsibility (2013) Philosophy & Technology, 26 (2), pp. 203-219; Shaw, E., Pereboom, D., Caruso, G.D., (2019) Free will skepticism in law and society, , (eds), Cambridge University Press, Cambridge; Sheikhtaheri, A., Sadoughi, F., Dehaghi, Z.H., Developing and using expert systems and neural networks in medicine: a review on benefits and challenges (2014) Journal of Medical Systems, 38 (9), p. 110; Shen, S., The curious case of human-robot morality (2011) Proceedings of the 6Th International Conference on Human-Robot Interaction, pp. 249-250. , ACM; Singer, P., (2011) Practical ethics, , 3, Cambridge University Press, Cambridge; Singer, A.E., (2013) Corporate and Artificial Moral Agency, pp. 4525-4531; Sliwa, P., Moral Worth and Moral Knowledge (2015) Philosophy and Phenomenological Research; Sparrow, R., Killer robots (2007) Journal of applied philosophy, 24 (1), pp. 62-77; Stahl, B.C., Information, ethics, and computers: The problem of autonomous moral agents (2004) Minds and Machines, 14 (1), pp. 67-83; Stahl, B.C., Responsible computers? A case for ascribing quasi-responsibility to computers independent of personhood or agency (2006) Ethics and Information Technology, 8 (4), pp. 205-213; Sullins, J.P., When is a robot a moral agent? (2006) International Review of Information Ethics, 6 (12), pp. 23-30; Sullins, J.P., RoboWarfare: can robots be more ethical than humans on the battlefield? (2010) Ethics and Information Technology, 12 (3), pp. 263-275; Swiatek, M.S., Intending to err: the ethical challenge of lethal, autonomous systems (2012) Ethics and Information Technology, 14 (4), pp. 241-254; Tonkens, R., A challenge for machine ethics (2009) Minds and Machines, 19 (3), pp. 421-438; Tonkens, R., Out of character: on the creation of virtuous machines (2012) Ethics and Information Technology, 14 (2), pp. 137-149; Torrance, S., Ethics and consciousness in artificial agents (2007) AI & SOCIETY, 22 (4), pp. 495-521; Vargas, M., (2013) Building better beings: A theory of moral responsibility, , OUP Oxford, Oxford; Verbeek, P.P., (2011) Moralizing technology: Understanding and designing the morality of things, , University of Chicago Press, Chicago; Versenyi, L., Can robots be moral? (1974) Ethics, 84 (3), pp. 248-259; Veruggio, G., Operto, F., Roboethics: Social and ethical implications of robotics (2008) Springer handbook of robotics, pp. 1499-1524. , Springer, Berlin; Wallace, R.J., Practical Reason (2014) The Stanford Encyclopedia of Philosophy, , http://plato.stanford.edu/archives/sum2014/entries/practical-reason/, E. N. Zalta; Wallach, W., Allen, C., (2008) Moral machines: Teaching robots right from wrong, , Oxford University Press, Oxford; Wang, F.-Y., Letâ€™s Go: From AlphaGo to parallel intelligence (2016) Science & Technology Review, 34 (7), pp. 72-74; Warren, M.A., (1997) Moral status: Obligations to persons and other living things, , Clarendon Press, Oxford; Yampolskiy, R.V., Artificial intelligence safety engineering: Why machine ethics is a wrong approach (2013) Philosophy and theory of artificial intelligence, pp. 389-396. , Springer, Berlin},
correspondence_address1={Behdadi, D.; Department of Philosophy, Box 200, Sweden; email: dorna.behdadi@gu.se},
editor={NA},
publisher={Springer},
issn={09246495},
isbn={NA},
language={English},
abbrev_source_title={Minds Mach},
document_type={Article},
source={Scopus},
number={2},
art_number={NA},
funding_details={VetenskapsrÃ¥detVetenskapsrÃ¥det,Â VR,Â 2014-40},
coden={MMACE},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Haas2020219,
type={ARTICLE},
author={Haas, J.},
title={Moral Gridworlds: A Theoretical Proposal for Modeling Artificial Moral Cognition},
journal={Minds and Machines},
year={2020},
volume={30},
pages={219-246},
doi={10.1007/s11023-020-09524-9},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084153579&doi=10.1007%2fs11023-020-09524-9&partnerID=40&md5=3aa27a3d14d7f2a9bd1a5be6c260455b},
affiliation={Department of Philosophy, Rhodes College, Memphis, 38112, United States},
abstract={I describe a suite of reinforcement learning environments in which artificial agents learn to value and respond to moral content and contexts. I illustrate the core principles of the framework by characterizing one such environment, or â€œgridworld,â€ in which an agent learns to trade-off between monetary profit and fair dealing, as applied in a standard behavioral economic paradigm. I then highlight the core technical and philosophical advantages of the learning approach for modeling moral cognition, and for addressing the so-called value alignment problem in AI. Â© 2020, Springer Nature B.V.},
author_keywords={Artificial intelligence;  Fairness;  Machine ethics;  Moral AI;  Moral cognition;  Moral psychology;  Reinforcement learning},
keywords={Computer aided instruction;  Economic; social effects;  Philosophical aspects, Alignment Problems;  Artificial agents;  Behavioral economics;  Fair dealing;  Learning approach;  Trade off, Reinforcement learning},
references={Adamson, G., Havens, J.C., Chatila, R., Designing a value-driven future for ethical autonomous and intelligent systems (2019) Proceedings of the IEEE, 107 (3), pp. 518-525; Allen, C., Smit, I., Wallach, W., Artificial morality: Top-down, bottom-up, and hybrid approaches (2005) Ethics and Information Technology, 7 (3), pp. 149-155; Allen, C., Wallach, W., Moral machines: Contradiction in terms or abdication of human responsibility (2012) Robot Ethics: The Ethical and Social Implications of Robotics, pp. 55-68. , Cambridge, MIT Press; Alvard, M.S., The ultimatum game, fairness, and cooperation among big game hunters (2004) Foundations of human sociality, pp. 413-435. , Henrich J, Boyd R, Bowles S, Camerer C, Fehr E, Gintis H, (eds), Oxford University Press, Oxford; Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., ManÃ©, D., (2016) Concrete Problems in AI Safety, , . arXiv preprint arXiv; Anderson, M., Anderson, S.L., GenEth: a general ethical dilemma analyzer (2018) Paladyn, Journal of Behavioral Robotics, 9 (1), pp. 337-357; Anderson, M., Anderson, S.L., Armen, C., MedEthEx: A prototype medical ethics advisor (2006) Proceedings of the National Conference on Artificial Intelligence, 21 (2), p. 1759. , Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999; Anderson, M., Anderson, S.L., Berenz, V., A value-driven eldercare robot: Virtual and physical instantiations of a case-supported principle-based behavior paradigm (2019) Proceedings of the IEEE, 107 (3), pp. 526-540; Arnold, T., Kasenberg, D., Scheutz, M., Value alignment or misalignmentâ€”What will keep systems accountable? (2017) Workshops at the Thirty-First AAAI Conference on Artificial Intelligence; Barocas, S., Selbst, A.D., Big dataâ€™s disparate impact (2016) California Law Review, 104, p. 671; Bechtel, W., Mundale, J., Multiple realizability revisited: Linking cognitive and neural states (1999) Philosophy of Science, 66 (2), pp. 175-207; Bengio, Y., LeCun, Y., Scaling learning algorithms towards AI (2007) Large-scale Kernel Machines, 34 (5), pp. 1-41; Berns, G.S., Bell, E., Capra, C.M., Prietula, M.J., Moore, S., Anderson, B., Ginges, J., Atran, S., The price of your soul: Neural evidence for the non-utilitarian representation of sacred values (2012) Philosophical Transactions of the Royal Society B: Biological Sciences, 367 (1589), pp. 754-762; Bigman, Y.E., Waytz, A., Alterovitz, R., Gray, K., Holding robots responsible: The elements of machine morality (2019) Trends in Cognitive Sciences, 23 (5), pp. 365-368; Boksem, M.A., De Cremer, D., Fairness concerns predict medial frontal negativity amplitude in ultimatum bargaining (2010) Social Neuroscience, 5 (1), pp. 118-128; Bonnefon, J.F., Shariff, A., Rahwan, I., The social dilemma of autonomous vehicles (2016) Science, 352 (6293), pp. 1573-1576; Borenstein, J., Arkin, R., Robots, ethics, and intimacy: The need for scientific research (2019) On the Cognitive, Ethical, and Scientific Dimensions of Artificial Intelligence, 134, pp. 299-309. , D. Berkich & M. V. dâ€™Alfonso, Springer; Botvinick, M., Ritter, S., Wang, J.X., Kurth-Nelson, Z., Blundell, C., Hassabis, D., Reinforcement learning, fast and slow (2019) Trends in Cognitive Sciences; Bremner, P., Dennis, L.A., Fisher, M., Winfield, A.F., On proactive, transparent, and verifiable ethical reasoning for robots (2019) Proceedings of the IEEE, 107 (3), pp. 541-561; Brown, D., (1991) Human universals, , McGraw-Hill, New York; Brumbaugh, S.M., Sanchez, L.A., Nock, S.L., Wright, J.D., Attitudes toward gay marriage in states undergoing marriage law transformation (2008) Journal of Marriage and Family, 70 (2), pp. 345-359; Cave, S., Nyrup, R., Vold, K., Weller, A., Motivations and risks of machine ethics (2018) Proceedings of the IEEE, 107 (3), pp. 562-574; Cervantes, J.A., LÃ³pez, S., RodrÃ­guez, L.F., Cervantes, S., Cervantes, F., Ramos, F., Artificial moral agents: A survey of the current status (2019) Science and Engineering Ethics; Corradi-Dellâ€™Acqua, C., Civai, C., Rumiati, R.I., Fink, G.R., Disentangling self-and fairness-related neural mechanisms involved in the ultimatum game: an fMRI study (2013) Social Cognitive and Affective Neuroscience, 8 (4), pp. 424-431; Crawford, K., Calo, R., There is a blind spot in AI research (2016) Nature, 538 (7625), pp. 311-313; Crockett, M.J., Models of morality (2013) Trends in Cognitive Sciences, 17 (8), pp. 363-366; Crockett, M.J., How formal models can illuminate mechanisms of moral judgment and decision making (2016) Current Directions in Psychological Science, 25 (2), pp. 85-90; Crockett, M.J., Siegel, J.Z., Kurth-Nelson, Z., Dayan, P., Dolan, R.J., Moral transgressions corrupt neural representations of value (2017) Nature Neuroscience, 20 (6), p. 879; Cushman, F., From moral concern to moral constraint (2015) Current Opinion in Behavioral Sciences, 3, pp. 58-62; Debove, S., Baumard, N., AndrÃ©, J.B., Models of the evolution of fairness in the ultimatum game: A review and classification (2016) Evolution Andhuman Behavior, 37 (3), pp. 245-254; De Sio, F.S., Killing by autonomous vehicles and the legal doctrine of necessity (2017) Ethical Theory and Moral Practice, 20 (2), pp. 411-429; Dennis, L., Fisher, M., Slavkovik, M., Webster, M., Formal verification of ethical choices in autonomous systems (2016) Robotics and Autonomous Systems, 77, pp. 1-14; Dietrich, F., List, C., What matters and how it matters: a choice-theoretic representation of moral theories (2017) Philosophical Review, 126 (4), pp. 421-479; Doran, D., Schulz, S., Besold, T.R., (2017) What does explainable AI really mean? A new conceptualization of perspectives, , . arXiv preprint arXiv; Doris, J.M., (2002) Lack of character: Personality and moral behavior, , Cambridge University Press, Cambridge; Dretske, F., If you can't make one, you don't know how it works (1994) Midwest Studies in Philosophy, 19, pp. 468-482; Driver, J., Normative ethics (2005) The Oxford Handbook of Contemporary Philosophy, pp. 31-62. , Jackson F, Smith M, (eds), Oxford University Press, Oxford; Elgin, C.Z., (2017) True enough, , MIT Press, Cambridge; Everitt, T., Krakovna, V., Orseau, L., Hutter, M., Legg, S., (2017) Reinforcement learning with a corrupted reward channel, , . arXiv preprint. arXiv; Everitt, T., Lea, G., Hutter, M., (2018) AGI Safety Literature Review, , . arXiv preprint arXiv; Farrell, J., Cheap talk, coordination, and entry (1987) The Rand Journal of Economics, 18 (1), pp. 34-39; Fehr, E., Schmidt, K., Theories of fairness and reciprocityâ€“evidence and economic applications (2003) Advances in Economics and Econometrics, 8Th World Congress, Econometric Society Monographs; Feng, C., Luo, Y.J., Krueger, F., Neural signatures of fairness-related normative decision making in the ultimatum game: A coordinate-based meta-analysis (2015) Human Brain Mapping, 36 (2), pp. 591-602; Flanagan, O., Sarkissian, H., Wong, D., Naturalizing ethics (2007) Moral psychology, Vol. 1. The evolution of morality: Adaptations and innateness, pp. 1-25. , Sinnott-Armstrong W, (ed), MIT Press, Cambridge; Fleetwood, J., Public health, ethics, and autonomous vehicles (2017) American Journal of Public Health, 107 (4), pp. 532-537; Forsythe, R., Horowitz, J.L., Savin, N.E., Sefton, M., Fairness in simple bargaining experiments (1994) Games and Economic Behavior, 6 (3), pp. 347-369; GÃ¡bor, Z., KalmÃ¡r, Z., SzepesvÃ¡ri, C., Multi-criteria reinforcement learning (1998) ICML, 98, pp. 197-205. , July, Chicago; Glimcher, P.W., (2011) Foundations of neuroeconomic analysis, , OUP USA, Oxford; Gogoll, J., MÃ¼ller, J.F., Autonomous cars: in favor of a mandatory ethics setting (2017) Science and Engineering Ethics, 23 (3), pp. 681-700; GÃ¼th, W., Schmittberger, R., Schwarze, B., An experimental analysis of ultimatum bargaining (1982) Journal of Economic Behavior & Organization, 3 (4), pp. 367-388; Hadfield-Menell, D., Milli, S., Abbeel, P., Russell, S.J., Dragan, A., Inverse reward design (2017) Advances in Neural Information Processing Systems, pp. 6765-6774; Hartmann, S., The world as a process: Simulations in the natural and social sciences. in Hegselmann (1996) Mueller, and Troitzsch, 1996, pp. 77-100; Hass, J., Valuation mechanisms in moral cognition (2019) Behavioral and Brain Sciences, , https://doi.org/10.1017/S0140525X18002686; Henrich, J., Ensminger, J., McElreath, R., Barr, A., Barrett, C., Bolyanatz, A., Cardenas, J.C., Lesorogol, C., Markets, religion, community size, and the evolution of fairness and punishment (2010) Science, 327 (5972), pp. 1480-1484; Henrich, J., Heine, S.J., Norenzayan, A., The weirdest people in the world? (2010) Behavioral and Brain Sciences, 33 (2-3), pp. 61-83; Henrich, J., Heine, S.J., Norenzayan, A., Most people are not WEIRD (2010) Nature, 466 (7302), p. 29; Himmelreich, J., Never mind the trolley: The ethics of autonomous vehicles in mundane situations (2018) Ethical Theory and Moral Practice, 21 (3), pp. 669-684; Holstein, K., Wortman Vaughan, J., DaumÃ©, H., Dudik, M., Wallach, H., Improving fairness in machine learning systems: What do industry practitioners need? (2019) Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1-16; Honarvar, A.R., Ghasem-Aghaee, N., Casuist BDI-agent: A new extended BDI architecture with the capability of ethical reasoning (2009) International Conference on Artificial Intelligence and Computational Intelligence, pp. 86-95. , Berlin, Heidelberg, Springer; Hoppenbrouwers, S.S., Van der Stigchel, S., Slotboom, J., Dalmaijer, E.S., Theeuwes, J., Disentangling attentional deficits in psychopathy using visual search: Failures in the use of contextual information (2015) Personality and Individual Differences, 86, pp. 132-138; Howard, D., Muntean, I., Artificial moral cognition: Moral functionalism and autonomous moral agency (2017) Philosophy and Computing, pp. 121-159. , Cham, Springer; Iyer, R., Li, Y., Li, H., Lewis, M., Sundar, R., Sycara, K., Transparency and explanation in deep reinforcement learning neural networks (2018) Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp. 144-150; Jobin, A., Ienca, M., Vayena, E., The global landscape of AI ethics guidelines (2019) Nature Machine Intelligence, 1 (9), pp. 389-399; Kahneman, D., Knetsch, J.L., Thaler, R., Fairness as a constraint on profit seeking: Entitlements in the market (1986) The American Economic Review, pp. 728-741; Kamm, F.M., (2008) Intricate ethics: Rights, responsibilities, and permissable harm, , Oxford University Press, Oxford; Ku, H.H., Hung, Y.C., Framing effects of per-person versus aggregate prices in group meals (2019) Journal of Consumer Behaviour, 18 (1), pp. 43-52; Larson, J., Mattu, S., Kirchner, L., Angwin, J., How we analyzed the COMPAS recidivism algorithm (2016) ProPublica, 5, p. 9; Leike, J., Martic, M., Krakovna, V., Ortega, P.A., Everitt, T., Lefrancq, A., Orseau, L., Legg, S., (2017) AI safety gridworlds; Liu, C., Xu, X., Hu, D., Multiobjective reinforcement learning: A comprehensive overview (2014) IEEE Transactions on Systems, Man, and Cybernetics: Systems, 45 (3), pp. 385-398; Lugo, L., Cooperman, A., (2013) A Portrait of Jewish Americans: Findings from a Pew Research Center Survey of U.S. Jews, , https://www.pewforum.org/2013/10/01/jewish-american-beliefs-attitudes-culture-survey/; Malle, B.F., Integrating robot ethics and machine morality: The study and design of moral competence in robots (2016) Ethics and Information Technology, 18 (4), pp. 243-256; Mannor, S., Shimkin, N., A geometric approach to multi-criterion reinforcement learning (2004) Journal of Machine Learning Research, 5, pp. 325-360; Marchetti, A., Baglio, F., Massaro, D., Griffanti, L., Rossetto, F., Sangiuliano Intra, F., Valle, A., Castelli, I., Can psychological labels influence the decision-making process in an unfair condition? Behavioral and neural evidences using the ultimatum game task (2019) Journal of Neuroscience, Psychology, and Economics, 12 (2), p. 105; May, J., (2018) Regard for reason in the moral mind, , Oxford University Press, Oxford; May, J., Defending optimistic rationalism: A reply to commentators (2019) Behavioral and Brain Sciences; Millar, J., Lin, P., Abney, K., Bekey, G.A., (2017) Ethics settings for autonomous vehicles, pp. 20-34. , MIT Press, Cambridge; Moor, J.H., The nature, importance, and difficulty of machine ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 18-21; Morgan, M.S., Learning from models (1999) Ideas in Context, 52, pp. 347-388; Nowak, M.A., Page, K.M., Sigmund, K., Fairness versus reason in the ultimatum game (2000) Science, 289 (5485), pp. 1773-1775; Nyholm, S., Smids, J., The ethics of accident-algorithms for self-driving cars: An applied trolley problem? (2016) Ethical Theory and Moral Practice, 19 (5), pp. 1275-1289; Omohundro, S.M., The basic AI drives (2008) In AGI, 171, pp. 483-492; Padoa-Schioppa, C., Neurobiology of economic choice: A good-based model (2011) Annual Review of Neuroscience, 34, pp. 333-359; Picard, R., (1997) Affective computing, , MIT Press, Cambridge; Rand, D.G., Tarnita, C.E., Ohtsuki, H., Nowak, M.A., Evolution of fairness in the one-shot anonymous Ultimatum Game (2013) Proceedings of the National Academy of Sciences, 110 (7), pp. 2581-2586; Roff, H., Expected Utilitarianism, , manuscript; Rosen, J.B., Rott, E., Ebersbach, G., Kalbe, E., Altered moral decision-making in patients with idiopathic Parkinsonâ€™s disease (2015) Parkinsonism & Related Disorders, 21 (10), pp. 1191-1199; Russell, S., Dewey, D., Tegmark, M., Research priorities for robust and beneficial artificial intelligence (2015) Ai Magazine, 36 (4), pp. 105-114; Russell, S.J., Norvig, P., (2016) Artificial intelligence: A modern approach, , Pearson Education Limited, Malaysia; Sanfey, A.G., Rilling, J.K., Aronson, J.A., Nystrom, L.E., Cohen, J.D., The neural basis of economic decision-making in the ultimatum game (2003) Science, 300 (5626), pp. 1755-1758; Scheutz, M., Malle, B.F., (2017) Moral Robots. the Routledge Handbook of Neuroethics, , Nueva York, Routledge/Taylor & Francis; Schroeder, T., Roskies, A.L., Nichols, S.B., Moral motivation (2010) The Moral Psychology Handbook, , Doris J, (ed), Oxford University Press, Oxford; Shenhav, A., Greene, J.D., Moral judgments recruit domain-general valuation mechanisms to integrate representations of probability and magnitude (2010) Neuron, 67 (4), pp. 667-677; Shevlin, H., De-Skilling and Social Necessity, , manuscript; Sinnott-Armstrong, W., Mallon, R., Mccoy, T., Hull, J.G., Intention, temporal order, and moral judgments (2008) Mind & Language, 23 (1), pp. 90-106; Soares, N., Fallenstein, B., Armstrong, S., Yudkowsky, E., Corrigibility (2015) Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence.; Sripada, C.S., Stich, S., A framework for the psychology of norms (2005) The Innate Mind, 2, pp. 280-301; Sripada, C.S., Stich, S., A framework for the psychology of norms (2006) The Innate Mind, 2, pp. 280-301; Sterelny, K., Fraser, B., Evolution and moral realism (2017) The British Journal for the Philosophy of Science, 68 (4), pp. 981-1006; Sutton, R.S., (2019) The Bitter Lesson, , http://www.incompleteideas.net/IncIdeas/BitterLesson.html; Sutton, R.S., Barto, A.G., (1998) Introduction to Reinforcement Learning, 135. , Cambridge, MIT press; Sutton, R.S., Barto, A.G., (2018) Reinforcement Learning: An Introduction, , MIT press; Taylor, J., Yudkowsky, E., LaVictoire, P., Critch, A., (2016) Alignment for advanced machine learning systems, , Machine Intelligence Research Institute, Berkeley; Thaler, R.H., Anomalies: The ultimatum game (1988) Journal of economic perspectives, 2 (4), pp. 195-206; Tracer, D., Market integration, reciprocity and fairness in rural papua new guinea: Results from a twovillage ultimatum game study (2004) Artefactual Field Experiments 00112, , https://ideas.repec.org/p/feb/artefa/00112.html, The Field Experiments Website. Available online at; Vallor, S., Moral deskilling and upskilling in a new machine age: Reflections on the ambiguous future of character (2015) Philosophy & Technology, 28 (1), pp. 107-124; Vamplew, P., Dazeley, R., Foale, C., Firmin, S., Mummery, J., Human-aligned artificial intelligence is a multiobjective problem (2018) Ethics and Information Technology, 20 (1), pp. 27-40; Vanderelst, D., Winfield, A., An architecture for ethical robots inspired by the simulation theory of cognition (2018) Cognitive Systems Research, 48, pp. 56-66; van Moffaert, K., Drugan, NowÃ©, M.M.A., Hypervolume-based multi-objective reinforcement learning (2013) International Conference on Evolutionary Multi-Criterion Optimization, pp. 352-366. , In, (pp,), Springer, Berlin, Heidelberg; Van Moffaert, K., NowÃ©, A., Multi-objective reinforcement learning using sets of pareto dominating policies (2014) The Journal of Machine LearningResearch, 15 (1), pp. 3483-3512; Wallach, W., Allen, C., (2008) Moral machines: Teaching robots right from wrong, , Oxford University Press, Oxford; Wallach, W., Franklin, S., Allen, C., A conceptual and computational model of moral decision making in human and artificial agents (2010) Topics in Cognitive Science, 2 (3), pp. 454-485; Wallach, W., Marchant, G., Toward the agile and comprehensive international governance of AI and Robotics (2019) Proceedings of the IEEE, 107 (3), pp. 505-508; Wei, C., Zheng, L., Che, L., Cheng, X., Li, L., Guo, X., Social support modulates neural responses to unfairness in the ultimatum game (2018) Frontiers in Psychology, 9, p. 182; Winfield, A., (2019) An Updated round up of Ethical Principles of Robotics and AI, , http://alanwinfield.blogspot.com/2019/04/an-updated-round-up-of-ethical.html?m=1, [Blog post]. Retrieved from; Winfield, A.F., Michael, K., Pitt, J., Evers, V., Machine ethics: the design and governance of ethical AI and autonomous systems (2019) Proceedings of the IEEE, 107 (3), pp. 509-517; Wolf, S., Moral saints (1982) The Journal of Philosophy, 79 (8), pp. 419-439; Woodward, J., (2003) Making things happen: A theory of causal explanation, , Oxford University Press, Oxford; Yang, R., Sun, X., Narasimhan, K., A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation (2019) Advances in Neural Information Processing Systems, pp. 14610-14621; Zhong, S., Israel, S., Shalev, I., Xue, H., Ebstein, R.P., Dopamine D4 receptor gene associated with fairness preference in ultimatum game (2010) PLoSONE, 5 (11)},
correspondence_address1={Haas, J.; Department of Philosophy, United States; email: juliashaas@gmail.com},
editor={NA},
publisher={Springer},
issn={09246495},
isbn={NA},
language={English},
abbrev_source_title={Minds Mach},
document_type={Article},
source={Scopus},
number={2},
art_number={NA},
funding_details={NA},
coden={MMACE},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Cervantes2020501,
type={ARTICLE},
author={Cervantes, J.-A. and LÃ³pez, S. and RodrÃ­guez, L.-F. and Cervantes, S. and Cervantes, F. and Ramos, F.},
title={Artificial Moral Agents: A Survey of the Current Status},
journal={Science and Engineering Ethics},
year={2020},
volume={26},
pages={501-532},
doi={10.1007/s11948-019-00151-x},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075200736&doi=10.1007%2fs11948-019-00151-x&partnerID=40&md5=99fe752fd03d7d4d10a6f7e8635b2f6b},
affiliation={Department of Computer Science and Engineering, Centro Universitario de los Valles, Universidad de Guadalajara, Carretera Guadalajara â€“ Ameca Km. 45.5, Ameca, 46600, Mexico; Department of Computer Science, Instituto TecnolÃ³gico de Sonora, Sonora, Mexico; Department of Electronics, Systems and Informatics, Instituto TecnolÃ³gico y de Estudios Superiores de Occidente, Tlaquepaque, Mexico; Department of Computer Science, Centro de InvestigaciÃ³n y de Estudios Avanzados del Instituto PolitÃ©cnico Nacional, Guadalajara, Mexico},
abstract={One of the objectives in the field of artificial intelligence for some decades has been the development of artificial agents capable of coexisting in harmony with people and other systems. The computing research community has made efforts to design artificial agents capable of doing tasks the way people do, tasks requiring cognitive mechanisms such as planning, decision-making, and learning. The application domains of such software agents are evident nowadays. Humans are experiencing the inclusion of artificial agents in their environment as unmanned vehicles, intelligent houses, and humanoid robots capable of caring for people. In this context, research in the field of machine ethics has become more than a hot topic. Machine ethics focuses on developing ethical mechanisms for artificial agents to be capable of engaging in moral behavior. However, there are still crucial challenges in the development of truly Artificial Moral Agents. This paper aims to show the current status of Artificial Moral Agents by analyzing models proposed over the past two decades. As a result of this review, a taxonomy to classify Artificial Moral Agents according to the strategies and criteria used to deal with ethical problems is proposed. The presented review aims to illustrate (1) the complexity of designing and developing ethical mechanisms for this type of agent, and (2) that there is a long way to go (from a technological perspective) before this type of artificial agent can replace human judgment in difficult, surprising or ambiguous moral situations. Â© 2019, Springer Nature B.V.},
author_keywords={Artificial agent;  Ethical agent;  Machine ethics;  Moral dilemma},
keywords={artificial intelligence;  decision making;  human;  morality;  questionnaire;  software, Artificial Intelligence;  Humans;  Judgment;  Morals;  Software;  Surveys; Questionnaires},
references={Abbass, H.A., Petraki, E., Merrick, K., Harvey, J., Barlow, M., Trusted autonomy and cognitive cyber symbiosis: Open challenges (2016) Cognitive Computation, 8 (3), pp. 385-408; Alaieri, F., Vellino, A., Ethical decision making in robots: Autonomy, trust and responsibility (2016) International Conference on Social Robotics, pp. 159-168. , Cham, Springer; Allen, C., Smit, I., Wallach, W., Artificial morality: Top-down, bottom-up, and hybrid approaches (2005) Ethics and Information Technology, 7 (3), pp. 149-155; Amstutz, M.R., (2013) International ethics: Concepts, theories, and cases in global politics, , Rowman & Littlefield Publishers, New York; Anderson, M., Anderson, S.L., Machine ethics: Creating an ethical intelligent agent (2007) AI Magazine, 28 (4), pp. 15-26; Anderson, M., Anderson, S.L., The status of machine ethics: A report from the AAAI symposium (2007) Minds and Machines, 17 (1), pp. 1-10; Anderson, M., Anderson, S.L., Ethical healthcare agents (2008) Advanced computational intelligence paradigms in healthcare-3, pp. 233-257. , Sordo M, Vaidya S, Jain LC, (eds), Springer, Berlin; Anderson, M., Anderson, S.L., Robot be good (2010) Scientific American, 303 (4), pp. 72-77; Anderson, M., Anderson, S.L., Geneth: A general ethical dilemma analyzer (2014) Twenty-Eighth AAAI Conference on Artificial Intelligence, pp. 253-261; Anderson, M., Anderson, S.L., Armen, C., Towards machine ethics (2004) . in Proceedings of the AOTPâ€™04â€”The AAAI-04 Workshop on Agent Organizations: Theory and Practice.; Anderson, M., Anderson, S.L., Armen, C., Medethex: Toward a medical ethics advisor (2005) Proceedings of the AAAI 2005 Fall Symposium on Caring Machines: AI in Elder Care, pp. 9-16; Anderson, M., Anderson, S.L., Armen, C., An approach to computing ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 56-63; Anderson, M., Anderson, S.L., Armen, C., Medethex: A prototype medical ethics advisor (2006) Proceedings of the National Conference on Artificial Intelligence, 21 (2), pp. 1759-1765. , (b), Menlo Park, CA/Cambridge, MA, AAAI Press/MIT Press; Andino, C., Place of ethics between technical knowledge. A philosophical approach (2015) Revista CientÃ­fica de la UCSA, 2 (2), pp. 85-94; Arkin, R., (2009) Governing lethal behavior in autonomous robots, , Chapman and Hall/CRC, London; Arkin, R.C., The case for ethical autonomy in unmanned systems (2010) Journal of Military Ethics, 9 (4), pp. 332-341; Arkin, R., Lethal autonomous systems and the plight of the noncombatant (2018) The political economy of robots, pp. 317-326. , Kiggins R, (ed), Springer, Cham; Arkoudas, K., Bringsjord, S., Bello, P., Toward ethical robots via mechanized deontic logic (2005) AAAI Fall Symposium on Machine Ethics, pp. 17-23; Ashrafian, H., Artificial intelligence and robot responsibilities: Innovating beyond rights (2015) Science and Engineering Ethics, 21 (2), pp. 317-326; Bandyopadhyay, D., Sen, J., Internet of things: Applications and challenges in technology and standardization (2011) Wireless Personal Communications, 58 (1), pp. 49-69; Batty, M., Axhausen, K.W., Giannotti, F., Pozdnoukhov, A., Bazzani, A., Wachowicz, M., Ouzounis, G., Portugali, Y., Smart cities of the future (2012) The European Physical Journal Special Topics, 214 (1), pp. 481-518; Beauvisage, T., Computer usage in daily life (2009) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 575-584. , ACM; Bedaf, S., Draper, H., Gelderblom, G.J., Sorell, T., de Witte, L., Can a service robot which supports independent living of older people disobey a command? The views of older people, informal carers and professional caregivers on the acceptability of robots (2016) International Journal of Social Robotics, 8 (3), pp. 409-420; Belloni, A., Berger, A., Besson, V., Boissier, O., Bonnet, G., Bourgne, G., Towards a framework to deal with ethical conflicts in autonomous agents and multi-agent systems (2014) CEPE 2014 Well-Being, Flourishing, and Icts, pp. 1-10; Belloni, A., Berger, A., Boissier, O., Bonnet, G., Bourgne, G., Chardel, P.A., Dealing with ethical conflicts in autonomous agents and multi-agent systems (2015) 1St International Workshop on Artificial Intelligence and Ethics at the 29Th AAAI Conference on Artificial Intelligence; Blass, J.A., Interactive learning and analogical chaining for moral and commonsense reasoning (2016) Thirtieth AAAI Conference on Artificial Intelligence, pp. 4289-4290; Blass, J.A., Forbus, K.D., Moral decision-making by analogy: Generalizations versus exemplars (2015) Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 501-507; Bonnemains, V., Saurel, C., Tessier, C., Embedded ethics: Some technical and ethical challenges (2018) Ethics and Information Technology, 20 (1), pp. 41-58; Borenstein, J., Arkin, R., Robots, ethics, and intimacy: The need for scientific research (2019) On the cognitive, ethical, and scientific dimensions of artificial intelligence, pp. 299-309. , Berkich D, dAlfonso M, (eds), Springer, Cham; Borst, J.P., Anderson, J.R., Using the ACT-R cognitive architecture in combination with fMRI data (2015) An introduction to model-based cognitive neuroscience, pp. 339-352. , Forstmann B, Wagenmakers EJ, (eds), Springer, Berlin; Brachman, R.J., Systems that know what theyâ€™re doing (2002) IEEE Intelligent Systems, 17 (6), pp. 67-71; Briggs, G., Scheutz, M., (2015) Sorry, I canâ€™t do thatâ€: Developing mechanisms to appropriately reject directives in humanâ€“robot interactions; Bringsjord, S., Sundar, G.N., Thero, D., Si, M., Akratic robots and the computational logic thereof (2014) Proceedings of the IEEE 2014 International Symposium on Ethics in Engineering, Science, and Technology, pp. 1-8. , IEEE Press; Brundage, M., Limitations and risks of machine ethics (2014) Journal of Experimental & Theoretical Artificial Intelligence, 26 (3), pp. 355-372; Capraro, V., Rand, D.G., Do the right thing: Experimental evidence that preferences for moral behavior, rather than equity or efficiency per se, drive human prosociality (2018) Forthcoming in Judgment and Decision Making, 13 (1), pp. 99-111; Cervantes, J.A., RodrÃ­guez, L.F., LÃ³pez, S., Ramos, F., Robles, F., Autonomous agents and ethical decision-making (2016) Cognitive Computation, 8 (2), pp. 278-296; Cervantes, J.A., Rosales, J.H., LÃ³pez, S., Ramos, F., Ramos, M., Integrating a cognitive computational model of planning and decision-making considering affective information (2017) Cognitive Systems Research, 44, pp. 10-39; Choi, D., Langley, P., Evolution of the icarus cognitive architecture (2018) Cognitive Systems Research, 48, pp. 25-38; Coeckelbergh, M., Moral appearances: Emotions, robots, and human morality (2010) Ethics and Information Technology, 12 (3), pp. 235-241; Conway, P., Gawronski, B., Deontological and utilitarian inclinations in moral decision making: A process dissociation approach (2013) Journal of Personality and Social Psychology, 104 (2), pp. 216-235; Cook, D.J., Das, S.K., Pervasive computing at scale: Transforming the state of the art (2012) Pervasive and Mobile Computing, 8 (1), pp. 22-35; Cristani, M., Burato, E., Approximate solutions of moral dilemmas in multiple agent system (2009) Knowledge and Information Systems, 18 (2), pp. 157-181; Czubenko, M., Kowalczuk, Z., Ordys, A., Autonomous driver based on an intelligent system of decision-making (2015) Cognitive Computation, 7 (5), pp. 569-581; Dehghani, M., Tomai, E., Forbus, K.D., Klenk, M., An integrated reasoning approach to moral decision-making (2008) Twenty-Third AAAI Conference on Artificial Intelligence, pp. 1280-1286; Deng, B., Machine ethics: The robotâ€™s dilemma (2015) Nature, 523 (7558), pp. 24-26; Dennis, L.A., Fisher, M., Lincoln, N.K., Lisitsa, A., Veres, S.M., Practical verification of decision-making in agent-based autonomous systems (2016) Automated Software Engineering, 23 (3), pp. 305-359; Dennis, L., Fisher, M., Slavkovik, M., Webster, M., Formal verification of ethical choices in autonomous systems (2016) Robotics and Autonomous Systems, 77, pp. 1-14; Epting, S., A different trolley problem: The limits of environmental justice and the promise of complex moral assessments for transportation infrastructure (2016) Science and Engineering Ethics, 22 (6), pp. 1781-1795; Erdur, M., Moral realism and the incompletability of morality (2018) The Journal of Value Inquiry, 52 (2), pp. 227-237; Fagin, R., Halpern, J.Y., Vardi, M.Y., A nonstandard approach to the logical omniscience problem (1990) Proceedings of the 3Rd Conference on Theoretical Aspects of Reasoning about Knowledge, pp. 41-55. , Morgan Kaufmann Publishers Inc; Feil-Seifer, D., MatariÄ‡, M.J., Socially assistive robotics (2011) IEEE Robotics and Automation Magazine, 18 (1), pp. 24-31; Ferrell, O.C., Gresham, L.G., A contingency framework for understanding ethical decision making in marketing (1985) The Journal of Marketing, 49 (3), pp. 87-96; Fleetwood, J., Vaught, W., Feldman, D., Gracely, E., Kassutto, Z., Novack, D., Medethex online: A computer-based learning program in medical ethics and communication skills (2000) Teaching and Learning in Medicine, 12 (2), pp. 96-104; Fumagalli, M., Priori, A., Functional and clinical neuroanatomy of morality (2012) Brain, 135 (7), pp. 2006-2021; Gerdes, A., Ã˜hrstrÃ¸m, P., Issues in robot ethics seen through the lens of a moral turing test (2015) Journal of Information, Communication and Ethics in Society, 13 (2), pp. 98-109; Gogoll, J., MÃ¼ller, J.F., Autonomous cars: In favor of a mandatory ethics setting (2017) Science and Engineering Ethics, 23 (3), pp. 681-700; Govindarajulu, N.S., Bringjsord, S., Ghosh, R., (2018) One formalization of virtue ethics via learning; Greene, J.D., Morelli, S.A., Lowenberg, K., Nystrom, L.E., Cohen, J.D., Cognitive load selectively interferes with utilitarian moral judgment (2008) Cognition, 107 (3), pp. 1144-1154; Greene, J., Rossi, F., Tasioulas, J., Venable, K.B., Williams, B.C., Embedding ethical principles in collective decision support systems (2016) Thirtieth AAAI Conference on Artificial Intelligence, pp. 4147-4151; Greene, J.D., Sommerville, R.B., Nystrom, L.E., Darley, J.M., Cohen, J.D., An fMRI investigation of emotional engagement in moral judgment (2001) Science, 293 (5537), pp. 2105-2108; Guerini, M., Pianesi, F., Stock, O., Is it morally acceptable for a system to lie to persuade me? (2015) Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 53-60; Han, T.A., Pereira, L.M., Evolutionary machine ethics (2018) Handbuch Maschinenethik, pp. 1-25. , Bendel O, (ed), Springer, Wiesbaden; Hassabis, D., Kumaran, D., Summerfield, C., Botvinick, M., Neuroscience-inspired artificial intelligence (2017) Neuron, 95 (2), pp. 245-258; Honarvar, A.R., Ghasem-Aghaee, N., Casuist BDI-agent: A new extended BDI architecture with the capability of ethical reasoning (2009) International Conference on Artificial Intelligence and Computational Intelligence, pp. 86-95. , Berlin, Springer; Howard, D., Muntean, I., A minimalist model of the artificial autonomous moral agent (AAMA) (2016) The 2016 AAAI Spring Symposium Series, pp. 217-225; Hughes, G.J., (2001) Routledge philosophy guidebook to Aristotle on ethics, , Routledge, London; Kahn, Kanda, T., Jr., Ishiguro, H., Gill, B.T., Ruckert, J.H., Shen, S., Do people hold a humanoid robot morally accountable for the harm it causes? (2012) Proceedings of the Seventh Annual ACM/IEEE International Conference on Humanâ€“Robot Interaction, pp. 33-40. , ACM; Kirchin, S., (2012) What is Metaethics? In Metaethics, pp. 1-20. , London, Palgrave Macmillan; Kishi, T., Hashimoto, K., Takanishi, A., Human like face and head mechanism (2017) Humanoid robotics: A reference, pp. 1-26. , Goswami A, Vadakkepat P, (eds), Springer, Dordrecht; Kruglanski, A.W., Gigerenzer, G., Intuitive and deliberate judgments are based on common principles (2011) Psychological Review, 118 (1), pp. 97-109; Laird, J.E., Extending the soar cognitive architecture (2008) Frontiers in Artificial Intelligence and Applications, 171, pp. 224-235; Laird, J.E., Kinkade, K.R., Mohan, S., Xu, J.Z., Cognitive robotics using the soar cognitive architecture (2012) Workshops at the Twenty-Sixth AAAI Conference on Artificial Intelligence, pp. 46-54; Laird, J.E., Lebiere, C., Rosenbloom, P.S., A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics (2017) AI Magazine, 38 (4), pp. 13-26; Lombrozo, T., The role of moral commitments in moral judgment (2009) Cognitive Science, 33 (2), pp. 273-286; Long, L.N., Kelley, T.D., Review of consciousness and the possibility of conscious robots (2010) Journal of Aerospace Computing, Information, and Communication, 7 (2), pp. 68-84; Madl, T., Franklin, S., Constrained incrementalist moral decision making for a biologically inspired cognitive architecture (2015) A construction manual for robotsâ€™ ethical systems, pp. 137-153. , Trappl R, (ed), Springer, Cham; Malle, B.F., Integrating robot ethics and machine morality: The study and design of moral competence in robots (2016) Ethics and Information Technology, 18 (4), pp. 243-256; Malle, B.F., Scheutz, M., Arnold, T., Voiklis, J., Cusimano, C., Sacrifice one for the good of many? People apply different moral norms to human and robot agents (2015) Proceedings of the Tenth Annual ACM/IEEE International Conference on humanâ€“robot Interaction, pp. 117-124. , ACM; Mermet, B., Simon, G., Formal verification of ethical properties in multiagent systems (2016) In ECAI 2016 Workshop on Ethics in the Design of Intelligent Agents (EDIAâ€™16), , The Netherlands, The Hague; Metta, G., Natale, L., Nori, F., Sandini, G., Vernon, D., Fadiga, L., Von Hofsten, C., Santos-Victor, J., The iCub humanoid robot: An open-systems platform for research in cognitive development (2010) Neural Networks, 23 (8), pp. 1125-1134; Mikhail, J., Universal moral grammar: Theory, evidence and the future (2007) Trends in Cognitive Sciences, 11 (4), pp. 143-152; Moor, J.H., The nature, importance, and difficulty of machine ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 18-21; Mordoch, E., Osterreicher, A., Guse, L., Roger, K., Thompson, G., Use of social commitment robots in the care of elderly people with dementia: A literature review (2013) Maturitas, 74 (1), pp. 14-20; Mostafa, S.A., Ahmad, M.S., Mustapha, A., Adjustable autonomy: A systematic literature review (2019) Artificial Intelligence Review, 51 (2), pp. 149-186; Mostafa, S.A., Mustapha, A., Mohammed, M.A., Ahmad, M.S., Mahmoud, M.A., A fuzzy logic control in adjustable autonomy of a multi-agent system for an automated elderly movement monitoring application (2018) International Journal of Medical Informatics, 112, pp. 173-184; Pellizzoni, S., Siegal, M., Surian, L., The contact principle and utilitarian moral judgments in young children (2010) Developmental Science, 13 (2), pp. 265-270; Podschwadek, F., Do androids dream of normative endorsement? On the fallibility of artificial moral agents (2017) Artificial Intelligence and Law, 25 (3), pp. 325-339; Reig, S., Norman, S., Morales, C.G., Das, S., Steinfeld, A., Forlizzi, J., A field study of pedestrians and autonomous vehicles (2018) Proceedings of the 10Th International Conference on Automotive User Interfaces and Interactive Vehicular Applications, pp. 198-209. , ACM; RodrÃ­guez, L.F., Ramos, F., Development of computational models of emotions for autonomous agents: A review (2014) Cognitive Computation, 6 (3), pp. 351-375; Schaich Borg, J., Hynes, C., Van Horn, J., Grafton, S., Sinnott-Armstrong, W., Consequences, action, and intention as factors in moral judgments: An fMRI investigation (2006) Journal of Cognitive Neuroscience, 18 (5), pp. 803-817; Scheutz, M., Malle, B.F., Think and do the right thing: A plea for morally competent autonomous robots (2014) Proceedings of the IEEE 2014 International Symposium on Ethics in Engineering, Science, and Technology, p. 9. , IEEE Press; Schroeder, M., Normative ethics and metaethics (2017) The Routledge handbook of metaethics, pp. 674-686. , McPherson T, Plunkett D, (eds), Routledge, London; Sharkey, A., Sharkey, N., Granny and the robots: Ethical issues in robot care for the elderly (2012) Ethics and Information Technology, 14 (1), pp. 27-40; Shigemi, S., ASIMO and humanoid robot research at Honda (2018) Humanoid Robotics: A Reference (Pp. 1â€“36), , A. Goswami, P. Vadakkepat, Springer; Tikhanoff, V., Cangelosi, A., Metta, G., Integration of speech and action in humanoid robots: iCub simulation experiments (2011) IEEE Transactions on Autonomous Mental Development, 3 (1), pp. 17-29; Trafton, G., Hiatt, L., Harrison, A., Tamborello, F., Khemlani, S., Schultz, A., ACT-R/E: An embodied cognitive architecture for humanâ€“robot interaction (2013) Journal of Humanâ€“Robot Interaction, 2 (1), pp. 30-55; van Riemsdijk, M.B., Jonker, Lesser, C.M.V., Creating socially adaptive electronic partners: Interaction, reasoning and ethical challenges (2015) Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pp. 1201-1206. , International Foundation for Autonomous Agents and Multiagent Systems; Van Staveren, I., Beyond utilitarianism and deontology: Ethics in economics (2007) Review of Political Economy, 19 (1), pp. 21-35; Van Wynsberghe, A., Robbins, S., Critiquing the reasons for making artificial moral agents (2018) Science and Engineering Ethics, 25 (3), pp. 1-17; Vanderelst, D., Winfield, A., An architecture for ethical robots inspired by the simulation theory of cognition (2018) Cognitive Systems Research, 48, pp. 56-66; Vernon, D., Metta, G., Sandini, G., A survey of artificial cognitive systems: Implications for the autonomous development of mental capabilities in computational agents (2007) IEEE Transactions on Evolutionary Computation, 11 (2), pp. 151-180; Viroli, M., Pianini, D., Montagna, S., Stevenson, G., Pervasive ecosystems: A coordination model based on semantic chemistry (2012) Proceedings of the 27Th Annual ACM Symposium on Applied Computing, pp. 295-302. , ACM; Von der Pfordten, D., Five elements of normative ethicsâ€”A general theory of normative individualism (2012) Ethical Theory and Moral Practice, 15 (4), pp. 449-471; Von Wright, G.H., Deontic logic (1951) Mind, 60 (237), pp. 1-15; Waldrop, M.M., Autonomous vehicles: No drivers required (2015) Nature News, 518 (7537), p. 20; Walker, L.J., Hennig, K.H., Differing conceptions of moral exemplarity: Just, brave, and caring (2004) Journal of Personality and Social Psychology, 86 (4), pp. 629-647; Wallach, W., Implementing moral decision making faculties in computers and robots (2008) AI & Society, 22 (4), pp. 463-475; Wallach, W., Robot minds and human ethics: The need for a comprehensive model of moral decision making (2010) Ethics and Information Technology, 12 (3), pp. 243-250; Wallach, W., Allen, C., Smit, I., Machine morality: Bottom-up and top-down approaches for modelling human moral faculties (2008) AI & Society, 22 (4), pp. 565-582; Wallach, W., Franklin, S., Allen, C., A conceptual and computational model of moral decision making in human and artificial agents (2010) Topics in Cognitive Science, 2 (3), pp. 454-485; Wang, S., Wan, J., Zhang, D., Li, D., Zhang, C., Towards smart factory for industry 4.0: A self-organized multi-agent system with big data based feedback and coordination (2016) Computer Networks, 101, pp. 158-168; Wellman, M.P., Rajan, U., Ethical issues for autonomous trading agents (2017) Minds and Machines, 27 (4), pp. 609-624; Winfield, A.F., Blum, C., Liu, W., Towards an ethical robot: Internal models, consequences and ethical action selection (2014) Conference Towards Autonomous Robotic Systems, pp. 85-96. , Cham, Springer; Yampolskiy, R.V., Artificial intelligence safety engineering: Why machine ethics is a wrong approach (2013) Philosophy and theory of artificial intelligence, pp. 389-396. , MÃ¼ller V, (ed), Springer, Berlin; Young, L., Durwin, A., Moral realism as moral motivation: The impact of meta-ethics on everyday decision-making (2013) Journal of Experimental Social Psychology, 49 (2), pp. 302-306; Zambonelli, F., Viroli, M., A survey on nature-inspired metaphors for pervasive service ecosystems (2011) International Journal of Pervasive Computing and Communications, 7 (3), pp. 186-204; Zieba, S., Polet, P., Vanderhaegen, F., Debernard, S., Principles of adjustable autonomy: A framework for resilient humanâ€“machine cooperation (2010) Cognition, Technology & Work, 12 (3), pp. 193-203},
correspondence_address1={Cervantes, J.-A.; Department of Computer Science and Engineering, Carretera Guadalajara â€“ Ameca Km. 45.5, Mexico; email: antoniocervantes@valles.udg.mx},
editor={NA},
publisher={Springer},
issn={13533452},
isbn={NA},
language={English},
abbrev_source_title={Sci. Eng. Ethics},
document_type={Review},
source={Scopus},
number={2},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={31721023},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Nath2020103,
type={ARTICLE},
author={Nath, R. and Sahu, V.},
title={The problem of machine ethics in artificial intelligence},
journal={AI and Society},
year={2020},
volume={35},
pages={103-111},
doi={10.1007/s00146-017-0768-6},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031784550&doi=10.1007%2fs00146-017-0768-6&partnerID=40&md5=0b561c3a2ad7e511772407fbb8bf5073},
affiliation={Department of Humanities and Social Sciences, Indian Institute of Technology Bombay, Mumbai, India; Department of Humanities and Social Sciences, IIT Kanpur, Kanpur, India},
abstract={The advent of the intelligent robot has occupied a significant position in society over the past decades and has given rise to new issues in society. As we know, the primary aim of artificial intelligence or robotic research is not only to develop advanced programs to solve our problems but also to reproduce mental qualities in machines. The critical claim of artificial intelligence (AI) advocates is that there is no distinction between mind and machines and thus they argue that there are possibilities for machine ethics, just as human ethics. Unlike computer ethics, which has traditionally focused on ethical issues surrounding human use of machines, AI or machine ethics is concerned with the behaviour of machines towards human users and perhaps other machines as well, and the ethicality of these interactions. The ultimate goal of machine ethics, according to the AI scientists, is to create a machine that itself follows an ideal ethical principle or a set of principles; that is to say, it is guided by this principle or these principles in decisions it makes about possible courses of action it could takea. Thus, machine ethics task of ensuring ethical behaviour of an artificial agent. Although, there are many philosophical issues related to artificial intelligence, but our attempt in this paper is to discuss, first, whether ethics is the sort of thing that can be computed. Second, if we are ascribing mind to machines, it gives rise to ethical issues regarding machines. And if we are not drawing the difference between mind and machines, we are not only redefining specifically human mind but also the society as a whole. Having a mind is, among other things, having the capacity to make voluntary decisions and actions. The notion of mind is central to our ethical thinking, and this is because the human mind is self-conscious, and this is a property that machines lack, as yet. Â© 2017, Springer-Verlag London Ltd.},
author_keywords={Artificial intelligence;  Artificial moral agent;  Mind;  Moral agency;  Subjectivity},
keywords={Artificial intelligence;  Human computer interaction;  Intelligent robots, Artificial agents;  Computer ethics;  Ethical issues;  Ethical principles;  Mind;  Moral agency;  Moral agents;  Subjectivity, Philosophical aspects},
references={Allen, C., Varner, G., Zinser, J., Prolegomena to any future artificial moral agent (2000) J Exp Theor Artif Intell, 12 (3), pp. 251-261; Anderson, M., Anderson, S.L., Machine ethics: creating an ethical intelligent agent (2007) AI Mag, 28 (4), pp. 15-26; Beavers, A.F., Between angels and animals: The question of robot ethics, or is Kantian moral agency desirable (2009) Association for Practical and Professional Ethics. Eighteenth Annual Meeting, , http://faculty.evansville.edu/tb2/PDFs/Robot%20Ethics%20-%20APPE.pdf, Cincinnati, Ohio, March 5â€“8, Retrieved on 1 Oct 2017; Chalmers, D.J., (1996) The Conscious Mind, , Oxford and New York: Oxford; DeBaets, A.M., Can a robot pursue the good? Exploring artificial moral agency (2014) J Evolut Technol, 24 (3), pp. 76-86; Gunkel, D.J., (2012) The machine question: critical perspectives on AI, robots, and ethics, , MIT Press, Cambridge; Haugeland, J., (1989) Artificial intelligence: the very idea, , The MIT Press, Cambridge; Kant, I., (1993) Grounding for the metaphysics of morals, , [1785],. Translated by Ellington, James W (3rd Edn), Hackett; LaChat, M.R., Artificial intelligence and ethics: an exercise in the moral imagination (1986) AI Magz, 7 (2), pp. 70-79; Lycan, W.G., (1987) Consciousness, , The MIT Press, Massachusetts; McCorduck, P., (1979) Machines who thinks, , W.H. Freeman and Company, San Francisco; McGinn, C., Can we solve the mindâ€“body problem? (1997) The Nature of Consciousness, , Ned B, Owen F, GÃ¼ven G, The MIT Press, Cambridge; Moor, J.H., The nature, importance and difficulty of machine ethics (2006) IEEE Intell Syst, 21 (4), pp. 18-21; Nagel, T., What it is like to be a bat (1998) The Nature of Consciousness, , Ned B, Owen F, GÃ¼ven G, The MIT Press, Cambridge, MA; Nath, R., (2009) Philosophy of artificial intelligence. A critique of the mechanistic theory of mind, , The Universal Publishers, Boca Raton; Nath, R., Can naturalism explain consciousness: a critique (2016) Artif Intell Soc J Knowl Cult Commun; Searle, J.R., Minds and brains without programs (1987) Mindwaves: Thoughts on Intelligence, Identity, and Consciousness, , Blakemore C, Greenfield S, Bail Blackwell, Oxford; Searle, J.R., Is the brain a digital computer? (1990) Proc Address Am Philos Assoc, 64 (3), pp. 21-37; Searle, J.R., (1994) The rediscovery of the mind, , Harvard University Press, Cambridge; Searle, J.R., (1996) Minds, brains and science, , Harvard University Press, Cambridge; Simon, H.A., Guest foreword (1987) Encyclopedia of Artificial Intelligence, 1. , Stuart CS, Wiley, New York; Tanimoto, S.L., (1987) The elements of artificial intelligence, , Computer Science Press Inc, Maryland; Turing, A.M., Computing machinery and intelligence (1950) Mind, 49, pp. 433-460; Winston, P.H., (1984) Artificial intelligence, , Addison-Wesley Publishing Company, London; Wittgenstein, L., (1976) Philosophical investigations, , Anscombe GEM (translated), Basil Blackwell, Oxford},
correspondence_address1={Nath, R.; Department of Humanities and Social Sciences, India; email: rajakishorenath@iitb.ac.in},
editor={NA},
publisher={Springer},
issn={09515666},
isbn={NA},
language={English},
abbrev_source_title={AI Soc.},
document_type={Article},
source={Scopus},
number={1},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Cunneen202059,
type={ARTICLE},
author={Cunneen, M. and Mullins, M. and Murphy, F. and Shannon, D. and Furxhi, I. and Ryan, C.},
title={Autonomous Vehicles and Avoiding the Trolley (Dilemma): Vehicle Perception, Classification, and the Challenges of Framing Decision Ethics},
journal={Cybernetics and Systems},
year={2020},
volume={51},
pages={59-80},
doi={10.1080/01969722.2019.1660541},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073835364&doi=10.1080%2f01969722.2019.1660541&partnerID=40&md5=f1f11ab1874fe81737e1d7d9e0369957},
affiliation={Department of Accounting and Finance, University of Limerick, Limerick, Ireland},
abstract={This article aims to introduce a degree of technological and ethical realism to the framing of autonomous vehicle perception and decisionality. The objective is to move the socioethical dialog surrounding autonomous vehicle decisionality from the dominance of â€œtrolley framingsâ€ to more pressing ethical issues. The article argues that more realistic ethical framings of autonomous vehicle technologies should focus on the matters of HMI, machine perception, classification, and data privacy, which are some distance from the decisionality framing premise of the MIT Moral Machine experiment. To support this claim the article appeals to state-of-the-art technologies and emerging technologies concerning autonomous vehicle perception and decisionality, as a means to inform and frame ethical contexts. This is further supported by considering a context specific ethical framing for each time phase we anticipate regarding emerging autonomous vehicle technology. Â© 2019, Â© 2019 The Author(s). Published with license by Taylor & Francis Group, LLC.},
author_keywords={AI;  artificial intelligence;  Autonomous vehicles;  classification;  computer vision;  machine perception;  trolley dilemma;  trolley problem},
keywords={Artificial intelligence;  Classification (of information);  Computer vision;  Data privacy;  Philosophical aspects, Autonomous vehicle technologies;  Emerging technologies;  Ethical issues;  Machine perception;  State-of-the-art technology;  trolley dilemma;  trolley problem, Autonomous vehicles},
references={Ali, H.H., Moftah, H.M., Youssif, A.A.A., Depth-based human activity recognition: A comparative perspective study on feature extraction' (2018) Future Computing and Informatics Journal, 3 (1), pp. 51-67; Anderson, M., Anderson, S.L., Machine ethics: Creating an ethical intelligent agent (2007) AI Magazine, 28 (4), p. 15; Atallah, R.R., Kamsin, A., Ismail, M.A., Abdelrahman, S.A., Zerdoumi, S., Face recognition and age estimation implications of changes in facial features: A critical review study (2018) IEEE Access, 6, pp. 28290-28304; Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., Bonnefon, J.-F., Rahwan, I., The Moral machine experiment (2018) Nature, 563 (7729), pp. 59-64; Bagloee, S.A., Tavana, M., Asadi, M., Oliver, T., Autonomous vehicles: Challenges, opportunities, and future implications for transportation policies (2016) Journal of Modern Transportation, 24, pp. 284-303. , 4; Banerjee, S.S., Jha, S., Cyriac, J., Kalbarczyk, Z.T., Iyer, R.K., Hands off the wheel in autonomous vehicles? A systems perspective on over a million miles of field data (2018) IEEE, pp. 586-97,. , In; Bento, L.C., Parafita, R., Rakha, H.A., Nunes, U.J., A study of the environmental impacts of intelligent automated vehicle control at intersections via V2V and V2I communications' (2019) Journal of Intelligent Transportation Systems, pp. 1-19; Brown, A., Rodriguez, G., Best, B., Hoang, K.T., Safford, H., Anderson, G., Dâ€™Agostino, M.C., (2018) Federal, state, and local governance of automated vehicles.; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., The cityscapes dataset for semantic urban scene understanding (2016) In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223; Cunneen, M., Mullins, M., Murphy, F., Gaines, S., Artificial driving intelligence and moral agency: Examining the decision ontology of unavoidable road traffic accidents through the prism of the trolley dilemma (2019) Applied Artificial Intelligence, 33 (3), pp. 267-293; Cunneen, M., Mullins, M., Murphy, F., Gaines, S., Autonomous vehicles and embedded artificial intelligence: The challenges of framing machine driving decisions (2019) Applied Artificial Intelligence, 33 (8), pp. 706-731; During, M., Lemmer, K., Cooperative maneuver planning for cooperative driving (2016) IEEE Intelligent Transportation Systems Magazine, 8 (3), pp. 8-22; Dutta, S., An overview on the evolution and adoption of deep learning applications used in the industry (2018) Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8 (4), p. e1257; Floridi, L., Rodella, F., (2019) Artificial intelligence will not replace us behind the wheel of a car. OpenMind, , https://www.bbvaopenmind.com/en/technology/artifficial-intelligence/artificial-intelligence-will-not-replace-us-behind-the-wheel-of-car/; Frank, M.R., Wang, D., Cebrian, M., Rahwan, I., The evolution of citation graphs in artificial intelligence research (2019) Nature Machine Intelligence, 1 (2), p. 79; Fridman, L., Brown, D.E., Glazer, M., Angell, W., Dodd, S., Jenik, B., Terwilliger, J., Seaman, S., Mit autonomous vehicle technology study: Large-scale deep learning based analysis of driver behavior and interaction with automation (2017) arXiv Preprint; Geiger, A., Lenz, P., Stiller, C., Urtasun, R., Vision meets robotics: The KITTI dataset (2013) The International Journal of Robotics Research, 32 (11), pp. 1231-1237; Girshick, R., (2015), pp. 1440-1448. , Fast r-Cnn. Proceedings of the IEEE International Conference on Computer Vision; Golias, M., Dedes, G., Douligeris, C., Mishra, S., Challenges, risks and opportunities for connected vehicle services in smart cities and communities (2019) IFAC-PapersOnLine, 51 (34), pp. 139-144; Gu, Y., Hashimoto, Y., Hsu, L.-T., Kamijo, S., Motion planning based on learning models of pedestrian and driver behaviors. In 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC (2016) IEEE, pp. 808-13,; Habibi, G., Jaipuria, N., How, J.P., Context-aware pedestrian motion prediction in urban intersections (2018) arXiv Preprint; Haruno, M., Wolpert, D.M., Kawato, M., Mosaic model for sensorimotor learning and control (2001) Neural Computation, 13 (10), pp. 2201-2220; Hoffmann, H., Payton, D.W., (2018) Adaptive control system capable of recovering from unexpected situations; Hu, Y., Zhan, W., Tomizuka, M., Probabilistic prediction of vehicle semantic intention and motion. In 2018 IEEE Intelligent Vehicles Symposium (IV (2018) IEEE, pp. 307-13,; Johnson, C., (2017) Readiness of the road network for connected and autonomous vehicles, , London, UK: RAC Foundation; Karnouskos, S., Kerschbaum, F., Privacy and integrity considerations in hyperconnected autonomous vehicles' (2018) Proceedings of the IEEE, 106 (1), pp. 160-170; Kim, S.-W., Liu, W., Cooperative autonomous driving: A mirror neuron inspired intention awareness and cooperative perception approach (2016) IEEE Intelligent Transportation Systems Magazine, 8 (3), pp. 23-32; Koopman, P., Wagner, M., Challenges in autonomous vehicle testing and validation (2016) SAE International Journal of Transportation Safety, 4 (1), pp. 15-24; Koopman, P., Wagner, M., Autonomous vehicle safety: An interdisciplinary challenge (2017) IEEE Intelligent Transportation Systems Magazine, 9 (1), pp. 90-96; (2018) Autonomous Vehicles Readiness Index: Assessing countries openness and preparedness for autonomous vehicles, , https://assets.kpmg.com/content/dam/kpmg/nl/pdf/2018/sector/automotive/autonomous-vehicles-readiness-index.pdf; LefÃ¨vre, S., Vasquez, D., Laugier, C., A survey on motion prediction and risk assessment for intelligent vehicles' (2014) ROBOMECH Journal, 1 (1), p. 1; Liu, Z., Luo, P., Qiu, S., Wang, X., Tang, X., DeepFashion: Powering robust clothes recognition and retrieval with rich annotations (2016) 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1096-104,. , In June 27â€“30; Maddox, J.P., Sweatmansayer, J., Intelligent vehicles + infrastructure to address transportation problemsâ€“a strategic approach (2015) 24Th International Technical Conference on the Enhanced Safety of Vehicles (ESV), , Gothenburg, Sweden; Maturana, D., Scherer, S., Voxnet: A 3d convolutional neural network for real-time object recognition. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS (2015) IEEE, pp. 922-8,; McDuff, D.C., Identifying bias in AI using simulation (2018) arXiv Preprint; Melotti, G., Asvadi, A., Premebida, C., CNN-LIDAR pedestrian classification: Combining range and reflectance data. In 2018 IEEE International Conference on Vehicular Electronics and Safety (ICVES (2018) IEEE, pp. 1-6,; Metz, D., Developing policy for urban autonomous vehicles: Impact on congestion (2018) Urban Science, 2 (2), p. 33; Miikkulainen, R., Liang, J., Meyerson, E., Rawal, A., Fink, D., Francon, O., Raju, B., Duffy, N., Evolving deep neural networks (2019) Artificial Intelligence in the Age of Neural Networks and Brain Computing, pp. 293-312. , Amsterdam, The Netherlands: Elsevier, and; (2019) Future-of-mobility, , https://www.mobileye.com/future-of-mobility/, Retrieved from; Naik, B., Shinde, S.B., Thite, S.R., A review on recognition of cloth patterns for visually impaired people (2017) International Journal of Advanced Research in Electrical, Electronics and Instrumentation Engineering, 6 (12); Pendleton, S., Andersen, H., Du, X., Shen, X., Meghjani, M., Eng, Y., Rus, D., Ang, M., Perception, planning, control, and coordination for autonomous vehicles (2017) Machines, 5 (1), p. 6; Ren, S., He, K., Girshick, R., Sun, J., Faster r-cnn: Towards real-time object detection with region proposal networks (2015) Advances in Neural Information Processing Systems, pp. 91-99; Rosique, F., Navarro, P.J., FernÃ¡ndez, C., Padilla, A., A systematic review of perception system and simulators for autonomous vehicles research (2019) Sensors, 19 (3), p. 648; Schellekens, M., Self-driving cars and the chilling effect of liability law (2015) Computer Law & Security Review, 34 (4), p. 506; Schwarz, B., LIDAR: Mapping the world in 3D (2010) Nature Photonics, 4 (7), p. 429; Sivaraman, S., Trivedi, M.M., Looking at vehicles on the road: a survey of visionbased vehicle detection, tracking, and behavior analysis (2013) IEEE Transactions on Intelligent Transportation Systems, 14, pp. 1773-1795. , 4; Sooklal, S., Hosein, P., Teelucksingh, S., A review of human body shape detection techniques and their application to the prediction of health risks (2016) In International Conference e-Health 2016, Madeira, Portugal, July; Slovic, P., Fischhoff, B., Lichtenstein, S., Characterizing perceived risk (1985) Perilous Progress: Managing the Hazards of Technology, pp. 91-125; Talebian, A., Mishra, S., Golias, M., Khan, J.A., Santo, C.A., Wang, L., Jacobs, E.L., Simpson, J., (2019) A holistic index for readiness to accommodate connected autonomous vehicles (no. 19-02097); Tian, Y., Pei, K., Jana, S., Ray, B., Deeptest: Automated testing of deep-neural-network-driven autonomous cars (2018) Proceedings of the 40th International Conference on Software Engineering, pp. 303-314. , ACM, and; Van Brummelen, J., Oâ€™Brien, M., Gruyer, D., Najjaran, H., Autonomous vehicle perception: The technology of today and tomorrow (2018) Transportation Research Part C: Emerging Technologies.; Yoshioka, M., Suganuma, N., Yoneda, K., Aldibaja, M., Real-time object classification for autonomous vehicle using LIDAR (2017) 2017 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS), pp. 210-1,. , IEEE, and,. â€“1; Zhang, S., Wei, Z., Nie, J., Huang, L., Wang, S., Li, Z., A review on human activity recognition using vision-based method (2017) Journal of Healthcare Engineering, 2017, p. 1; (2017), 18. , Zhu, H., K. V. Yuen, L. Mihaylova, and Leung, H. (,). Overview of environment perception for intelligent vehicles., IEEE Transactions on Intelligent Transportation Systems, (10):2584â€“2601; (2015), 30. , Zuboff, S. (,). Big other: surveillance capitalism and the prospects of an information civilization., Journal of Information Technology, (1):75â€“89},
correspondence_address1={Cunneen, M.; Department of Accounting and Finance, Ireland; email: Martin.Cunneen@ul.ie},
editor={NA},
publisher={Taylor and Francis Inc.},
issn={01969722},
isbn={NA},
language={English},
abbrev_source_title={Cybern Syst},
document_type={Article},
source={Scopus},
number={1},
art_number={NA},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme,Â H2020,Â 690772 funding_textÂ 1={All authors are members of the Vision Inspired Driver Assistance Systems (VI-DAS) H2020 research consortia.},
coden={CYSYD},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Friedman20203,
type={ARTICLE},
author={Friedman, C.},
title={Human-Robot Moral Relations: Human Interactants as Moral Patients of Their Own Agential Moral Actions Towards Robots},
journal={Communications in Computer and Information Science},
year={2020},
volume={1342},
pages={3-20},
doi={10.1007/978-3-030-66151-9_1},
note={cited By 3; Conference of 1st Southern African Conference for Artificial Intelligence Research, SACAIR 2020 ; Conference Date: 22 February 2021 Through 26 February 2021;  Conference Code:253469},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098266870&doi=10.1007%2f978-3-030-66151-9_1&partnerID=40&md5=647da4907c21d6a0a93c56758db6264b},
affiliation={Department of Philosophy, University of Pretoria, Pretoria, South Africa; Centre for AI Research (CAIR), Pretoria, South Africa},
abstract={This paper contributes to the debate in the ethics of social robots on how or whether to treat social robots morally by way of considering a novel perspective on the moral relations between human interactants and social robots. This perspective is significant as it allows us to circumnavigate debates about the (im)possibility of robot consciousness and moral patiency (debates which often slow down discussion on the ethics of HRI), thus allowing us to address actual and urgent current ethical issues in relation to human-robot interaction. The paper considers the different ways in which human interactants may be moral patients in the context of interaction with social robots: robots as conduits of human moral action towards human moral patients; humans as moral patients to the actions of robots; and human interactants as moral patients of their own agential moral actions towards social robots. This third perspective is the focal point of the paper. The argument is that due to perceived robot consciousness, and the possibility that the immoral treatment of social robots may morally harm human interactants, there is a unique moral relation between humans and social robots wherein human interactants are both the moral agents of their actions towards robots, as well as the actual moral patients of those agential moral actions towards robots. Robots, however, are no more than perceived moral patients. This discussion further adds to debates in the context of robot moral status, and the consideration of the moral treatment of robots in the context of human-robot interaction. Â© 2020, Springer Nature Switzerland AG.},
author_keywords={Human-robot interaction;  Moral patiency;  Robot ethics},
keywords={Artificial intelligence;  Man machine systems;  Patient treatment;  Philosophical aspects, Ethical issues;  Focal points;  Human robots;  Moral agents, Social robots},
references={(2019), https://standards.ieee.org/content/dam/ieeestandards/standards/web/documents/other/ead1e.pdf; Anderson, M., Anderson, S., Machine ethics: Creating an ethical intelligent agent (2007) AI Mag, 28 (4), pp. 15-26; Arnold, T., Scheutz, M., HRI ethics and type-token ambiguity: What kind of robotic identity is most responsible? (2018) Ethics Inf. Technol.; Asaro, P., What should we want from a robot ethic? (2006) Int. Rev. Inf. Ethics, 6 (12), pp. 9-16; Barquin, R.C., (1992) Ten Commandments of Computer Ethics; BoltuÄ‡, P., (2017) Chuch-Turing Lovers, , Oxford University Press, Oxford; Bostrom, N., Yudkwosky, E., (2014) The Cambdridge Handook of Artificial Intelligence, pp. 316-334. , The ethics of artificial intelligence. In: Frankish, K., Ramsey, W. (eds.), pp., Cambridge University Press, Cambridge; Breazeal, C., (2002), Designing Sociable Robots. MIT Press, Cambridge; Breazeal, C., Towards sociable robots (2003) Robot. Auton. Syst., 42, pp. 167-175; Broadbent, E., Interactions with robots: The truths we reveal about ourselves (2017) Annu. Rev. Psychol., 68, pp. 627-652; Brundage, M., Limitations and risks of machine ethics (2014) J. Exp. Theor. Artif. Intell., 26 (3), pp. 355-372; Bryson, J., Robots should be slaves (2009) Close Engage. Artif. Companions: Key Soc. Psychol. Ethical Des. Issues; http://consc.net/papers/facing.pdf, Chalmers, D.: Facing up to the problem of consciousness (1995); Chalmers, D., (2002) Philosophy of Mind: Classical and Contemporary Readings, , Oxford University Press, Oxford; Coeckelbergh, M., (2010) Artificial Companions: Empathy and Vulnerability Mirroring in Human-Robot Relations. Stud. Ethics Law Technol. 4(; Coeckelbergh, M., Health care, capabilities, and AI assistive technologies (2010) Ethical Theory Moral Pract, 13, pp. 181-190; Damiano, L., Dumouchel, P., Anthropomorphism in human-robot co-evolution (2018) Front. Psychol., 9 (1-9); Danaher, J., (2017) The Symbolic-Consequences Argument in the Sex Robot Debate, , MIT Press, Cambridge; Danaher, J., The rise of the robots and the crisis of moral patiency (2019) AI & Soc, 34, pp. 129-136; Danaher, J., Earp, B., Sandberg, A., (2017) Should We Campaign against Sex Robots, , The MIT Press, Cambridge; Danaher, J., McArthur, N., (2017) Robot Sex: Social and Ethical Implications, , The MIT Press, Cambridge; Dautenhahn, K., The art of designing socially intelligent agents-science, fiction, and the human in the loop (1998) Appl. Artif. Intelli., 12, pp. 573-617; Dautenhahn, K., Socially intelligent robots: Dimensions of human-robot interaction (2007) Philos. Trans. Roy. Soc., 362, pp. 679-704; Deng, B., Machine ethics: The robotâ€™s dilemma (2015) Nat. News, 523, pp. 24-26; Floridi, L., Sanders, J., On the morality of artificial agents (2004) Mind Mach, 14, pp. 349-379; Fong, T., Nourbakhsh, I., Dautenhahn, K., A survey of socially inter-active robots (2003) â€™. Robots Auton. Syst., 42, pp. 143-166; Gunkel, D., Moral patiency (2012) The Machine Question: Critical Perspectives on AI. Robots, and Ethics, pp. 93-157. , pp., The MIT Press, Cambridge; Gunkel, D., The other question: Can and should robots have rights? (2017) Ethics Inf. Technol., 20, pp. 87-99; (2019) ICRC: Autonomy, Artificial Intelligence and Robotics: Technical Aspects of Human Control, , Geneva; Jaworska, A., Tannenbaum, J., The grounds of moral status (2018) Stanford Encyclopedia of Philosophy; Kanda, T., Freier, N., Severson, R., Gill, B., Robovie, youâ€™ll have to go into the closet now: Childrenâ€™s social and moral relationships with a humanoid robot (2012) Dev. Psychol., 48 (2), pp. 303-314; Kanda, T., Ishiguro, H., Imai, M., Ono, T., (2004) Development and Evaluation of Interactive Humaoid Robots, pp. 1839-1850; Kant, I., (1996) The Metaphysics of Morals, , Cambridge University Press, Cambridge; Kant, I., (1997) Lectures on Ethics, , Cambridge University Press, Cambridge; Kirk, R., Carruthers, P., Consciousness and concepts (1992) Proceedings of the Aristotelian Society, Supplementary, 66, pp. 23-59. , vol., pp; Komatsubara, T., Can a social robot help childrenâ€™s understanding of science in classrooms? (2014) In: Proceedings of the Second International Conference on Human-Agent Interaction, pp. 83-90. , pp; Levy, D., (2007) Love and Sex with Robots: The Evolution of Human-Robot Relationships, , Harper; Levy, D., The ethical treatment of artificially conscious robots (2009) Int. J. Soc. Robot., 1 (3), pp. 209-216; Lin, P., Abney, K., Bekey, G., (2012) Robot Ethics: The Ethical and Social Implications of Robotics, , The MIT Press, Cambridge; Lin, P., Abney, K., Bekey, G., (2012) Robotics, Ethical Theory, and Metaethics: A Guide for the Perplexed, , The MIT Press, Cambridge; Lumbreras, S., The limits of machine ethics (2017) Religions, 8 (100), pp. 2-10; Matthias, A., The responsibility gap: Ascribing responsibility for the actions of learning automata (2004) Ethics Inf. Technol., 6, pp. 175-183; McDermott, D., Why ethics is a high hurdle for AI (2008) North American Conference on Computers and Philosophy, , Bloomington, Indiana; Melson, G., Kahn, P., Beck, A., Friedman, B., Robotic pets in human lives: Implications for the human-animal bond and for human relationships with personified technologies (2009) J. Soc. Issues, 65 (3), pp. 545-567; Moon, Y., Nass, C., Machines and mindlessness: Social responses to computers (2000) J. Soc. Issues, 56, pp. 81-103; Moor, J., The nature, importance, and difficulty of machine ethics (2006) IEEE, 21 (4), pp. 18-21; MÃ¼ller, V.: Ethics of artificial intelligence and robotics (2020, edition); Nyholm, S., Frank, L., It loves me, it loves me not: Is it morally problematic to design sex robots that appear to love their owners? (2019) technÃ©: Research in Philosophy and Technology, , Issue December; Ramey, C., (2005) For The Sake of Others: The â€˜personalâ€™ Ethics of Human-Android Interaction, , Stresa, Italy; Sharkey, A., Sharkey, N., Granny and the robots: Ethical issues in robot care for the elderly (2010) Ethics Inf. Technol., 14 (1), pp. 27-40; Sharkey, A., Should we welcome robot teachers? (2016) Ethics Inf. Technol., 18, pp. 283-297; Sparrow, R., Robots, rape, and representation (2017) Int. J. Soc. Robot., 9 (3), pp. 465-477; Sparrow, R., Sparrow, L., In the hands of machines? The future of aged care (2006) Minds Mach, 16, pp. 141-161; Sullins, J., When is a robot a moral agent? (2006) Int. Rev. Inf. Ethics, 6 (12); Sullins, J., Robots, love and sex: The ethics of building a love machine (2012) IEEE Trans. Affect. Comput., 3 (4), pp. 398-409; Torrance, S., Artificial agents and the expanding ethical circle (2013) AI Soc, 28, pp. 399-414; Turkle, S., A nascent robotics culture: New complicities for companionship (2006) AAAI Technical Report Series; Turkle, S., Authenticity in the age of digital companions (2007) Interact. Stud., 8 (3), pp. 501-517; Turner, J., (2019) Why Robot Rights? In: Robot Rules: Regulating Artificial Intelligence, pp. 145-171. , pp., Palgrave Macmillan, Cham; Vallor, S., Carebots and caregivers: Sustaining the ethical ideal of care in the twenty-first century (2011) Philos. Technol., 24, pp. 251-268; Wallach, W., Allen, C., (2009) Moral Machines: Teachinf Robots Right from Wrong, , Oxford University Press, New York; Wang, W., Siau, K., Ethical and moral issues with AI: A case study on healthcare robots (2018) Twenty-Fourth Americas Conference on Information Systems, , New Orleans},
correspondence_address1={Friedman, C.; Department of Philosophy, South Africa; email: cindzfriedman@gmail.com},
editor={Gerber A.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18650929},
isbn={9783030661502},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
number={NA},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Misselhorn2020,
type={ARTICLE},
author={Misselhorn, C.},
title={Artificial systems with moral capacities? A research design and its implementation in a geriatric care system},
journal={Artificial Intelligence},
year={2020},
volume={278},
pages={NA},
doi={10.1016/j.artint.2019.103179},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074135264&doi=10.1016%2fj.artint.2019.103179&partnerID=40&md5=52fbac65d9cb3d3fa8b33c2135f1ff57},
affiliation={Georg-August-UniversitÃ¤t GÃ¶ttingen, Philosophisches Seminar, Humboldtallee 19 (Eingang A), GÃ¶ttingen, D-37073, Germany},
abstract={The development of increasingly intelligent and autonomous technologies will eventually lead to these systems having to face morally problematic situations. This gave rise to the development of artificial morality, an emerging field in artificial intelligence which explores whether and how artificial systems can be furnished with moral capacities. This will have a deep impact on our lives. Yet, the methodological foundations of artificial morality are still sketchy and often far off from possible applications. One important area of application of artificial systems with moral capacities is geriatric care. The goal of this article is to afford the methodological foundations for artificial morality, i.e., for implementing moral capacities in artificial systems in general, and to discuss them with respect to an assistive system in geriatric care which is capable of moral learning. Â© 2019 Elsevier B.V.},
author_keywords={Artificial moral agents;  Artificial morality;  Assistive systems in geriatric care;  Elder care robots;  Functional morality;  Machine ethics;  Moral capacities;  Moral implementation},
keywords={Artificial intelligence, Artificial morality;  Elder care;  Functional morality;  Geriatric cares;  Moral agents;  Moral capacities;  Moral implementation, Geriatrics},
references={Alston, W.P., Moral attitudes and moral judgments (1968) NoÃ»s, 2, pp. 1-23; Anderson, S., Anderson, M., Armen, C., MedEthEx: a prototype medical ethics advisor (2006) Proceedings of the Eighteenth Conference on Innovative Applications of Artificial Intelligence, , AAAI Boston, Massachusetts; Anderson, S., Anderson, M., ETHEL: toward a principled ethical elder care robot (2008) Proceedings of the AAAI Fall 2008 Symposium on AI in Eldercare: New Solutions to Old Problems, Arlington, Virginia; Anderson, S., Anderson, M., A prima facie duty approach to machine ethics and its application to elder care (2011) Proceedings of the AAAI Workshop on Human-Robot Interaction in Elder Care, San Francisco; Arkin, R., Governing Lethal Behavior: Embedding Ethics in a Hybrid Deliberative/Reactive Robot Architecture (2007), Technical Report GIT-GVU-07-11 College of Computing, Georgia Institute of Technology; Axelrod, R., The Evolution of Cooperation (1984), Basic Books New York; Bajo, J., GR-MAS: multi-agent system for geriatric residences (2008) ECAI 2008, Frontiers in Artificial Intelligence and Applications, 178, pp. 875-876; Beauchamp, T., Childress, J., Principles of Biomedical Ethics (1979), Oxford University Press Oxford, New York; Bermudez, J., Philosophy of Psychology: A Contemporary Introduction (2005), Routledge New York; Block, N., On a confusion about the function of consciousness (1995) Behavioral and Brain Sciences, 18, pp. 227-247; Bratman, M.E., Faces of Intention: Selected Essays on Intention and Agency (1999), Cambridge University Press Cambridge; Bratman, M.E., Structures of Agency. Essays (2007), Oxford University Press Oxford, New York; Breazeal, C., Scassellati, B., Robots that imitate humans (2002) Trends in Cognitive Sciences, 6, pp. 481-487; Chisholm, R., Human Freedom and the Self. The Lindley Lectures (1964) Free Will, pp. 26-37. , Department of Philosophy, University of Kansas, repr. G. Watson 2nd edition Oxford University Press Oxford 2003; Coeckelbergh, M., How I learned to love the robot (2012) The Capability Approach, Technology and Design, pp. 77-86. , I. Oosterlaken J. van den Hoven Springer Dordrecht; Dancy, J., Moral particularism (2013) The Stanford Encyclopedia of Philosophy (Fall 2013 Edition), , http://plato.stanford.edu/archives/fall2013/entries/moral-particularism/, E.N. Zalta; Davidson, D., Essays on Actions and Events (1980), Oxford University Press Oxford, New York; Dennett, D.C., The Intentional Stance (1987), MIT Press Cambridge, MA; Dennett, D.C., Cognitive science as reverse engineering. Several meanings of â€œTop-downâ€ and â€œBottom-upâ€ (1994) Logic, Methodology and Philosophy of Science 9, pp. 679-693. , D. Prawitz B. Skyrms D. Westerstahl Elsevier Amsterdam; De Paz Santana, J.F., An integrated system for helping disabled and dependent people: AGALZ, AZTECA, and MOVI-MAS projects (2015) Adv. Intell. Syst. Comput., 333, pp. 3-24; Dretske, F., Explaining Behavior: Reasons in a World of Causes (1995), 4th printing Cambridge University Press Cambridge; Floridi, L., Sanders, J.W., On the morality of artificial agents (2004) Minds Mach., 14, pp. 349-379; Fong, T., Illah, N., Dautenhahn, K., A Survey of Socially Interactive Robots: Concepts, Design, and Applications (2002), Technical Report No. CMU-RI-TR: 2â€“29 Robotics Institute, Carnegie Mellon University; Frankena, W.K., The concept of morality (1966) Journal of Philosophy, 63, pp. 688-696; Frankfurt, H., Freedom of the will and the concept of a person (1971) Journal of Philosophy, 68, pp. 5-20; Froese, T., Di Paolo, E., Modelling social interaction as perceptual crossing: an investigation into the dynamics of the interaction process (2010) Connect. Sci., 22, pp. 43-68; Horgan, T., Timmons, M., What does the frame problem tell us about moral normativity? (2009) Ethical Theory and Moral Practice, 12, pp. 25-51; Knobe, J., What is experimental philosophy? (2004) Philos. Mag., 28, pp. 37-39; Korsgaard, C.M., The Sources of Normativity (1996), Cambridge University Press Cambridge; Levy, N., Consciousness and Moral Responsibility (2014), Oxford University Press Oxford; Marr, D., Vision: A Computational Investigation into the Human Representation and Processing of Visual Information (1982), MIT Press Cambridge, MA; McCarthy, J., Hayes, P.J., Some Philosophical Problems from the Standpoint of Artificial Intelligence (1969) Machine Intelligence, pp. 463-502. , B. Meltzer D. Michie Edinburgh University Press Edinburgh, UK; Misselhorn, C., Robots as moral agents? (2013) Roboethics, Proceedings of the Annual Conference on Ethics of the German Association for Social Science Research on Japan, pp. 30-42. , F. Roevekamp Iudicum MÃ¼nchen; Misselhorn, C., Ethical considerations regarding the use of social robots in the fourth age (2013) GeroPsych, J. Gerontopsychol. Geriatr. Psychiatry, 26, pp. 121-133. , Special issue: emotional and social robots for aging well?; Misselhorn, C., Collective agency and cooperation in natural and artificial systems (2015) Collective Agency and Cooperation in Natural and Artificial Systems. Explanation, Implementation and Simulation, Philosophical Studies Series, 122, pp. 3-25. , C. Misselhorn Springer Dordrecht; Nichols, S., Knobe, J., (2008) Experimental Philosophy, , Oxford University Press Oxford, New York; Nussbaum, M., â€œFinely aware and richly responsible.â€ Moral attention and the moral task of literature (1985) Journal of Philosophy, 82, pp. 516-529; Nussbaum, M., Frontiers of Justice: Disability, Nationality, Species Membership (2006), The Belknap Press of Harvard University Press Cambridge, London; Nussbaum, M., Adaptive preferences and women's options (2001) Economics and Philosophy, 17, pp. 67-88; Nussbaum, M., Sen, A., The Quality of Life (1993), Clarendon Press Oxford; Parra, V., A multiagent system to assist elder people by TV communication (2014) ADCAIJ, Adv. Distrib. Comput. Artif. Intell. J., 3, pp. 10-16; Rawls, J., Political Liberalism (1993), Columbia University Press New York; Ross, W.D., The Right and the Good (1930), Clarendon Press Oxford; Stahl, B.C., Information, ethics, and computers: the problem of autonomous moral agents (2004) Minds and Machines, 14, pp. 67-83; Shanahan, M., The frame problem (2009) The Stanford Encyclopedia of Philosophy (Winter 2009 Edition), , http://plato.stanford.edu/archives/win2009/entries/frame-problem/, Edward N. Zalta; Von der Pfordten, D., Five elements of normative ethics â€“ a general theory of normative individualism (2012) Ethical Theory and Moral Practice, 15, pp. 449-471; Wallach, W., Allen, C., Moral Machines: Teaching Robots Right from Wrong (2009), Oxford University Press Oxford, New York; Zato DomÃ­nguez, C., Virtual organizations of agents for monitoring elderly and disabled people in geriatric residences. Information Fusion (FUSION) 2013 (2013) 16th International Conference, Istanbul, Turkey, July 9â€“12, pp. 327-333},
correspondence_address1={NA},
editor={NA},
publisher={Elsevier B.V.},
issn={00043702},
isbn={NA},
language={English},
abbrev_source_title={Artif Intell},
document_type={Article},
source={Scopus},
number={NA},
art_number={103179},
funding_details={NA},
coden={AINTB},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Dorobantu20191004,
type={ARTICLE},
author={Dorobantu, M. and Wilks, Y.},
title={MORAL ORTHOSES: A NEW APPROACH TO HUMAN AND MACHINE ETHICS},
journal={Zygon},
year={2019},
volume={54},
pages={1004-1021},
doi={10.1111/zygo.12560},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075171196&doi=10.1111%2fzygo.12560&partnerID=40&md5=256b365bd80fe11eac9cd45f31e683c4},
affiliation={University of Strasbourg, Strasbourg, France; University of Sheffield, Oxford, United Kingdom},
abstract={Machines are increasingly involved in decisions with ethical implications, which require ethical explanations. Current machine learning algorithms are ethically inscrutable, but not in a way very different from human behavior. This article looks at the role of rationality and reasoning in traditional ethical thought and in artificial intelligence, emphasizing the need for some explainability of actions. It then explores Neil Lawrence's embodiment factor as an insightful way of looking at the differences between human and machine intelligence, connecting it to the theological understanding of embodiment, relationality, and personhood. Finally, it proposes the notion of artificial moral orthoses, which could provide ethical explanations for both artificial and human agents, as a more promising unifying approach to human and machine ethics. Â© 2019 by the Joint Publication Board of Zygon},
author_keywords={artificial companions;  artificial intelligence;  David Hume;  embodiment;  ethics;  explainable AI;  machine learning;  Neil Lawrence;  relationality;  theology},
keywords={NA},
references={Akoudas, K., Bringsjord, S., Bello, P., Towards Ethical Robots via Mechanized Deontic Logic (2005) AAAI Fall Symposium on Machine Ethics, pp. 17-23. , edited by, Geert-Jan M Kruijff, Fiora Pirri, Menlo Park, CA, The AAAI Press; Ananthanarayanan, R., Esser, S.K., Simon, H.D., Modha, D.S., The Cat Is Out of the Bag: Cortical Simulations with 109 Neurons, 1013 Synapses (2009) Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis, , https://doi.org/10.1145/1654059.1654124, â€”-sc â€™09, Article 63; Anderson, M., Anderson, S.L., Robot Be Good (2010) Scientific American, pp. 72-77. , 303; Asimov, I., Runaround (1950) I, Robot, , edited by, Isaac Asimov, 25â€“45., New York, NY, Doubleday; Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., Bonnefo, J.-F., Rahwan, I., The Moral Machine Experiment (2018) Nature, 563, pp. 59-64; Bostrom, N., Yudkowsky, E., The Ethics of Artificial Intelligence (2014) The Cambridge Handbook of Artificial Intelligence, pp. 316-334. , edited by, Keith Frankish, William M. Ramsey, Cambridge, UK, Cambridge University Press; Charniak, E., (1996) Statistical Language Learning, , Cambridge, MA, Bradford Books; Clocksin, W.F., (2002) Artificial Intelligence and Theological Anthropology, , â€ Report on Theological Anthropology, Faith and Order Commission, World Council of Churches FO 33. Grand-Saconnex, Switzerland World Council of Churches; Dennett, D., Intentional Systems (1971) The Journal of Philosophy, 68, pp. 87-106; Dorobantu, M., Recent Advances in Artificial Intelligence (AI) and Some of the Issues in the Theology and AI Dialogue (2019) ESSSAT News and Reviews, 29, pp. 4-17; Eubanks, V., (2018) Automating Inequality, , New York, NY, Macmillan; Foot, P., (2002) Moral Dilemmas, , Oxford, UK, Clarendon Press; Ford, K.M., Hayes, P.J., Glymour, C., Allen, J., Cognitive Orthoses: Towards Human-centered AI (2015) AI Magazine, 36, pp. 5-8; Gergen, K.J., (1991) The Saturated Self: Dilemmas of Identity in Contemporary Life, , New York, NY, Basic Books; Gide, A., (1914) Les caves du Vatican, , Paris, France, Editions de la nouvelle revue; Gray, J., (2002) Straw Dogs, , London, UK, Granta Books; Haidt, J., (2006) The Happiness Hypothesis: Finding Modern Truth in Ancient Wisdom, , New York, NY, Basic Books; Herzfeld, N., Co-creator or co-Creator? The Problem with Artificial Intelligence (2005) Creative Creatures: Values and Ethical Issues in Theology, Science and Technology, pp. 45-52. , edited by, Ulf GÃ¶rman, Willem Drees, Hubert Meisinger, London, UK, T&T Clark; Hume, D., (1751) An Enquiry Concerning the Principles of Morals, , 1907., London, UK, Longman, GreenCo; Hume, D., (2007) A Treatise of Human Nature, (1738). , Oxford, UK, Clarendon Press; Jaynes, J., (1976) The Origin of Consciousness in the Breakdown of the Bicameral Mind, , Boston, MA, Houghton Mifflin; Kahneman, D., (2011) Thinking, Fast and Slow, , New York, NY, Farrar, Straus and Giroux; Lawrence, N., (2017) Living Together: Mind and Machine Intelligence, , https://arxiv.org/abs/1705.07996; Leibniz, G.W., Opinion on the Principles of Pufendorf (1988) Leibniz: Political Writings, pp. 64-76. , edited by, Patrick Riley, Cambridge, UK, Cambridge University Press; Levy, D., (2007) Love and Sex with Robots, , New York, NY, Harper Collins; MacIntyre, A., (1985) After Virtue, , 2nd ed., London, UK, Duckworth; Marsella, S., Gratch, J., Petta, P., Computational Models of Emotion (2010) A Blueprint for Affective Computing: A Sourcebook, pp. 21-46. , edited by, Klaus R. Scherer, Tanja BÃ¤nziger, Etienne B. Roesch, Oxford, UK, Oxford University Press; McDermott, D., Why Ethics Is a High Hurdle for AI (2008) Proceedings of the North American Conference on Computers and Philosophy (NACAP), , Bloomington, IN; McFadyen, A., (1990) The Call to Personhood: A Christian Theory of the Individual in Social Relationships, , Cambridge, UK, Cambridge University Press; Moor, J.H., The Nature, Importance, and Difficulty of Machine Ethics (2006) IEEE Intelligent Systems, 21, pp. 18-21; Mueller, S.T., Hoffman, R.R., Clancey, W., Emrey, A., Klein, G., (2019) Explanation in Human-AI Systems: A Literature Meta-Review Synopsis of Key Ideas and Publications and Bibliography for Explainable AI, , https://arxiv.org/pdf/1902.01876.pdf, â€ DARPA XAI Program; Parisi, D., Mental Robotics (2007) Artificial Consciousness, pp. 191-211. , edited by, Antonio Chella, Riccardo Manzotti, Exeter, UK, Imprint Academic; Pearl, J., (2018) The Book of Why: The New Science of Cause and Effect, , New York, NY, Basic Books; Shults, F.L., (2003) Reforming Theological Anthropology: After the Philosophical Turn to Relationality, , Grand Rapids, MI, William B. Eerdmans; Vincent, J., (2019) World's Fastest Supercomputer Will Be Built by AMD and Cray for US Government, , https://www.theverge.com/2019/5/7/18535078/worlds-fastest-exascale-supercomputer-frontier-amd-cray-doe-oak-ridge-national-laboratory, The Verge; Waldrop, M.M., A Question of Responsibility (1987) AI Magazine, 8, pp. 29-39; Wason, P.C., Johnson-Laird, P., (1972) Psychology of Reasoning: Structure and Content, , Cambridge, MA, Harvard University Press; Wilks, Y., Understanding without Proofs (1973) Proceedings of the 3rd International Joint Conference on Artificial Intelligence, pp. 270-277. , San Francisco, CA, Morgan Kaufmann Publishers; Wilks, Y., Machines and Consciousness (1984) Minds, Machines and Evolution, pp. 105-129. , edited by, Christopher Hookway, Cambridge, UK, Cambridge University Press; Wilks, Y., (2010) Artificial Companions, , ed., Amsterdam, The Netherlands, John Benjamins; Wilks, Y., Ballim, A., Liability and Consent (1990) Law, Computers and Artificial Intelligence, , edited by, Ajit Narayanan, Mervyn Bennun, Norwood, NJ, Ablex; Yudkowsky, E., (2016) The AI Alignment Problem: Why It's Hard and Where to Start. Machine Intelligence Research Institute (MIRI) website, , https://intelligence.org/files/AlignmentHardStart.pdf; Zittrain, J., (2019) The Hidden Costs of Automated Thinking, , The New Yorker, July 23},
correspondence_address1={NA},
editor={NA},
publisher={Blackwell Publishing Ltd},
issn={05912385},
isbn={NA},
language={English},
abbrev_source_title={Zygon},
document_type={Article},
source={Scopus},
number={4},
art_number={NA},
funding_details={Magdalene College, University of CambridgeMagdalene College, University of Cambridge funding_textÂ 1={We are indebted to comments and criticisms from Selmer Bringsjord, Clark Glymour, Noel Sharkey, Fraser Watts, John Tait, and Eugene Charniak. The errors are, as always, all our own. A version of this article was presented at the annual meeting of the Epiphany Philosophers held at Magdalene College, Cambridge, UK, on January 9, 2019.},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Frank2019,
type={ARTICLE},
author={Frank, D.-A. and Chrysochou, P. and Mitkidis, P. and Ariely, D.},
title={Human decision-making biases in the moral dilemmas of autonomous vehicles},
journal={Scientific Reports},
year={2019},
volume={9},
pages={NA},
doi={10.1038/s41598-019-49411-7},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072118119&doi=10.1038%2fs41598-019-49411-7&partnerID=40&md5=4638e8e7705e627bf9ad59d1c6955ff0},
affiliation={Department of Management, Aarhus University, Aarhus, Denmark; Ehrenberg-Bass Institute for Marketing Science, School of Marketing, University of South AustraliaSA, Australia; Center for Advanced Hindsight, Duke University, Durham, United States},
abstract={The development of artificial intelligence has led researchers to study the ethical principles that should guide machine behavior. The challenge in building machine morality based on peopleâ€™s moral decisions, however, is accounting for the biases in human moral decision-making. In seven studies, this paper investigates how peopleâ€™s personal perspectives and decision-making modes affect their decisions in the moral dilemmas faced by autonomous vehicles. Moreover, it determines the variations in peopleâ€™s moral decisions that can be attributed to the situational factors of the dilemmas. The reported studies demonstrate that peopleâ€™s moral decisions, regardless of the presented dilemma, are biased by their decision-making mode and personal perspective. Under intuitive moral decisions, participants shift more towards a deontological doctrine by sacrificing the passenger instead of the pedestrian. In addition, once the personal perspective isÂ made salient participants preserve the lives of that perspective, i.e. the passenger shifts towards sacrificing the pedestrian, and vice versa. These biases in peopleâ€™s moral decisions underline the social challenge in the design of a universal moral code for autonomous vehicles. We discuss the implications of our findings and provide directions for future research. Â© 2019, The Author(s).},
author_keywords={NA},
keywords={adult;  article;  human;  morality;  pedestrian;  artificial intelligence;  car;  decision making;  female;  male, Adult;  Artificial Intelligence;  Automobiles;  Decision Making;  Female;  Humans;  Male;  Morals},
references={(2016) Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles, , http://standards.sae.org/j3016_201609/; (2017) Automated Driving Systems 2.0: A Vision for Safety, , https://www.nhtsa.gov/sites/nhtsa.dot.gov/files/documents/13069a-ads2.0_090617_v9a_tag.pdf; Pillath, S., (2016) Automated Vehicles in the EU, , http://www.europarl.europa.eu/RegData/etudes/BRIE/2016/573902/EPRS_BRI(2016)573902_EN.pdf, European Parliament; Kockelman, K., (2016) Implications of Connected and Automated Vehicles on the Safety and Operations of Roadway Networks: A Final Report, , https://library.ctr.utexas.edu/ctr-publications/0-6849-1.pdf, Center for Transportation Research, The University of Texas; Silberg, G., (2012) Self-Driving Cars: The Next Revolution, , https://staff.washington.edu/jbs/itrans/self_driving_cars[1].pdf, KPMG LLP & Center of Automotive Research; Thompson, C., Why driverless cars will be safer than human drivers (2016) Business Insider Nordic, , https://www.businessinsider.de/why-driverless-cars-will-be-safer-than-human-drivers-2016-11; Bonnefon, J.-F., Shariff, A., Rahwan, I., The social dilemma of autonomous vehicles (2016) Science, 352, p. 1573. , COI: 1:CAS:528:DC%2BC28XhtVaitLzK, PID: 27339987; Althauser, J., Moral programming will define the future of autonomous transportation (2017) Venturebeat, , https://venturebeat.com/2017/09/24/moral-programming-will-define-the-future-of-autonomous-transportation/; Himmelreich, J., (2018) The Everyday Ethical Challenges of Self-Driving Cars, , https://theconversation.com/the-everyday-ethical-challenges-of-self-driving-cars-92710, The Conversation; Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., Bonnefon, J.-F., Rahwan, I., The Moral Machine experiment (2018) Nature, 563 (7729), pp. 59-64; Meder, B., Fleischhut, N., Krumnau, N.-C., Waldmann, M.R., How Should Autonomous Cars Drive? A Preference for Defaults in Moral Judgments Under Risk and Uncertainty (2019) Risk Analysis, 39, pp. 295-314. , PID: 30157299; Noothigattu, R., A Voting-Based System for Ethical Decision Making (2017) arXiv preprint arXiv, 1709, p. 06692; SÃ¼tfeld, L.R., Gast, R., KÃ¶nig, P., Pipa, G., Using Virtual Reality to Assess Ethical Decisions in Road Traffic Scenarios: Applicability of Value-of-Life-Based Models and Influences of Time Pressure (2017) Frontiers in Behavioral Neuroscience, 11. , &; Shariff, A., Bonnefon, J.-F., Rahwan, I., Psychological roadblocks to the adoption of self-driving vehicles (2017) Nature Human Behaviour, 1 (10), pp. 694-696; Greene, J.D., Sommerville, R.B., Nystrom, L.E., Darley, J.M., Cohen, J.D., An fMRI Investigation of Emotional Engagement in Moral Judgment (2001) Science, 293, pp. 2105-2108. , COI: 1:CAS:528:DC%2BD3MXntVCitro%3D, PID: 11557895; Greene, J.D., Morelli, S.A., Lowenberg, K., Nystrom, L.E., Cohen, J.D., Cognitive load selectively interferes with utilitarian moral judgment (2008) Cognition, 107, pp. 1144-1154. , PID: 18158145; Rand, D.G., Cooperation, Fast and Slow: Meta-Analytic Evidence for a Theory of Social Heuristics and Self-Interested Deliberation (2016) Psychological Science, 27, pp. 1192-1206. , PID: 27422875; Thomson, J.J., The Trolley Problem (1985) Yale Law J, 94, pp. 1395-1415; Kahneman, D., (2011) Thinking, Fast and Slow, , Macmillan; Evans, J.S.B.T., Curtis-Holmes, J., Rapid responding increases belief bias: Evidence for the dual-process theory of reasoning (2005) Thinking & Reasoning, 11, pp. 382-389; Stanovich, K.E., West, R.F., Individual differences in reasoning: Implications for the rationality debate? (2000) Behavioral and Brain Sciences, 23, pp. 645-665. , COI: 1:STN:280:DC%2BD3MzjslCrtw%3D%3D, PID: 11301544; Epstein, S., Pacini, R., (1999) Some Basic Issues regarding Dual-Process Theories from the Perspective of cognitiveâ€“experiential Self-Theory in Dual-Process Theories in Social Psychology, pp. 462-482. , Guilford Press; Greene, J.D., Nystrom, L.E., Engell, A.D., Darley, J.M., Cohen, J.D., The Neural Bases of Cognitive Conflict and Control in Moral Judgment (2004) Neuron, 44, pp. 389-400. , COI: 1:CAS:528:DC%2BD2cXpslOgtrc%3D, PID: 15473975; Simon, H.A., Theories of bounded rationality (1972) Decision and Organization, 1, pp. 161-176; March, J.G., Bounded Rationality, Ambiguity, and the Engineering of Choice (1978) The Bell Journal of Economics, 9, pp. 587-608; Kahneman, D., Maps of Bounded Rationality: Psychology for Behavioral Economics (2003) The American Economic Review, 93, pp. 1449-1475; Greene, J.D., Dual-process morality and the personal/impersonal distinction: A reply to McGuire, Langdon, Coltheart, and Mackenzie (2009) Journal of Experimental Social Psychology, 45, pp. 581-584; Oatley, K., Johnson-laird, P.N., Towards a Cognitive Theory of Emotions (1987) Cognition and Emotion, 1, pp. 29-50; Bar-Eli, M., Azar, O.H., Ritov, I., Keidar-Levin, Y., Schein, G., Action bias among elite soccer goalkeepers: The case of penalty kicks (2007) Journal of Economic Psychology, 28, pp. 606-621; Patt, A., Zeckhauser, R., Action Bias and Environmental Decisions (2000) Journal of Risk and Uncertainty, 21, pp. 45-72; Kahneman, D., Knetsch, J.L., Thaler, R.H., Anomalies: The Endowment Effect, Loss Aversion, and Status Quo Bias (1991) Journal of Economic Perspectives, 5, pp. 193-206; Levy, D.T., Youth and traffic safety: The effects of driving age, experience, and education. Accident Analysis & (1990) Prevention, 22, pp. 327-334. , COI: 1:STN:280:DyaK3M%2FisFeqsA%3D%3D; Bandura, A., Toward a Psychology of Human Agency (2006) Perspectives on Psychological Science, 1, pp. 164-180. , PID: 26151469; De Groot, J.I.M., Steg, L., Morality and Prosocial Behavior: The Role of Awareness, Responsibility, and Norms in the Norm Activation Model (2009) The Journal of Social Psychology, 149, pp. 425-449. , PID: 19702104; Sneijder, P., te Molder, H.F.M., Moral logic and logical morality: Attributions of responsibility and blame in online discourse on veganism (2005) Discourse & Society, 16, pp. 675-696; Brickman, P., Ryan, K., Wortman, C.B., Causal chains: Attribution of responsibility as a function of immediate and prior causes (1975) Journal of Personality and Social Psychology, 32, pp. 1060-1067; Johnson, J.T., Drobny, J., Proximity biases in the attribution of civil liability (1985) Journal of Personality and Social Psychology, 48, pp. 283-296; Turiel, E., (1983) The Development of Social Knowledge: Morality and Convention, , Cambridge University Press; Cialdini, R.B., Trost, M.R., (1998) Social Influence: Social Norms, Conformity and Compliance, , McGraw-Hill; McShane, B.B., BÃ¶ckenholt, U., Single-Paper Meta-Analysis: Benefits for Study Summary, Theory Testing, and Replicability (2017) Journal of Consumer Research, 43, pp. 1048-1063; Wakabayashi, D., Uber Ordered to Take Its Self-Driving Cars Off Arizona Roads (2018) The New York Times, , https://www.nytimes.com/2018/03/26/technology/arizona-uber-cars.html},
correspondence_address1={Frank, D.-A.; Department of Management, Denmark; email: research@dariusfrank.com},
editor={NA},
publisher={Nature Publishing Group},
issn={20452322},
isbn={NA},
language={English},
abbrev_source_title={Sci. Rep.},
document_type={Article},
source={Scopus},
number={1},
art_number={13080},
funding_details={Aarhus UniversitetAarhus Universitet,Â AU,Â 26122 funding_textÂ 1={This research received funding from the Interacting Minds Center (IMC), Aarhus University (Seed number: 26122).},
coden={NA},
pubmed_id={31511560},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Bennett201928,
type={CONFERENCE},
author={Bennett, S.J.},
title={Investigating the role of moral decision-making in emerging artificial intelligence technologies},
journal={Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW},
year={2019},
volume={NA},
pages={28-32},
doi={10.1145/3311957.3361858},
note={cited By 1; Conference of 22nd ACM Conference on Computer-Supported Cooperative Work and Social Computing, CSCW 2019 ; Conference Date: 9 November 2019 Through 13 November 2019;  Conference Code:155036},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076101605&doi=10.1145%2f3311957.3361858&partnerID=40&md5=2a892a7ef13fd4e9ca99f4928e555ab8},
affiliation={University of Edinburgh, Edinburgh, EH8 9YL, United Kingdom},
abstract={In the midst of the current boom in ethical principles, frameworks and guidelines for emerging applications of artificial intelligence (AI), it is difficult to assess how these translate into the context of real-world applications. Through interviews and ethnography, my research explores AI specialists' accounts of navigating the ethical and social impact of their work, examining and providing insight into the various interactions impacting ethical decision-making in AI system development. Having investigated behavior of AI specialists as proactive moral agents, the work then aims to explore how we can support meaningful applications of ethics in system design and development. Â© 2019 Copyright is held by the author/owner(s).},
author_keywords={Artificial intelligence;  Design;  Ethics},
keywords={Artificial intelligence;  Decision making;  Design;  Interactive computer systems;  Philosophical aspects, Artificial intelligence technologies;  Emerging applications;  Ethical decision making;  Ethical principles;  Ethics;  Moral agents;  Social impact;  System design; development, Groupware},
references={Bogner, A., Littig, B., Menz, W., Introduction: Expert interviews - An introduction to a new methodological debate (2009) Interviewing Experts, pp. 1-13. , Palgrave Macmillan, London; Buolamwini, J., Gebru, T., Gender shades: Intersectional accuracy disparities in commercial gender classification (2018) Conference on Fairness, Accountability and Transparency, pp. 77-91. , January; Chouldechova, A., Fair prediction with disparate impact: A study of bias in recidivism prediction instruments (2017) Big Data, 5 (2), pp. 153-163; Friedman, B., Kahn, P.H., Borning, A., Value sensitive design and information systems (2008) The Handbook of Information and Computer Ethics, pp. 69-101; Green, B., (2018) Data Science as Political Action: Grounding Data Science in a Politics of Justice; Hagendorff, T., (2019) The Ethics of AI Ethics - An Evaluation of Guidelines; Hornung, H., Pereira, R., Baranauskas, M.C.C., Liu, K., Challenges for human-data interaction - A semiotic perspective (2015) International Conference on HumanComputer Interaction, pp. 37-48. , August Springer, Cham; Anderson, R.E., Social impacts of computing: Codes of professional ethics (1992) Soc Sci Comput Rev, 10 (2), pp. 453-469; Kilbertus, N., Carulla, M.R., Parascandolo, G., Hardt, M., Janzing, D., SchÃ¶lkopf, B., Avoiding discrimination through causal reasoning (2017) Advances in Neural Information Processing Systems, pp. 656-666; Noy, C., Sampling knowledge: The hermeneutics of snowball sampling in qualitative research (2008) International Journal of Social Research Methodology, 11 (4), pp. 327-344; Rittel, H.W.J., Webber, M.M., Dilemmas in the general theory of planning (1973) Policy Sciences, 4, pp. 155-169; Yapo, A., Weiss, J., (2018) Ethical Implications of Bias in Machine Learning},
correspondence_address1={Bennett, S.J.; University of EdinburghUnited Kingdom; email: sarah.bennett@ed.ac.uk},
editor={NA},
publisher={Association for Computing Machinery},
issn={NA},
isbn={9781450366922},
language={English},
abbrev_source_title={Proc. ACM Conf. Comput. Support. Coop. Work CSCW},
document_type={Conference Paper},
source={Scopus},
number={NA},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={ACM SIGCHI; Adobe; Amazon; et al.; Facebook; IBM Research},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Wang20191311,
type={CONFERENCE},
author={Wang, T. and Liu, J. and Zhao, J. and Yang, X. and Shi, S. and Yu, H. and Ren, X.},
title={Privacy-preserving crowd-guided AI decision-making in ethical dilemmas},
journal={International Conference on Information and Knowledge Management, Proceedings},
year={2019},
volume={NA},
pages={1311-1320},
doi={10.1145/3357384.3357954},
note={cited By 5; Conference of 28th ACM International Conference on Information and Knowledge Management, CIKM 2019 ; Conference Date: 3 November 2019 Through 7 November 2019;  Conference Code:154362},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075448584&doi=10.1145%2f3357384.3357954&partnerID=40&md5=2b402c87af33713e6086abd88b0ca565},
affiliation={Xi'an Jiaotong University Xi'an, Shaanxi, China; University of Hong Kong, Hong Kong; Nanyang Technological University, Singapore; Nanjing University Nanjing, Jiangsu, China},
abstract={With the rapid development of artificial intelligence (AI), ethical issues surrounding AI have attracted increasing attention. In particular, autonomous vehicles may face moral dilemmas in accident scenarios, such as staying the course resulting in hurting pedestrians or swerving leading to hurting passengers. To investigate such ethical dilemmas, recent studies have adopted preference aggregation, in which each voter expresses her/his preferences over decisions for the possible ethical dilemma scenarios, and a centralized system aggregates these preferences to obtain the winning decision. Although a useful methodology for building ethical AI systems, such an approach can potentially violate the privacy of voters since moral preferences are sensitive information and their disclosure can be exploited by malicious parties resulting in negative consequences. In this paper, we report a first-of-its-kind privacy-preserving crowd-guided AI decision-making approach in ethical dilemmas. We adopt the formal and popular notion of differential privacy to quantify privacy, and consider four granularities of privacy protection by taking voter-/record-level privacy protection and centralized/distributed perturbation into account, resulting in four approaches VLCP, RLCP, VLDP, and RLDP, respectively. Moreover, we propose different algorithms to achieve these privacy protection granularities, while retaining the accuracy of the learned moral preference model. Specifically, VLCP and RLCP are implemented with the data aggregator setting a universal privacy parameter and perturbing the averaged moral preference to protect the privacy of voters' data. VLDP and RLDP are implemented in such a way that each voter perturbs her/his local moral preference with a personalized privacy parameter. Extensive experiments based on both synthetic data and real-world data of voters' moral decisions demonstrate that the proposed approaches achieve high accuracy of preference aggregation while protecting individual voter's privacy. Â© 2019 Association for Computing Machinery.},
author_keywords={Artificial intelligence;  Differential privacy;  Ethical decision making},
keywords={Accidents;  Artificial intelligence;  Behavioral research;  Decision making;  Knowledge management;  Philosophical aspects, Centralized systems;  Differential privacies;  Ethical decision making;  Preference aggregations;  Preference modeling;  Privacy preserving;  Privacy protection;  Sensitive informations, Data privacy},
references={Moral Machine Platform, , http://moralmachine.mit.edu/, n. d; Abadi, M., Chu, A., Goodfellow, I., Brendan McMahan, H., Mironov, I., Talwar, K., Zhang, L., Deep learning with differential privacy (2016) Proc. ACM SIGSAC CCS., pp. 308-318; Acquisti, A., Grossklags, J., Privacy and rationality in individual decision making (2005) IEEE Security & Privacy, 3 (1), pp. 26-33. , 2005; Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., Bonnefon, J.-F., Rahwan, I., The moral machine experiment (2018) Nature, 563 (7729), p. 59. , 2018; Bonnefon, J.-F., Shariff, A., Rahwan, I., The social dilemma of autonomous vehicles (2016) Science, 352 (6293), pp. 1573-1576. , 2016; Brubaker, K., (2018) Artificial Intelligence: Issues of Consumer Privacy, Industry Risks, and Ethical Concerns, , Ph.D. Dissertation. Utica College; Conitzer, V., Sinnott-Armstrong, W., Borg, J.S., Deng, Y., Kramer, M., Moral decision making frameworks for artificial intelligence (2017) Proc. AAAI., pp. 4831-4835; Duchi, J.C., Jordan, M.I., Wainwright, M.J., Local privacy and statistical minimax rates (2013) Proc. IEEE FOCS., pp. 429-438; Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R., Fairness through awareness (2012) Proc. ITCS., pp. 214-226; Dwork, C., Kenthapadi, K., McSherry, F., Mironov, I., Naor, M., Our data, ourselves: Privacy via distributed noise generation (2006) Proc. Eurocrypt., pp. 486-503; Dwork, C., McSherry, F., Nissim, K., Smith, A., Calibrating noise to sensitivity in private data analysis (2006) Proc. TCC., pp. 265-284; Dwork, C., Roth, A., The algorithmic foundations of differential privacy (2014) Foundations and Trends in Theoretical Computer Science, 9 (3-4), pp. 211-407. , 2014; Erlingsson, Ãš., Pihur, V., Korolova, A., Rappor: Randomized aggregatable privacy-preserving ordinal response (2014) Proc. ACM CCS., pp. 1054-1067; Fredrikson, M., Jha, S., Ristenpart, T., Model inversion attacks that exploit confidence information and basic countermeasures (2015) Proc. ACM SIGSAC CCS., pp. 1322-1333; Greene, J., Rossi, F., Tasioulas, J., Venable, K.B., Williams, B.C., Embedding ethical principles in collective decision support systems (2016) Proc. AAAI, 16, pp. 4147-4151; Greene, J.D., Our driverless dilemma (2016) Science, 352 (6293), pp. 1514-1515. , 2016; Hammond, L., Belle, V., (2018) Deep Tractable Probabilistic Models for Moral Responsibility, , arXiv preprint 2018; Jones, M.L., Kaufman, E., Edenberg, E., AI and the ethics of automating consent (2018) IEEE Security & Privacy, 16 (3), pp. 64-72. , 2018; Jorgensen, Z., Yu, T., Cormode, G., Conservative or liberal? Personalized differential privacy (2015) Proc. IEEE ICDE., pp. 1023-1034; Mosteller, F., Remarks on the method of paired comparisons: I. The least squares solution assuming equal standard deviations and equal correlations (2006) Selected Papers of Frederick Mosteller, pp. 157-162. , Springer; Noothigattu, R., Gaikwad, S.S., Awad, E., Dsouza, S., Rahwan, I., Ravikumar, P., Procaccia, A.D., A voting-based system for ethical decision making (2018) Proc. AAAI., pp. 1587-1594; Rudin, W., (1964) Principles of Mathematical Analysis, 3. , McGraw-hill New York; Shariff, A., Bonnefon, J.-F., Rahwan, I., Psychological roadblocks to the adoption of self-driving vehicles (2017) Nature Human Behaviour, 1 (10), p. 694. , 2017; Srivastava, M., Heidari, H., Krause, A., (2019) Mathematical Notions Vs. Human Perception of Fairness: A Descriptive Approach to Fairness for Machine Learning, , arXiv preprint 2019; Tang, J., Korolova, A., Bai, X., Wang, X., Wang, X., (2017) Privacy Loss in Apple's Implementation of Differential Privacy on MacOS 10.12, , arXiv preprint 2017; TramÃ¨r, F., Huang, Z., Hubaux, J.-P., Ayday, E., Differential privacy with bounded priors: Reconciling utility and privacy in genome-wide association studies (2015) Proc. ACM CCS., pp. 1286-1297; Wallach, W., Allen, C., (2008) Moral Machines: Teaching Robots Right from Wrong, , Oxford University Press; Wang, N., Xiao, X., Yang, Y., Zhao, J., Hui, S.C., Shin, H., Shin, J., Yu, G., Collecting and analyzing multidimensional data with local differential privacy (2019) Proc. IEEE ICDE., pp. 638-649; Yu, H., Shen, Z., Miao, C., Leung, C., Lesser, V.R., Yang, Q., Building ethics into artificial intelligence (2018) Proc. IJCAI., pp. 5527-5533; Zhang, H., Conitzer, V., A PAC framework for aggregating agentsÃ¢Ä‚Å¹ judgments (2019) Proc. AAAI., pp. 2237-2244; Zhang, J., Zhang, Z., Xiao, X., Yang, Y., Winslett, M., Functional mechanism: Regression analysis under differential privacy (2012) Proc. VLDB Endowment, 5 (11), pp. 1364-1375. , 2012},
correspondence_address1={NA},
editor={NA},
publisher={Association for Computing Machinery},
issn={NA},
isbn={9781450369763},
language={English},
abbrev_source_title={Int Conf Inf Knowledge Manage},
document_type={Conference Paper},
source={Scopus},
number={NA},
art_number={NA},
funding_details={M4082443.020},
coden={NA},
pubmed_id={NA},
sponsors={ACM SIGIR; ACM SIGWEB},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{vanWynsberghe2019719,
type={ARTICLE},
author={van Wynsberghe, A. and Robbins, S.},
title={Critiquing the Reasons for Making Artificial Moral Agents},
journal={Science and Engineering Ethics},
year={2019},
volume={25},
pages={719-735},
doi={10.1007/s11948-018-0030-8},
note={cited By 44},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042189041&doi=10.1007%2fs11948-018-0030-8&partnerID=40&md5=f2cc175e33498f68f690a39f3664bf7c},
affiliation={Technical University of Delft, Jaffalaan 5, Delft, 2628 BX, Netherlands},
abstract={Many industry leaders and academics from the field of machine ethics would have us believe that the inevitability of robots coming to have a larger role in our lives demands that robots be endowed with moral reasoning capabilities. Robots endowed in this way may be referred to as artificial moral agents (AMA). Reasons often given for developing AMAs are: the prevention of harm, the necessity for public trust, the prevention of immoral use, such machines are better moral reasoners than humans, and building these machines would lead to a better understanding of human morality. Although some scholars have challenged the very initiative to develop AMAs, what is currently missing from the debate is a closer examination of the reasons offered by machine ethicists to justify the development of AMAs. This closer examination is especially needed because of the amount of funding currently being allocated to the development of AMAs (from funders like Elon Musk) coupled with the amount of attention researchers and industry leaders receive in the media for their efforts in this direction. The stakes in this debate are high because moral robots would make demands on society; answers to a host of pending questions about what counts as an AMA and whether they are morally responsible for their behavior or not. This paper shifts the burden of proof back to the machine ethicists demanding that they give good reasons to build AMAs. The paper argues that until this is done, the development of commercially available AMAs should not proceed further. Â© 2018, The Author(s).},
author_keywords={Artificial moral agents;  Machine ethics;  Robot ethics},
keywords={artificial intelligence;  ethicist;  ethics;  morality;  robotics, Artificial Intelligence;  Ethical Analysis;  Ethicists;  Moral Development;  Morals;  Robotics},
references={Allen, C., Smit, I., Wallach, W., Artificial morality: Top-down, bottom-up, and hybrid approaches (2005) Ethics and Information Technology, 7 (3), pp. 149-155; Allen, C., Varner, G., Zinser, J., Prolegomena to any future artificial moral agent (2000) Journal of Experimental & Theoretical Artificial Intelligence, 12 (3), pp. 251-261; Allen, C., Wallach, W., Moral machines: Contradition in terms of abdication of human responsibility? (2011) Robot ethics: The ethical and social implications of robotics, pp. 55-68. , Lin P, Abney K, Bekey GA, (eds), MIT Press, Cambridge; Allen, C., Wallach, W., Smit, I., Why machine ethics? (2006) IEEE Intelligent Systems, 21 (4), pp. 12-17; Anderson, S.L., Machine metaethics (2011) Machine Ethics, , M. Anderson, S. L. Anderson, New York, Cambridge University Press; Anderson, M., Anderson, S.L., Machine ethics: Creating an ethical intelligent agent (2007) AI Magazine, 28 (4), pp. 15-26; Anderson, M., Anderson, S.L., Robot be good: A call for ethical autonomous machines (2010) Scientific American, 303 (4), pp. 15-24; Anderson, M., Anderson, S.L., (2011) Machine ethics, , Cambridge University Press, Cambridge; (1998) The Nicomachean ethics, , http://books.google.nl/books?id=Dk2VFlZyiJQC, Oxford University Press. Retrieved from, Accessed 24 Oct 2014; Arkin, R., (2009) Governing lethal behavior in autonomous robots, , CRC Press, Boca Raton; Asimov, I., (1963) I, Robot, , Spectra, New York; Baier, A., Trust and antitrust (1986) Ethics, 96 (2), pp. 231-260; Bryson, J., Robots should be slaves (2008) Close Engagements with Artificial Companions: Key Social, Psychological, Ethical and Design Issue, pp. 63-74. , https://books.google.nl/books?id=EPznZHeG89cC, In Y. Wilks (Ed.), Amsterdam: John Benjamins Publishing. Retrieved from, Accessed 7 Mar 2017; Cellan-Jones, R., Stephen Hawking warns artificial intelligence could end mankind (2014) BBC News, , http://www.bbc.com/news/technology-30290540.Accessed29Aug2016, Retrieved from; Coeckelbergh, M., Robot rights? Towards a social-relational justification of moral consideration (2010) Ethics and Information Technology, 12 (3), pp. 209-221; Darling, K., (2012) Extending Legal Protection to Social Robots: The Effects of Anthropomorphism, Empathy, and Violent Behavior Towards Robotic Objects, , https://papers.ssrn.com/abstract=2044797, Rochester, NY, Retrieved from; Deng, B., Machine ethics: The robotâ€™s dilemma (2015) Nature News, 523 (7558), p. 24; Dietrich, E., Homo sapiens 2.0: Why we should build the better robots of our nature (2001) Journal of Experimental & Theoretical Artificial Intelligence, 13 (4), pp. 323-328; Doris, J.M., Persons, situations, and virtue ethics (1998) Nous, 32 (4), pp. 504-530. , http://www.jstor.org/stable/pdfplus/2671873.pdf?acceptTC=true, (,).,., (,)., Retrieved from; Finlay, S., Four faces of moral realism (2007) Philosophy Compass, 2 (6), pp. 820-849; Floridi, L., Sanders, J.W., On the morality of artificial agents (2004) Minds and Machines, 14 (3), pp. 349-379; Friedman, B., Nissenbaum, H., Bias in computer systems (1996) ACM Transactions on Information Systems, 14 (3), pp. 330-347. , https://doi.org/10.1145/230538.230561, Retrieved 10 Feb 2017; Gershgorn, D., (2017) Inside the Mechanical Brain of the worldâ€™s First Robot Citizen, , https://qz.com/1121547/how-smart-is-the-first-robot-citizen/, Retrieved 29 Dec 2017; Gips, J., Toward the ethical robot (1994) Android Epistemology, , K. M. Ford, C. Glymour, P. Hayes, Cambridge, MIT Press; Greene, J., (2013) Moral tribes: Emotion, reason, and the gap between us and them, , 1, Penguin Press, New York; Gunkel, D.J., A vindication of the rights of machines (2014) Philosophy & Technology, 27 (1), pp. 113-132; Haidt, J., The emotional dog and its rational tail: A social intuitionist approach to moral judgment (2001) Psychological Review, 108 (4), pp. 814-834; Haidt, J., Joseph, C., The Innate Mind: Volume 3: Foundations and the Future (Evolution and Cognition) (2008) The innate mind, , Carruthers P, Laurence S, Stich S, (eds), Oxford University Press, New York; Hardwig, J., The role of trust in knowledge (1991) The Journal of Philosophy, 88 (12), pp. 693-708; Hatmaker, T., (2017) Saudi Arabia Bestows Citizenship on a Robot Named Sophia, , http://social.techcrunch.com/2017/10/26/saudi-arabia-robot-citizen-sophia/, Retrieved 12 Feb 2018; Himma, K.E., Artificial agency, consciousness, and the criteria for moral agency: What properties must an artificial agent have to be a moral agent? (2009) Ethics and Information Technology, 11 (1), pp. 19-29; Johnson, D.G., Miller, K.W., Un-making artificial moral agents (2008) Ethics and Information Technology, 10 (2-3), pp. 123-133; KristjÃ¡nsson, K., Emulation and the use of role models in moral education (2006) Journal of Moral Education, 35 (1), pp. 37-49. , http://www.tandfonline.com/doi/abs/10.1080/03057240500495278, Retrieved from, Accessed 25 Oct 2014; Lokhorst, G.-J., van den Hoven, J., Responsibility for Military Robots (2011) Robot ethics: The ethical and social implications of robotics, pp. 145-155. , Lin P, Abney K, Bekey GA, (eds), MIT Press, Cambridge; Markoff, J., Relax, the terminator is far away (2015) The New York Times, , http://www.nytimes.com/2015/05/26/science/darpa-robotics-challenge-terminator.html, Retrieved from, Accessed 29 Aug 2016; Merritt, M., Virtue ethics and situationist personality psychology (2000) Ethical Theory and Moral Practice, 3 (4), pp. 365-383; Miller, K.W., Wolf, M.J., Grodzinsky, F., This â€œethical trapâ€ is for roboticists, not robots: on the issue of artificial agent ethical decision-making (2017) Science and Engineering Ethics, 23 (2), pp. 389-401; Moor, J.H., The nature, importance, and difficulty of machine ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 18-21; Moor, J., Four kinds of ethical robots (2009) Philosophy Now, (72), pp. 12-14. , https://philosophynow.org/issues/72/Four_Kinds_of_Ethical_Robots, Retrieved from, Accessed 10 Feb 2017; (2012) The Economist, , http://www.economist.com/node/21556234, Retrieved from, Accessed 7 Mar 2017; Nagenborg, M., Artificial moral agents: An intercultural perspective (2007) International Review of Information Ethics, 7, pp. 129-133. , http://www.i-r-i-e.net/inhalt/007/13-nagenborg.pdf, Retrieved 12 Feb 2018; Nissenbaum, H., How computer systems embody values (2001) Computer -Los Almalitos-, 34, p. 120; Peters, A., Having a heart attack? (2018) This AI Helps Emergency Dispatchers Find Out, , https://www.fastcompany.com/40515740/having-a-heart-attack-this-ai-helps-emergency-dispatchers-find-out, Retrieved January 16, 2018, from; Pizarro, D., Nothing More than Feelings? The Role of Emotions in Moral Judgment (2000) Journal for the Theory of Social Behaviour, 30 (4), pp. 355-375; Roeser, S., (2010) Moral emotions and intuitions, , Springer, Berlin; Rutkin, A., (2014) Ethical Trap: Robot Paralysed by Choice of Who to Save, , https://www.newscientist.com/article/mg22329863-700-ethical-trap-robot-paralysed-by-choice-of-who-to-save/, Retrieved 12 Feb 2018; Scheutz, M., (2016) The need for moral competency in autonomous agent architectures, pp. 515-525. , http://link.springer.com/chapter/10.1007/978-3-319-26485-1_30, In V. C. MÃ¼ller (Ed.), Springer International Publishing. Retrieved from, Accessed 29 Aug 2016; Shafer-Landau, R., Ethical disagreement, ethical objectivism and moral indeterminacy (1994) Philosophy and Phenomenological Research, 54 (2), pp. 331-344; Sharkey, A., Should we welcome robot teachers? (2016) Ethics and Information Technology; Sharkey, A., Can robots be responsible moral agents? And why should we care? (2017) Connection Science, 29 (3), pp. 210-216; Sharkey, N., The ethical frontiers of robotics (2008) Science, 322 (5909), pp. 1800-1801; Sharkey, N., The evitability of autonomous robot warfare (2012) International Review of the Red Cross, 94 (886), pp. 787-799; Sharkey, N., Sharkey, A., The Rights and Wrongs of Robot Care (2011) Robot ethics: The ethical and social implications of robotics, pp. 267-282. , Lin P, Abney K, Bekey GA, (eds), MIT Press, Cambridge; Sharkey, N., Wynsberghe, A., Robbins, S., Hancock, E., (2017) Our Sexual Future with Robots, , https://responsible-roboticsmyxf6pn3xr.netdna-ssl.com/wp-content/uploads/2017/11/FRR-Consultation-Report-Our-Sexual-Future-with-robots-.pdf, The Hague, Netherlands, Retrieved from, Accessed 1 Feb 2018; Shirky, C., (2009) A Speculative Post on the Idea of Algorithmic Authority, , http://www.shirky.com/weblog/2009/11/a-speculative-post-on-the-idea-of-algorithmic-authority/, Retrieved 12 Feb 2018; Simon, J., The entanglement of trust and knowledge on the Web (2010) Ethics and Information Technology, 12 (4), pp. 343-355; Street, S., A darwinian dilemma for realist theories of value (2006) Philosophical Studies, 127 (1), pp. 109-166; Tonkens, R., A challenge for machine ethics (2009) Minds and Machines, 19 (3), pp. 421-438; Vallor, S., Moral deskilling and upskilling in a new machine age: Reflections on the ambiguous future of character (2015) Philosophy & Technology, 28 (1), pp. 107-124; van de Poel, I., Translating Values into design requirements (2013) Philosophy and engineering: Reflections on practice, principles, and process, , Mitchfelder D, McCarty N, Goldberg DE, (eds), Springer, Dordrecht; van den Hoven, J., ICT and value sensitive design (2007) The information society: Innovation, legitimacy, ethics and democracy in honor of professor Jacques Berleur s.j, 233, pp. 67-72. , Goujon P, Lavelle S, Duquenoy P, Kimppa K, (eds), Springer, Boston; van Wynsberghe, A., Designing robots for care: Care centered value-sensitive design (2012) Science and Engineering Ethics, 19 (2), pp. 407-433; van Wynsberghe, A., A method for integrating ethics into the design of robots (2013) Industrial Robot: An International Journal, 40 (5), pp. 433-440; van Wynsberghe, A., (2015) Healthcare Robots: Ethics, Design and Implementation. Healthcare Robots: Ethics, Design and Implementation, , https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946412196&partnerID=40&md5=5c270c5c2c8d9f4983cbe6c4f2369c97, Retrieved from, Accessed 29 Aug 2016; van Wynsberghe, A., Service robots, care ethics, and design (2016) Ethics and Information Technology, 18 (4), pp. 311-321; van Wynsberghe, A., Robbins, S., Ethicist as designer: A pragmatic approach to ethics in the lab (2014) Science and Engineering Ethics, 20 (4), pp. 947-961. , https://doi.org/10.1007/s11948-013-9498-4; Waldrop, M.M., A question of responsibility (1987) AI Magazine, 8 (1), p. 28; Wallach, W., Implementing moral decision making faculties in computers and robots (2007) AI & Society, 22 (4), pp. 463-475; Wallach, W., Robot minds and human ethics: The need for a comprehensive model of moral decision making (2010) Ethics and Information Technology, 12 (3), pp. 243-250; Wallach, W., Allen, C., (2010) Moral Machines: Teaching Robots Right from Wrong, , https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975, New York, Oxford University Press, Retrieved from, Accessed 10 Feb 2017; Wiegel, V., Building blocks for artificial moral agents (2006) Proceedings of Ethicalalife06 Workshop, , https://www.researchgate.net/profile/Vincent_Wiegel/publication/228615030_Building_blocks_for_artificial_moral_agents/links/55fabe5708aeafc8ac3fe6f8/Buildingblocks-for-artificial-moral-agents.pdf, Retrieved 12 Feb 2018; Wiegel, V., Wendell Wallach and Colin Allen: Moral machines: Teaching robots right from wrong (2010) Ethics and Information Technology, 12 (4), pp. 359-361},
correspondence_address1={van Wynsberghe, A.; Technical University of Delft, Jaffalaan 5, Netherlands; email: aimeevanrobot@gmail.com},
editor={NA},
publisher={Springer Netherlands},
issn={13533452},
isbn={NA},
language={English},
abbrev_source_title={Sci. Eng. Ethics},
document_type={Article},
source={Scopus},
number={3},
art_number={NA},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme,Â H2020},
coden={NA},
pubmed_id={29460081},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Hunyadi201959,
type={ARTICLE},
author={Hunyadi, M.},
title={Artificial moral agents. Really?},
journal={Springer Tracts in Advanced Robotics},
year={2019},
volume={130},
pages={59-69},
doi={10.1007/978-3-030-17974-8_5},
note={cited By 2; Conference of 4th Workshop of Anthropomorphic Motion Factory, 2017 ; Conference Date: 30 November 2017 Through 1 December 2017;  Conference Code:229709},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070742050&doi=10.1007%2f978-3-030-17974-8_5&partnerID=40&md5=5440bb7082a0ce0aa86dd8ef5d6c50d6},
affiliation={UniversitÃ© Catholique de Louvain, 1 Place de lâ€™UniversitÃ©, 1348, Ottignies-Louvain-la-Neuve, Belgium},
abstract={How can we plausibly refer to robots as artificial moral agents? Considering the useful classification of the philosopher of the field of artificial intelligence James H. Moor, who identified four different kinds of ethical, I will argue that the term of artificial moral agent is philosophically illegitimate. My argumentation is developed in three stages: the first stage addresses the actual choice of the ethical principles to be programmed into the machine; the second stage explores the difficulties inherent in giving these principles an algorithmic form; and the third focuses on the supreme difficulty arising from the very nature of moral reasoning. This analysis aims at encouraging the research on the concepts of moral reasoning and judgement. Indeed, a fine understanding of these notions should reveal the full extent of the problem with artificial moral agents; before we can discuss machine ethics or artificial ethics, we must, if we are to avoid speculation and ideology, have a clear understanding of what ethics is, what type of rationality it implements, and what is the nature of ethics and ethical conduct in general. Â© Springer Nature Switzerland AG 2019.},
author_keywords={Agent;  Artificial;  Ethics;  Moore;  Moral},
keywords={Agents;  Behavioral research;  Mooring;  Robotics, Artificial;  Ethical principles;  Ethics;  Moral;  Moral agents;  Moral reasoning, Philosophical aspects},
references={Moor, J.H., The nature, importance, and difficulty of machine ethics (2006) IEEE Intell. Syst., 21 (4), pp. 18-21; Moor, J.H., Four kinds of ethical robots (2009) Philosophy Now, 72, pp. 12-14; Wallach, W., Allen, C., (2009) Moral Machines: Teaching Robots Right from Wrong, , Oxford University Press, Oxford; Anderson, M., Anderson, S., Machine ethics: Creating an ethical intelligent agent (2007) AI Mag, 28 (4), pp. 15-26; Hunyadi, M., (2012) : Lâ€™Homme En Contexte, , Cerf, Paris; Laumond, J.-P., Interview; La mÃ©thode Scientifique, , France Culture radio, 14 June 2017},
correspondence_address1={Hunyadi, M.; UniversitÃ© Catholique de Louvain, 1 Place de lâ€™UniversitÃ©, 1348, Belgium; email: mark.hunyadi@uclouvain.be},
editor={Laumond J.-P., Pieters C., Danblon E.},
publisher={Springer Verlag},
issn={16107438},
isbn={9783030179731},
language={English},
abbrev_source_title={Springer Tracts Adv. Rob.},
document_type={Conference Paper},
source={Scopus},
number={NA},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Kim2018197,
type={CONFERENCE},
author={Kim, R. and Kleiman-Weiner, M. and Abeliuk, A. and Awad, E. and Dsouza, S. and Tenenbaum, J.B. and Rahwan, I.},
title={A Computational Model of Commonsense Moral Decision Making},
journal={AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
year={2018},
volume={NA},
pages={197-203},
doi={10.1145/3278721.3278770},
note={cited By 11; Conference of 1st AAAI/ACM Conference on AI, Ethics, and Society, AIES 2018 ; Conference Date: 2 February 2018 Through 3 February 2018;  Conference Code:144126},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061032466&doi=10.1145%2f3278721.3278770&partnerID=40&md5=1192c02592b3d7814fa82104192a599c},
affiliation={Massachusetts Institute of Technology, Cambridge, MA, United States},
abstract={We introduce a computational model for building moral autonomous vehicles by learning and generalizing from human moral judgments. We draw on a cognitively inspired model of how people and young children learn moral theories from sparse and noisy data and integrate observations made from different people in different groups. The problem of moral learning for autonomous vehicles is cast as learning how to weigh the different features of the dilemma using utility calculus, with the goal of making these trade-offs reflect how people make them in a wide variety of moral dilemma. By modeling the structures of individuals and groups in a hierarchical Bayesian model, we show that an individual's moral values - as well as a group's shared values - can be inferred from sparse and noisy data. We evaluate our approach with data from the Moral Machine, a web application that collects human judgments on moral dilemmas involving autonomous vehicles, and show that the model rapidly and accurately infers people's preferences and can predict the difficulty of moral dilemmas from limited data. Â© 2018 ACM.},
author_keywords={artificial intelligence;  Bayesian inference;  machine ethics;  moral learning},
keywords={Artificial intelligence;  Autonomous vehicles;  Bayesian networks;  Behavioral research;  Calculations;  Computational methods;  Decision making;  Economic; social effects;  Inference engines;  Philosophical aspects, Bayesian inference;  Computational model;  Hierarchical Bayesian modeling;  Human judgments;  Moral judgment;  moral learning;  WEB application;  Young children, Computation theory},
references={Baron, J., GÃ¼rÃ§ay, B., A meta-analysis of response-time tests of the sequential two-systems model of moral judgment (2017) Memory & Cognition, 45 (4), pp. 566-575. , https://doi.org/10.3758/s13421-016-0686-8, (5 2017); Bentham, J., (1789) An Introduction to the Principles of Morals and Legislation, , https://doi.org/10.1111/j.2048-416X.2000.tb00070.x; Blake, P.R., McAuliffe, K., Corbit, J., Callaghan, T.C., Barry, O., Bowie, A., Kleutsch, L., Warneken, F., The ontogeny of fairness in seven societies (2015) Nature, 528 (7581), pp. 258-261. , https://doi.org/10.1038/nature15703, (2015); Bonnefon, J., Shariff, A., Rahwan, I., The social dilemma of autonomous vehicles (2016) Science, 352 (6293). , http://science.sciencemag.org/content/352/6293/1573.abstract, (6 2016), 1573 LP - 1576; Cain, N., Shea-Brown, E., (2012) Computational Models of Decision Making: Integration, Stability, and Noise, , https://doi.org/10.1016/j.conb.2012.04.013; Susan, C., (2009) The Origin of Concepts, 598p. , https://global.oup.com/academic/product/the-origin-of-concepts-9780199838806#.WfDc448zKVM.mendeley, Oxford University Press; Felbo, B., Mislove, A., SÃ¸gaard, A., Rahwan, I., Lehmann, S., Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm (2017) Conference on Empirical Methods in Natural Language Processing (EMNLP); Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin, D.B., (2013) Bayesian Data Analysis, Third Edition, , https://books.google.com/books?id=ZXL6AQAAQBAJ, Taylor & Francis; Goodman, B., Flaxman, S., (2016) European Union Regulations on Algorithmic Decision-making and A "right to Explanation", , http://arxiv.org/abs/1606.08813, (6 2016); Gopnik, A., Meltzoff, A.N., (1997) Words, Thoughts, and Theories, 268, p. 268. , The MIT Press, Cambridge, MA, US., xvi, xvi pages; Graham, J., Haidt, J., Nosek, B.A., Liberals and conservatives rely on different sets of moral foundations (2009) Journal of Personality and Social Psychology, , https://doi.org/10.1037/a0015141, (2009); Henrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., Gintis, H., McElreath, R., In search of homo economicus: Behavioral experiments in 15 small-scale societies (2001) The American Economic Review, 91 (2), pp. 73-78. , http://www.jstor.org/stable/2677736, (2001); House, B.R., Silk, J.B., Henrich, J., Clark Barrett, H., Scelza, B.A., Boyette, A.H., Hewlett, B.S., Laurence, S., Ontogeny of prosocial behavior across diverse societies (2013) Proceedings of the National Academy of Sciences, 110 (36), pp. 14586-14591. , https://doi.org/10.1073/pnas.1221217110, (2013); Kleiman-Weiner, M., Saxe, R., Tenenbaum, J.B., Learning a commonsense moral theory (2017) Cognition, 167, pp. 107-123. , https://doi.org/10.1016/j.cognition.2017.03.005, (2017); Kohlberg, L., Essays in moral development (1981) The Philosophy of Moral Development; Lei, T., Barzilay, R., Jaakkola, T., (2016) Rationalizing Neural Predictions, , http://arxiv.org/abs/1606.04155, (6 2016); Lewandowski, D., Kurowicka, D., Joe, H., Generating random correlation matrices based on vines and extended onion method (2009) Journal of Multivariate Analysis, 100 (9), pp. 1989-2001. , https://doi.org/10.1016/j.jmva.2009.04.008, (2009); Mikhail, J., Universal moral grammar: Theory, evidence and the future (2007) Trends in Cognitive Sciences, 11 (4), pp. 143-152. , https://doi.org/10.1016/j.tics.2006.12.007, (2007); Mikhail, J., (2011) Elements of Moral Cognition, , https://doi.org/10.1017/CBO9780511780578, Cambridge University Press, Cambridge; Noothigattu, R., Gaikwad, S.N.S., Awad, E., Dsouza, S., Rahwan, I., Ravikumar, P., Procaccia, A.D., (2017) A Voting-Based System for Ethical Decision Making, , http://arxiv.org/abs/1709.06692, (9 2017); Van-Den-Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Kavukcuoglu, K., (2016) WaveNet: A Generative Model for Raw Audio, pp. 1-15. , http://arxiv.org/abs/1609.03499, (2016); Rai, P., Hal Daume, The infinite hierarchical factor regression model (2009) Advances in Neural Information Processing Systems 21, pp. 1321-1328. , http://arxiv.org/abs/0908.0570, (2009); Ratcliff, R., McKoon, G., The diffusion decision model: Theory and data for two-choice decision tasks (2008) Neural Computation, 20 (4), pp. 873-922. , https://doi.org/10.1162/neco.2008.12-06-420, (4 2008); Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., Lillicrap, T., (2016) One-shot Learning with Memory-Augmented Neural Networks, , http://arxiv.org/abs/1605.06065, (5 2016); Smith, P.L., Ratcliff, R., Psychology and neurobiology of simple decisions (2004) Trends in Neurosciences, , https://doi.org/10.1016/j.tins.2004.01.006, (2004); Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Rabinovich, A., Going Deeper with Convolutions, , [n. d.]. ([n. d.]); Tenenbaum, J.B., Kemp, C., Griffiths, T.L., Goodman, N.D., Howto growa mind: Statistics, structure, and abstraction (2011) Science, 331 (6022), p. 1279. , http://science.sciencemag.org/content/331/6022/1279.abstract, (3 2011); Ghahramani, Z., Griffiths, T.L., Infinite latent feature models and the Indian buffet process (2005) Advances in Neural Information Processing Systems 18, pp. 475-482. , http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.3951, (2005); Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., Wierstra, D., (2016) Matching Networks for One Shot Learning, , http://arxiv.org/abs/1606.04080, (6 2016); Wu, Y., Schuster, M., Chen, Z., Le, Q.V., Norouzi, M., Macherey, W., Krikun, M., Dean, J., (2016) Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation, , http://arxiv.org/abs/1609.08144, (9 2016)},
correspondence_address1={NA},
editor={NA},
publisher={Association for Computing Machinery, Inc},
issn={NA},
isbn={9781450360128},
language={English},
abbrev_source_title={AIES - Proc. AAAI/ACM Conf. AI, Ethics, Soc.},
document_type={Conference Paper},
source={Scopus},
number={NA},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={AAAI; ACM SIGAI},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Awad201859,
type={ARTICLE},
author={Awad, E. and Dsouza, S. and Kim, R. and Schulz, J. and Henrich, J. and Shariff, A. and Bonnefon, J.-F. and Rahwan, I.},
title={The Moral Machine experiment},
journal={Nature},
year={2018},
volume={563},
pages={59-64},
doi={10.1038/s41586-018-0637-6},
note={cited By 457},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055831155&doi=10.1038%2fs41586-018-0637-6&partnerID=40&md5=de8fa111ffddea46578f643a6f47529c},
affiliation={The Media Lab, Massachusetts Institute of Technology, Cambridge, MA, United States; Department of Human Evolutionary Biology, Harvard University, Cambridge, MA, United States; Department of Psychology, University of British Columbia, Vancouver, BC, Canada; Toulouse School of Economics (TSM-R), CNRS, UniversitÃ© Toulouse Capitole, Toulouse, France; Institute for Data, Systems & Society, Massachusetts Institute of Technology, Cambridge, MA, United States},
abstract={With the rapid development of artificial intelligence have come concerns about how machines will make moral decisions, and the major challenge of quantifying societal expectations about the ethical principles that should guide machine behaviour. To address this challenge, we deployed the Moral Machine, an online experimental platform designed to explore the moral dilemmas faced by autonomous vehicles. This platform gathered 40 million decisions in ten languages from millions of people in 233 countries and territories. Here we describe the results of this experiment. First, we summarize global moral preferences. Second, we document individual variations in preferences, based on respondentsâ€™ demographics. Third, we report cross-cultural ethical variation, and uncover three major clusters of countries. Fourth, we show that these differences correlate with modern institutions and deep cultural traits. We discuss how these preferences can contribute to developing global, socially acceptable principles for machine ethics. All data used in this article are publicly available. Â© 2018, Springer Nature Limited.},
author_keywords={NA},
keywords={adult;  article;  ethics;  human;  language;  machine;  morality;  artificial intelligence;  decision making;  ethics;  female;  harm reduction;  information processing;  international cooperation;  Internet;  male;  motor vehicle;  pedestrian;  procedures;  public opinion;  robotics;  traffic accident;  translating (language), Accidents, Traffic;  Artificial Intelligence;  Data Collection;  Decision Making;  Female;  Harm Reduction;  Humans;  Internationality;  Internet;  Male;  Morals;  Motor Vehicles;  Pedestrians;  Public Opinion;  Robotics;  Translating},
references={Greene, J., (2013) Moral Tribes: Emotion, Reason and the Gap between Us and Them, , Atlantic Books, London; Tomasello, M.A., (2014) Natural History of Human Thinking, , Harvard Univ. Press, Cambridge; Cushman, F., Young, L., The psychology of dilemmas and the philosophy of morality (2009) Ethical Theory Moral Pract., 12, pp. 9-24; Asimov, I.I., (1950) Robot, , Doubleday, New York; Bryson, J., Winfield, A., Standardizing ethical design for artificial intelligence and autonomous systems (2017) Computer, 50, pp. 116-119; Wiener, N., Some moral and technical consequences of automation (1960) Science, 131, pp. 1355-1358. , COI: 1:STN:280:DC%2BC3cvmtFSqtw%3D%3D; Wallach, W., Allen, C., (2008) Moral Machines: Teaching Robots Right from Wrong, , Oxford Univ. Press, Oxford; Responsible autonomy (2017) Proc. 26Th International Joint Conference on Artificial Intelligence 4698â€“4704 (IJCAI; Dadich, S., (2016) Barack Obama, Neural Nets, Self-Driving Cars, and the Future of the World, , https://www.wired.com/2016/10/president-obama-mit-joi-ito-interview/, Wired; Shariff, A., Bonnefon, J.-F., Rahwan, I., Psychological roadblocks to the adoption of self-driving vehicles (2017) Nat. Hum. Behav., 1, pp. 694-696; Conitzer, V., Brill, M., Freeman, R., Crowdsourcing societal tradeoffs (2015) Proc. 2015 International Conference on Autonomous Agents and Multiagent Systems, pp. 1213-1217. , IFAAMAS; Bonnefon, J.-F., Shariff, A., Rahwan, I., The social dilemma of autonomous vehicles (2016) Science, 352, pp. 1573-1576. , COI: 1:CAS:528:DC%2BC28XhtVaitLzK; Hauser, M., Cushman, F., Young, L., Jin, K.-X.R., Mikhail, J., A dissociation between moral judgments and justifications (2007) Mind Lang., 22, pp. 1-21; Carlsson, F., Daruvala, D., Jaldell, H., Preferences for lives, injuries, and age: a stated preference survey (2010) Accid. Anal. Prev., 42, pp. 1814-1821; Johansson-Stenman, O., Martinsson, P., Are some lives more valuable? An ethical preferences approach (2008) J. Health Econ., 27, pp. 739-752; Johansson-Stenman, O., Mahmud, M., Martinsson, P., Saving lives versus life-years in rural Bangladesh: an ethical preferences approach (2011) Health Econ., 20, pp. 723-736; Graham, J., Meindl, P., Beall, E., Johnson, K.M., Zhang, L., Cultural differences in moral judgment and behavior, across and within societies (2016) Current Opinion in Psychology, 8, pp. 125-130; Hainmueller, J., Hopkins, D.J., Yamamoto, T., Causal inference in conjoint analysis: understanding multidimensional choices via stated preference experiments (2014) Polit. Anal., 22, pp. 1-30; Luetge, C., The German Ethics Code for automated and connected driving (2017) Philos. Technol., 30, pp. 547-558; MÃ¼llner, D., (2011) Modern Hierarchical, Agglomerative Clustering Algorithms, , https://arxiv.org/abs/1109.2378, Preprint at; Inglehart, R., Welzel, C., (2005) Modernization, Cultural Change, and Democracy: The Human Development Sequence, , Cambridge Univ. Press, Cambridge; Muthukrishna, M., (2018) Beyond WEIRD Psychology: Measuring and Mapping Scales of Cultural and Psychological Distance, , https://ssrn.com/abstract=3259613, Preprint at; Hofstede, G., (2003) Cultureâ€™s Consequences: Comparing Values, Behaviors, Institutions and Organizations across Nations, , Sage, Thousand Oaks; (2017) World Economic Outlook Database, , https://www.imf.org/external/pubs/ft/weo/2017/01/weodata/index.aspx; Kaufmann, D., Kraay, A., Mastruzzi, M., The worldwide governance indicators: methodology and analytical issues (2011) Hague J. Rule Law, 3, pp. 220-246; GÃ¤chter, S., Schulz, J.F., Intrinsic honesty and the prevalence of rule violations across societies (2016) Nature, 531, pp. 496-499; Oâ€™Neil, C., (2016) Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy, , Penguin, London; Henrich, J., In search of Homo Economicus: behavioral experiments in 15 small-scale societies (2001) Am. Econ. Rev., 91, pp. 73-78; (2017) Future of Life Institute. Asilomar AI Principles, , https://futureoflife.org/ai-principles/; Haidt, J., (2012) The Righteous Mind: Why Good People are Divided by Politics and Religion, , Knopf Doubleday, New York; Gastil, J., Braman, D., Kahan, D., Slovic, P., The cultural orientation of mass political opinion (2011) PS Polit. Sci. Polit., 44, pp. 711-714; Nishi, A., Christakis, N.A., Rand, D.G., Cooperation, decision time, and culture: online experiments with American and Indian participants (2017) PLoS One, 12},
correspondence_address1={Rahwan, I.; The Media Lab, United States; email: irahwan@mit.edu},
editor={NA},
publisher={Nature Publishing Group},
issn={00280836},
isbn={NA},
language={English},
abbrev_source_title={Nature},
document_type={Article},
source={Scopus},
number={7729},
art_number={NA},
funding_details={NA},
coden={NATUA},
pubmed_id={30356211},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Bendel2018204,
type={ARTICLE},
author={Bendel, O.},
title={Towards animal-friendly machines},
journal={Paladyn},
year={2018},
volume={9},
pages={204-213},
doi={10.1515/pjbr-2018-0019},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053273786&doi=10.1515%2fpjbr-2018-0019&partnerID=40&md5=95b4e53307e47a72891fd4e339993ced},
affiliation={School of Business FHNW, Bahnhofstrasse 6, Windisch, Aargau, CH-5210, Switzerland},
abstract={Semi-Autonomous machines, autonomous machines and robots inhabit closed, semi-closed and open environments. There they encounter domestic animals, farm animals, working animals and/or wild animals. These animals could be disturbed, displaced, injured or killed. Within the context of machine ethics, the School of Business FHNW developed several design studies and prototypes for animal-friendly machines,which can be understood as moral machines in the spirit of this discipline. They were each linked with an annotated decision tree containing the ethical assumptions or justifications for interactions with animals. Annotated decision trees are seen as an important basis in developing moral machines. They are not without problems and contradictions, but they do guarantee well-founded, secure actions that are repeated at a certain level. This article documents completed and current projects, compares their relative risks and benefits, and makes proposals for future developments in machine ethics. The findings in this article and proposals for the future hope to systemically promote animal well-being and prevent animal suffering in encounters between machines and animals. Â© by Oliver Bendel, published by De Gruyter 2018.},
author_keywords={animal ethics;  animal welfare;  annotated decision trees;  artificial intelligence;  decision trees;  machine ethics;  roboethics;  robotics},
keywords={NA},
references={Azad-Manjiri, M., A new architecture for making moral agents based on C4.5 decision tree algorithm (2014) International Journal of Information Technology and Computer Science (IJITCS), 6 (5), pp. 50-57; Bendel, O., Einfache moralische Maschinen: Vom Design zur Konzeption (2015) Prozesse, Technologie, Anwendungen, Systeme und Management, Mana-Buch, pp. 171-180. , T. Barton, B. Erdlenbruch, F. Herrmann et al. (Eds); Bendel, O., Considerations about the relationship between animal and machine ethics (2016) AI & SOCIETY, 31 (1), pp. 103-108; Bendel, O., Advanced driver assistance systems and animals (2014) KÃ¶nstliche Intelligenz, 28 (4), pp. 263-269; Mancini, C., Animal-Computer Interaction (ACI) a manifest (2011) Interactions, 18 (4), pp. 69-73; Anderson, M., Anderson, S.L., (2011) Machine Ethics Cambridge, , University Press; Bendel, O., Wirtschaftliche und technische Implikationen der Maschinenethik (2014) Die Betriebswirtschaft, 4, pp. 237-248; Bendel, O., (2012) Maschinenethik Gabler Wirtschaftslexikon, , http://wirtschaftslexikon.gabler.de/Definition/maschinenethik.html, Springer Gabler; Bendel, O., (2014) Tierethik Gabler Wirtschaftslexikon, , http://wirtschaftslexikon.gabler.de/Definition/tierethik.html, Springer Gabler; Donaldson, S., Kymlicka, W., (2011) Zoopolis: A Political Theory of Animal Rights, , Oxford University Press; Singer, P., (2011) Practical Ethics Third Edition, , Cambridge University Press; Wolf, U., (2012) Ethik der Mensch-Tier-Beziehung Klostermann; Fossgreen, A., (2017) Bei Den KÃ¶hen Piepts, , https://www.tagesanzeiger.ch/wissen/bei-denkuehen-piepts/story/26203583, Tages-Anzeiger, September 9; Eichler, S., (2013) Sitzen Statt BÃ¶cken DRadio Wissen, , http://www.dradiowissen.de/erdbeerernte-sitzen-stattbuecken35.de.html?dram:article_id=239972, March 13; Pluta, W., (2017) Roboterwolf Vertreibt Wildschweine, , https://www.golem.de/news/landwirtschaftroboterwolf-vertreibt-wildschweine-1708-129719.html, Golem August 28; Bendel, O., Die Roboter sind unter uns (2014) Netzwoche, 22, p. 28; Mondada, F., Halloy, J., Martinoli, A., A general methodology for the control of mixed natural-Artificial societies (2013) Handbook of Collective Robotics: Fundamentals and Challenges, , S. Kernbach (Ed) Taylor & Francis; Bendel, O., Annotated Decision Trees for Simple Moralmachines, the 2016 AAAI Spring Symposium Series, AAAI Press, pp. 195-201; Kolhagen, J., Autopiloten auf RÃ¤dern (2013) Versicherungswirtschaft, 11 (70). , June 1; Stoller, D., Vollautomatisch und ohne Fahrer in der Stadt unterwegs (2013) Ingenieur.De, , http://www.ingenieur.de/Themen/Automobil/Vollautomatisch-Fahrer-in-Stadt-unterwegs, July 15; Bendel, O., Fahrerassistenzsysteme aus ethischer Sicht (2014) Zeitschrift fÃ¶r Verkehrssicherheit, 2, pp. 108-110; Bendel, O., (2013) Ich Bremse Auch fÃ¶r Tiere: Ãœberlegungen zu Einfachen MoralischenMaschinen Inside-it.ch, , http://www.insideit.ch/articles/34646, December 4; Kopf, M., (1994) Ein Beitrag Zur Modellbasierten Adaptiven FahrerunterstÃ¶tzung fÃ¶r das Fahren Auf Deutschen Autobahnen, , PhD thesis VDI-Verlag; Lorenz, L.M., (2014) Entwicklung und Bewertung Aufmerksamkeitslenkender Warn-und Informationskonzepte fÃ¶r Fahrerassistenzsysteme: Aufmerksamkeitssteuerung in der FrÃ¶hen Phase Kritischer Verkehrssituationen, , PhD thesis, TU MÃ¶nchen, MÃ¶nchen; Pluta, W., (2017) EU-Projekt Timon Vernetzt FuÃŸgÃ¤nger, Autofahrer und Radler, Golem, , https://www.golem.de/news/verkehrssicherheit-eu-projekt-Timon-vernetztfussgaenger-Autofahrer-und-radler-1709-129669.html, September 1; Bendel, O., LADYBIRD the Animal-friendly Robot Vacuumcleaner, pp. 2-6. , The 2017 AAAI Spring Symposium Series, AAAI Press; Hueber, J., (2013) Wir Sehen Farbe-das KÃ¶nnen Sensoren Auch! SENSOR MAGAZIN, 1, pp. 8-11; Brisevac, A., Calcagno, S., Stierli, B., (2017) LADYBIRD Projektdokumentation, , School of Business FHNW; Bendel, O., (2018) Das LADYBIRD-Projekt Handbuch Maschinenethik, , O. Bendel (Ed) Springer Reference Geisteswissenschaften, Springer VS; Bendel, O., Service robots in public spaces (2017) Telepolis, , https://www.heise.de/tp/features/Service-Robotsin-Public-Spaces-3754173.html, June 25; Zhou, N., Volvo admits its self-driving cars are confused by kangaroos (2017) The Guardian, , https://www.theguardian.com/technology/2017/jul/01/volvoadmits-its-self-driving-cars-Are-confused-by-kangaroos, July 1; Wimmer, T., Israel, M., Haschberger, P., Rehkitzrettung mit dem Fliegenden Wildretter: Erfahrungen der ersten FeldeinsÃ¤tze (2013) Bornimer Agrartechnische Berichte, 81, pp. 85-95; Binder, U., (2017) Der Bauer Wird Digitalisiert, , https://www.swisscom.ch/de/storys/technologie/smarte-landwirtschaft.html, Swisscom October 11; Federle, S., Radar soll ZugvÃ¶gel schÃ¶tzen Tierwelt, (10), pp. 22-23. , March 5 2014},
correspondence_address1={Bendel, O.; School of Business FHNW, Bahnhofstrasse 6, Switzerland; email: oliver.bendel@fhnw.ch},
editor={NA},
publisher={De Gruyter},
issn={20814836},
isbn={NA},
language={English},
abbrev_source_title={Paladyn},
document_type={Article},
source={Scopus},
number={1},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Boyles2018182,
type={ARTICLE},
author={Boyles, R.J.M.},
title={A case for machine ethics in modeling human-level intelligent agents},
journal={Kritike},
year={2018},
volume={12},
pages={182-200},
doi={10.25138/12.1.a9},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049387382&doi=10.25138%2f12.1.a9&partnerID=40&md5=b8e70ec30555d4d99c07ba78428bc2a8},
affiliation={Department of Philosophy, De La Salle University, Philippines},
abstract={This paper focuses on the research field of machine ethics and how it relates to a technological singularity-a hypothesized, futuristic event where artificial machines will have greater-than-human-level intelligence. One problem related to the singularity centers on the issue of whether human values and norms would survive such an event. To somehow ensure this, a number of artificial intelligence researchers have opted to focus on the development of artificial moral agents, which refers to machines capable of moral reasoning, judgment, and decision-making. To date, different frameworks on how to arrive at these agents have been put forward. However, there seems to be no hard consensus as to which framework would likely yield a positive result. With the body of work that they have contributed in the study of moral agency, philosophers may contribute to the growing literature on artificial moral agency. While doing so, they could also think about how the said concept could affect other important philosophical concepts. Â© 2018 Robert James M. Boyles.},
author_keywords={Artificial moral agents;  Machine ethics;  Philosophy of artificial intelligence;  Technological singularity},
keywords={NA},
references={Anderson, M., Anderson, S.L., (2011) Machine Ethics, , New York: Cambridge University Press; Aquinas, T., (1952) Summa Theologica, , Chicago: Encyclopaedia Britannica, trans. by the Fathers of the English Dominican Province, rev. by Daniel J. Sullivan; Capek, K.R.U.R., (2016), Adelaide: eBooks@Adelaide, University of Adelaide Library trans. by David Wyllie; Carter, M., (2007) Minds and Computers: An Introduction to the Philosophy of Artificial Intelligence, , Edinburgh: Edinburgh University Press, Ltd; Chalmers, D.J., "The Singularity: A Philosophical Analysis," (2010) Journal of Consciousness Studies, 17, pp. 9-10; Counet, J.-M., "Mathematics and the Divine in Nicholas of Cusa," (2005) Mathematics and the Divine: A Historical Study, , ed. by Teun Koetsier and Luc Bergmans Amsterdam: Elsevier B.V; Denis, L., "Kant and Hume on Morality," The Stanford Encyclopedia of Philosophy, , https://plato.stanford.edu/archives/fall2012/entries/kant-hume-morality/, Fall 2012 ed., ed. by Edward N. Zalta (7 December 2015); Denise, T.C., (2008) Great Traditions in Ethics, , California: Thomson Wadsworth; Eshleman, A., "Moral Responsibility," The Stanford Encyclopedia of Philosophy, , https://plato.stanford.edu/archives/sum2014/entries/moral-responsibility/, Summer 2014 ed., ed. by Edward N. Zalta (June 2015); Garrett, B., (1998) Personal Identity and Self-Consciousness, , London: Routledge; Goertzel, B., "Human-level Artificial General Intelligence and the Possibility of a Technological Singularity: A Reaction to Ray Kurzweil's The Singularity is Near, and McDermott's Critique of Kurzweil," (2007) Artificial Intelligence, 171, p. 18; Good, I.J., "Speculations Concerning the First Ultraintelligent Machine," (1966) Advances in Computers, 6. , ed. by Franz L. Alt and Morris Rubinoff (New York: Academic Press); Graesser, A., "Is it an Agent, or just a Program?: A Taxonomy for Autonomous Agents," (1996) Proceedings of the Third International Workshop on Agent Theories, Architectures, and Languages, , (London: Springer-Verlag); Greene, B., (1999) The Elegant Universe: Superstrings, Hidden Dimensions, and the Quest for the Ultimate Theory, , New York: W.W. Norton & Co; Hawking, S.W., (1988) A Brief History of Time, , New York: Bantam Books; Himma, K.E., "Artificial Agency, Consciousness, and the Criteria for Moral Agency: What Properties must an Artificial Agent have to be a Moral Agent?" (2009) Ethics and Information Technology, p. 11; Joaquin, J.J., "Personal Identity and What Matters," (2017) Organon F, 24, p. 2; Kant, I., (2002) Groundwork for the Metaphysic of Morals, , ed. and trans. by Allen W. Wood (Yale University Press); Kurzweil, R., (2005) The Singularity is Near: When Humans Transcend Biology, , New York: Viking; Loosemore, R., Goertzel, B., "Why an Intelligence Explosion is Probable,", , http://hplusmagazine.com/2011/03/07/why-an-intelligence-explosion-is-probable/, in Humanity+ Magazine (7 March 2011); Mabaquiao, N.J., (2012) Mind, Science and Computation, , Manila: Vibal Publishing House, Inc; McInerny, R., "Aquinas's Moral Theory," (1987) in Journal of Medical Ethics, 13 (1); Moor, J.H., "The Nature, Importance, and Difficulty of Machine Ethics," (2011) Machine Ethics, , ed. by Michael Anderson and Susan Leigh Anderson New York: Cambridge University Press; Moravec, H.P., "Rise of the Robots," (2002) Understanding Artificial Intelligence, , ed. by Sandy Fritz New York: Warner Books, Inc; Moser, W., Moyes, C., "Literature-A Storehouse of Knowledge?" (1993) SubStance, p. 22; Muehlhauser, L., Bostrom, N., "Why We Need Friendly AI," (2014) in Think, 36 (13); Nadeau, J.E., "Only Androids Can Be Ethical," (2006) Thinking about Android Epistemology, , ed. by Kenneth M. Ford, Clark Glymour and Patrick Hayes Cambridge: MIT Press; Paipetis, S.A., (2010) The Unknown Technology in Homer, , Dordrecht: Springer Science+Business Media BV; Penrose, R., "Black Holes," (1991) The World Treasury of Physics, Astronomy and Mathematics, , ed. by Timothy Ferris New York: Little, Brown and Company; Shanahan, M., "The Frame Problem," The Stanford Encyclopedia of Philosophy, , https://plato.stanford.edu/archives/win2009/entries/frame-problem/, Winter 2009 ed., ed. by Edward N. Zalta (July 2013); Sullins, J., "Artificial Moral Agency in Technoethics," (2009) Handbook of Research on Technoethics, , ed. by Roccio Luppicini and Rebecca Adell Hershey: IGI Global Information Science; Pfeifer, R., Scheier, C., (1999) Understanding Intelligence, , Cambridge: MIT Press; Vinge, V., "The Coming of Technological Singularity: How to Survive in the Post-Human Era," (1993) Proceedings of A Symposium Cosponsored by NASA Lewis Research Center and the Ohio Aerospace Institute, , in Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace (Washington, D.C.: National Aeronautics and Space Administration, Office of Management, Scientific and Technical Information Program); Wallach, W., Allen, C., (2009) Moral Machines: Teaching Robots Right from Wrong, , New York: Oxford University Press; Wittgenstein, L., (1953) Philosophical Investigations, , Oxford: Blackwell Publishing, Ltd},
correspondence_address1={Boyles, R.J.M.; Department of Philosophy, Philippines},
editor={NA},
publisher={University of Santo Tomas - Department of Philosophy},
issn={19087330},
isbn={NA},
language={English},
abbrev_source_title={Kritike},
document_type={Article},
source={Scopus},
number={1},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Bogosian2017591,
type={ARTICLE},
author={Bogosian, K.},
title={Implementation of Moral Uncertainty in Intelligent Machines},
journal={Minds and Machines},
year={2017},
volume={27},
pages={591-608},
doi={10.1007/s11023-017-9448-z},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035795763&doi=10.1007%2fs11023-017-9448-z&partnerID=40&md5=e8cad5f53c8dee5944079776d9ba8f5a},
affiliation={Tulane University, New Orleans, LA, United States},
abstract={The development of artificial intelligence will require systems of ethical decision making to be adapted for automatic computation. However, projects to implement moral reasoning in artificial moral agents so far have failed to satisfactorily address the widespread disagreement between competing approaches to moral philosophy. In this paper I argue that the proper response to this situation is to design machines to be fundamentally uncertain about morality. I describe a computational framework for doing so and show that it efficiently resolves common obstacles to the implementation of moral philosophy in intelligent machines. Â© 2017, Springer Science+Business Media B.V.},
author_keywords={AI ethics;  Bottom-up;  Ethical trade;  Ethical trading;  Macaskill;  Machine ethics;  Machine intelligence;  Metaethics;  Metanormative theory;  Metanormativity;  Moral disagreement;  Moral divergence;  Moral trade;  Moral trading;  Moral uncertainty;  Moral voting;  Normative uncertainty;  Top-down;  Value alignment;  Value differences;  Value specification},
keywords={Artificial intelligence;  Behavioral research;  Commerce;  Computation theory;  Decision making, Bottom up;  Ethical trading;  Macaskill;  Machine intelligence;  Metaethics;  Metanormative theory;  Metanormativity;  Moral disagreement;  Moral divergence;  Moral trading;  Moral uncertainty;  Moral voting;  Normative uncertainty;  Topdown;  Value differences, Philosophical aspects},
references={Allen, C., Smit, I., Wallach, W., Artificial morality: Top-down, bottom-up, and hybrid approaches (2005) Ethics and Information Technology; Archard, D., Why moral philosophers are not and should not be moral experts (2011) Bioethics, 25 (3), pp. 119-127; Arkoudas, K., Bringsjord, S., Bello, P., Toward ethical robots via mechanized deontic logic (2005) Machine ethics: Papers from the 2005 AAAI fall symposium, pp. 17-23. , http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Toward+ethical+robots+via+mechanized+deontic+logic#0, Retrieved from, In; Bello, P., Bringsjord, S., On how to build a moral machine (2012) Topoi, 32 (2), pp. 251-266; Bostrom, N., (2009) Moral uncertaintyâ€”towards a solution?, , http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html; Bostrom, N., The superintelligent will: Motivation and instrumental rationality in advanced artificial agents (2012) Minds and Machines, 22 (2), pp. 71-85; Bostrom, N., (2014) Superintelligence: Paths, dangers, strategies, , Oxford University Press, Oxford; Brundage, M., Limitations and risks of machine ethics (2014) Journal of Experimental & Theoretical Artificial Intelligence, 26 (3), pp. 355-372; Cotton-Barratt, O., (2013) Geometric reasons for normalising variance to aggregate preferences, , http://users.ox.ac.uk/~ball1714/Variance%20normalisation.pdf; Cross, B., Moral philosophy, moral expertise, and the argument from disagreement (2016) Bioethics, 30 (3), pp. 188-194; Driver, J., Moral expertise: Judgement, practice, and analysis (2014) Social Philosophy and Policy, 30 (1-2), pp. 280-296; Gloor, L., Suffering-focused AI safety: Why â€œfail-safeâ€ measures might be our top intervention (2016) Foundational Research Institute, Report FRI-, pp. 11-16. , https://foundational-research.org/files/suffering-focused-ai-safety.pdf; Greene, J.D., Sommerville, R.B., Nystrom, L., Darley, J., Cohen, J., An fMRI investigation of emotional engagement in moral judgment (2001) Science, 293 (5537), pp. 2105-2108; Jones, K., Schroeter, F., Moral expertise (2012) Analyse & Kritik, 34 (2), pp. 217-230; Lockhart, T., (2000) Moral uncertainty and its consequences, , Oxford University Press, Oxford; MacAskill, W., The infectiousness of nihilism (2013) Ethics, 123 (3), pp. 508-520; MacAskill, W., (2014) Normative uncertainty, , http://commonsenseatheism.com/wp-content/uploads/2014/03/MacAskill-Normative-Uncertainty.pdf; MacAskill, W., Normative uncertainty as a voting problem (2016) Mind, 125 (500), pp. 967-1004; Nissan-Rozen, I., Against moral hedging (2015) Economics and Philosophy, 3, pp. 1-21; Oesterheld, C., Formalizing preference utilitarianism in physical world models (2015) Synthese; Oesterheld, C., Backup utility functions as a fail-safe AI technique (2016) Foundational Research Institute, Report FRI, pp. 12-16. , https://foundational-research.org/files/backup-utility-functions.pdf; (2009) Preliminary survey results, , http://philpapers.org/surveys/results.pl; Schultz, E., Cokely, E.T., Feltz, A., Persistent bias in expert judgments about free will and moral responsibility: A test of the expertise defense (2011) Consciousness and Cognition, 20 (4), pp. 1722-1731; Schwitzgebel, E., Cushman, F., Expertise in moral reasoning? Order effects on moral judgment in professional philosophers and non-philosophers (2012) Mind and Language, 27 (2), pp. 135-153; Shulman, C., Tarleton, N., Jonsson, H., Which consequentialism? Machine ethics and moral divergence (2009) In AP-CAP 2009: The 5th Asia-Pacific computing and philosophy conference, , https://intelligence.org/files/WhichConsequentialism.pdf; Williams, E., The possibility of an ongoing moral catastrophe (2015) Ethical Theory and Moral Practice, 18 (5), pp. 971-982; Wiltshire, T.J., A prospective framework for the design of ideal artificial moral agents: Insights from the science of heroism in humans (2015) Minds and Machines; Å»uradzki, T., (2016) Meta-reasoning in making moral decisions under normative uncertainty. In Argumentation and reasoned action: Proceedings of the 1st European conference on argumentation, Lisbon, 2015 (Vol. 2, pp. 1093â€“1104)},
correspondence_address1={Bogosian, K.; Tulane UniversityUnited States; email: kbogosia@tulane.edu},
editor={NA},
publisher={Springer Netherlands},
issn={09246495},
isbn={NA},
language={English},
abbrev_source_title={Minds Mach},
document_type={Article},
source={Scopus},
number={4},
art_number={NA},
funding_details={NA},
coden={MMACE},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Welsh20171,
type={BOOK},
author={Welsh, S.},
title={Ethics and security automata: Policy and technical challenges of the robotic use of force},
journal={Ethics and Security Automata: Policy and Technical Challenges of the Robotic Use of Force},
year={2017},
volume={NA},
pages={1-220},
doi={10.4324/9781315168951},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042004918&doi=10.4324%2f9781315168951&partnerID=40&md5=9b0196c566efdee8f0df4f99b096c165},
affiliation={NA},
abstract={Can security automata (robots and AIs) make moral decisions to apply force on humans correctly? If they can make such decisions, ought they be used to do so? Will security automata increase or decrease aggregate risk to humans? What regulation is appropriate? Addressing these important issues this book examines the political and technical challenges of the robotic use of force. The book presents accessible practical examples of the 'machine ethics' technology likely to be installed in military and police robots and also in civilian robots with everyday security functions such as childcare. By examining how machines can pass 'reasonable person' tests to demonstrate measurable levels of moral competence and display the ability to determine the 'spirit' as well as the 'letter of the law', the author builds upon existing research to define conditions under which robotic force can and ought to be used to enhance human security. The scope of the book is thus far broader than 'shoot to kill' decisions by autonomous weapons, and should attract readers from the fields of ethics, politics, and legal, military and international affairs. Researchers in artificial intelligence and robotics will also find it useful. Â© 2018 Sean Welsh. All rights reserved.},
author_keywords={NA},
keywords={NA},
references={Allen, C., Varner, A., Zinser, J., Prolegomena to Any Future Artificial Moral Agent (2000) Journal of Experimental and Theoretical Artificial Intelligence, 12 (3), pp. 251-261; Arkin, R.C., (2009) Governing Lethal Behaviour in Autonomous Robots, , Boca Rouge, CRC Press; Arkin, R.C., Ulam, P.D., An Ethical Adaptor: Behavioral Modification Derived from Moral Emotions (2009), http://hdl.handle.net/1853/31469, Retrieved 23rd Oct., 2014; Asimov, I., (1950) I, Robot, , New York, Gnome Press; Baars, B.J., (1997) In the Theatre of Consciousness: The Workspace of the Mind, , New York, Oxford University Press; Beck, K., (2003) Test-Driven Development: By Example, , Boston, MA, Addison-Wesley; Bekey, G.A., (2005) Autonomous Robots: From Biological Inspiration to Implementation and Control, , Cambridge, MA, MIT press; Bentham, J., An Introduction to the Principles of Morals and Legislation (1780), www.econlib.org/library/Bentham/bnthPML.html, Retrieved 8th Oct., 2016; Block, N., On a Confusion about a Function of Consciousness (1995) Behavioral and Brain Sciences, 18 (2), pp. 227-247; Boltuc, P., The Engineering Thesis in Machine Consciousness (2012) TechnÃ©: Research in Philosophy and Technology, 16 (2), pp. 187-207; Briggs, G., Scheutz, M., Sorry, I Can't Do That: Developing Mechanisms to Appropriately Reject Directives in Human-Robot Interactions (2015) Proceedings of the 2015 AAAI fall symposium on AI and HRI, , Washington, DC; Chalmers, D., Facing Up to the Problem of Consciousness (1995) Journal of Consciousness Studies, 2 (3), pp. 200-219; Chein, M., Mugnier, M.-L., (2008) Graph-Based Knowledge Representation: Computational Foundations of Conceptual Graphs, , London, Springer-Verlag; Chomsky, N., (1965) Aspects of the Theory of Syntax, , Cambridge, MA, MIT press; Croitoru, M., Oren, N., Miles, S., Luck, M., Graphical Norms via Conceptual Graphs (2012) Knowledge-Based Systems, 29, pp. 31-43; Damasio, A., (2010) Self Comes to Mind: Constructing the Conscious Brain, , New York, Pantheon; Patriot System Performance (2005), www.acq.osd.mil/dsb/reports/ADA435837.pdf, Retrieved 18th Feb., 2015; Principles of Robotics (2010), www.epsrc.ac.uk/research/ourportfolio/themes/engineering/activities/principlesofrobotics/, Retrieved 19th Jan., 2017; Flammini, F., Setola, R., Franceschetti, G., (2013) Effective Surveillance for Homeland Security: Balancing Technology and Social Issues, , Hoboken, Taylor and Francis; Gabbay, D., Horty, J., Parent, X., van der Mayden, R., van der Torre, L., (2013) Handbook of Deontic Logic and Normative Systems, , Milton Keynes, College Publications; Galliott, J., Responsibility and War Machines: Towards a Forward-Looking and Functional Account (2015) Rethinking Machine Ethics in the Age of Ubiquitous Technology, pp. 152-165. , J. White and R. Searle. Hershey, PA, IGI Global; Goldberg, E., (2009) The New Executive Brain: Frontal Lobes in a Complex World, , Oxford; New York, Oxford University Press; Grice, P., (1991) Studies in the Way of Words, , Cambridge, MA, Harvard University Press; Guarini, M., Computational Neural Modeling and the Philosophy of Ethics (2011) Machine Ethics, pp. 316-334. , M. Anderson and S. L. Anderson. Cambridge, Cambridge University Press; Gunkel, D.J., (2012) The Machine Question: Critical Perspectives on AI, Robots, and Ethics, , Cambridge, MA, MIT Press; Haidt, J., (2012) The Righteous Mind, , New York, Pantheon Books; Harnad, S., The Symbol Grounding Problem (1990) Physica D: Nonlinear Phenomena, 42 (1), pp. 335-346; Hauser, M.D., (2006) Moral Minds: How Nature Designed Our Universal Sense of Right and Wrong, , New York, HarperCollins; Heider, F., Simmel, M., An Experimental Study of Apparent Behavior (1944) The American Journal of Psychology, 57 (2), pp. 243-259; Hursthouse, R., (1999) On Virtue Ethics, , Oxford, Oxford University Press; Husserl, E., (1931) Ideas: General Introduction to Pure Phenomenology, , London, Allen & Unwin; Kant, I., Groundwork of the Metaphysics of Morals (1785), www.gutenberg.org/ebooks/5682, Retrieved 29th Nov., 2015; Korsgaard, C.M., (2009) Self-Constitution: Agency, Identity, and Integrity, , Oxford; New York, Oxford University Press; Kurzweil, R., (2012) How to Create a Mind: The Secret of Human Thought Revealed, , New York, Viking Penguin; Leveringhaus, A., (2016) Ethics and Autonomous Weapons, , London, Palgrave Macmillan; Levy, D., (2007) Love and Sex with Robots: The Evolution of Human-Robot Relationships, , New York, HarperCollins; Lucas, G.R., Postmodern War (2010) Journal of Military Ethics, 9 (4), pp. 289-298; Lucas, G.R., Jr., Engineering, Ethics and Industry: The Moral Challenges of Lethal Autonomy (2013) Killing by Remote Control: The Ethics of an Unmanned Military, pp. 211-228. , B. J. Strawser. New York, Oxford University Press; Lucas, J.R., The Philosophy of the Reasonable Man (1963) The Philosophical Quarterly (1950), 13 (51), pp. 97-106; Madl, T., Franklin, S., Constrained Incrementalist Moral Decision Making for a Biologically Inspired Cognitive Architecture (2015) A Construction Manual for Robots', pp. 137-153. , Ethical Systems. R. Trappl. London, Springer; Malle, B.F., Scheutz, M., (2014) Moral Competence in Social Robots, , Ethics in Science, Technology and Engineering, 2014 IEEE International Symposium on, IEEE; McDermott, D., What Matters to a Machine? (2012) Machine Ethics, pp. 88-114. , M. Anderson and S. L. Anderson. Cambridge, Cambridge University Press; Metzinger, T., Two Principles of Robot Ethics (2013), www.blogs.uni-mainz.de/fb05philosophieengl/files/2013/07/Metzinger_RG_2013_penultimate.pdf, Retrieved 22nd Jul., 2015; Mill, J.S., (1863) Utilitarianism, , London, Parker, Son and Bourn; Aircraft Accident to Royal Air Force Tornado GR MK4A ZG710 (2004), www.gov.uk/government/uploads/system/uploads/attachment_data/file/82817/maas03_02_tornado_zg710_22mar03.pdf, Retrieved 6th Jan., 2015; Nagel, T., What Is It Like to Be a Bat? (1974) The Philosophical Review, 83 (4), pp. 435-450; Noddings, N., (1984) Caring: A Feminine Approach to Ethics and Moral Education, , Berkeley, University of California Press; Parfit, D., (2011) On What Matters, , Oxford; New York, Oxford University Press; Penrose, R., (1990) The Emperor's New Mind: Concerning Computers, Minds, and the Laws of Physics, , Oxford, Oxford University Press; Pereira, L.M., Saptawijaya, A., (2016) Programming Machine Ethics, , London, Springer; Picard, R.W., (1997) Affective Computing, , Cambridge, MA, MIT Press; Rachels, S., Rachels, J., (2014) The Elements of Moral Philosophy, , Dubuque, McGraw- Hill Education; Rawls, J., (1972) A Theory of Justice, , Oxford, Clarendon Press; Reader, S., (2007) Needs and Moral Necessity, , London; New York, Routledge; Ross, W.D., (1930) The Right and the Good, , Oxford, The Clarendon Press; Scanlon, T., (1998) What We Owe to Each Other, , Cambridge, MA, Harvard University Press; Scherer, K., BÃ¤nziger, T., Roesch, E., (2010) A Blueprint for Affective Computing: A Sourcebook and Manual, , Oxford, Oxford University Press; Scheutz, M., The Inherent Dangers of Unidirectional Emotional Bonds between Humans and Social Robots (2012) Robot Ethics, pp. 205-222. , P. Lin, K. Abney and G. Bekey. Cambridge, MA, MIT Press; Sidgwick, H., The Methods of Ethics (1907), www.gutenberg.org/files/46743/46743-h/46743-h.htm, Seventh Edition. Retrieved 5th Oct., 2016; Singer, P.W., (2009) Wired for War: The Robotics Revolution and Conflict in the Twenty- First Century, , London, Penguin; Stasi, A., An Introduction to the Nature and Role of the Reasonable Person Standard in Asian Civil Law Jurisdictions (2015) American Law Register, 49 (3), pp. 148-164; Steels, L., The Symbol Grounding Problem Has Been Solved. So What's Next (2008) Symbols and Embodiment. Debates on Meaning and Cognition, pp. 223-244. , M. de Vega, A. Glenberg and A. Graesser. Oxford, OUP; Tonkens, R., Out of Character: On the Creation of Virtuous Machines (2012) Ethics and Information Technology, 14 (2), pp. 137-149; Tononi, G., Koch, C., Consciousness: Here, There and Everywhere? (2015) Philosophy Transactions of the Royal Society B, 370 (1668), p. 20140167; Treiber, M., (2010) An Introduction to Object Recognition: Selected Algorithms for a Wide Variety of Applications, , London, Springer; Healthcare at Home Limited (Appellant) v The Common Services Agency (Respondent) (Scotland) (2014), www.supremecourt.uk/decided-cases/docs/UKSC_2013_0108_Judgment.pdf, Retrieved 30th Jan., 2015; (1891) The War of the Rebellion: A Compilation of the Official Records of the Union and Confederate Armies, , Washington, Government Printing Office; Vilmer, J.-B.J., Terminator Ethics: Should We Ban 'Killer Robots'? (2015), www.ethicsandinternationalaffairs.org/2015/terminator-ethics-bankiller-robots/, Retrieved 13th Jan., 2017; Walzer, M., (1977) Just and Unjust Wars: A Moral Argument with Historical Illustrations, , New York, Basic Books; Weizenbaum, J., McCarthy, J., (1977) Computer Power and Human Reason: From Judgment to Calculation, , San Francisco, WH Freeman; Anderson, M., Anderson, S.L.A., (2005) Toward Machine Ethics: Implementing Two Action-Based Ethical Theories, , Machine Ethics: Papers from the AAAI Fall Symposium. Technical Report FS-05-06, Washington DC, Association for the Advancement of Artificial Intelligence, Menlo Park, CA; Arkin, R.C., (2009) Governing Lethal Behaviour in Autonomous Robots, , Boca Rouge, CRC Press; Bekey, G.A., (2005) Autonomous Robots: From Biological Inspiration to Implementation and Control, , Cambridge, MA, MIT press; Bentham, J., An Introduction to the Principles of Morals and Legislation (1780), www.econlib.org/library/Bentham/bnthPML.html, Retrieved 8th Oct., 2016; Brachman, R., Levesque, H., (2004) Knowledge Representation and Reasoning, , Boston, Elsevier; Brock, G., Needs and Global Justice (2005) The Philosophy of Need, pp. 51-72. , S. Reader. Cambridge, Cambridge University Press. Royal Institute of Philosophy Supplement 57; Colyvan, M., Cox, D., Steele, K., Modelling the Moral Dimension of Decisions (2010) NoÃ»s, 44 (3), pp. 503-529; Dancy, J., (2004) Ethics Without Principles, , Oxford, OUP; Defoe, D., Robinsoe Crusoe (1719), www.gutenberg.org/files/521/521-h/521-h.htm, Retrieved 3rd Jan., 2017; Gert, B., (1988) Morality, , Oxford. Oxford University Press; Gigerenzer, G., Moral Intuition = Fast and Frugal Heuristics (2007) Moral Psychology: The Cognitive Science of Morality: Intuition and Diversity, pp. 1-26. , W. Sinnott-Armstrong. Cambridge MA, MIT Press. 2; Gips, J., Towards the Ethical Robot. The Second International Workshop on Human and Machine Cognition: Android Epistemology (1991), Perdido Key, Florida; Goertzel, B., Geisweiller, N., Coelho, L., Janicic, P., Pennachin, C., (2011) Real World Reasoning: Toward Scalable, Uncertain Spatiotemporal, Contextual and Causal Inference, , Paris, Atlantis Press; Gould, S.J., Non-overlapping Magisteria (1997) Natural History, 106, pp. 16-22; Harris, S., (2010) The Moral Landscape: How Science Can Determine Human Values, , London, Bantam; Hobbes, T., Leviathan (1651), www.gutenberg.org/files/3207/3207-h/3207-h.htm, Retrieved 17th Jul., 2015; Hume, D., A Treatise of Human Nature (1738), www.gutenberg.org/files/4705/4705-h/4705-h.htm, Retrieved 17th Jul., 2015; Hursthouse, R., (1999) On Virtue Ethics, , Oxford, Oxford University Press; Protocol Additional to the Geneva Conventions of 12 Aug., 1949, and Relating to the Protection of Victims of International Armed Conflicts (Protocol I), 8 Jun., 1977. Article 36 (1977), www.icrc.org/ihl/WebART/470-750045?OpenDocument, Retrieved 24th Feb., 2015; Kant, I., Groundwork of the Metaphysics of Morals (1785), www.gutenberg.org/ebooks/5682, Retrieved 29th Nov., 2015; Kernighan, B.W., Ritchie, D., (1978) The C Programming Language, , Englewood Cliffs, NJ, Prentice-Hall; Korsgaard, C.M., (2009) Self-Constitution: Agency, Identity, and Integrity, , Oxford; New York, Oxford University Press; Kurzweil, R., (2012) How to Create a Mind: The Secret of Human Thought Revealed, , New York, Viking Penguin; Locke, J., Second Treatise on Government (1689), www.gutenberg.org/files/7370/7370-h/7370-h.htm, Retrieved 1st Dec., 2015; MacIntyre, A.C., Hume on 'Is' and 'Ought' (1959) The Philosophical Review, 68 (4), pp. 451-468; Malle, B.F., Scheutz, M., (2014) Moral Competence in Social Robots, , Ethics in Science, Technology and Engineering, 2014 IEEE International Symposium on Ethics in Science, Technology and Engineering, IEEE, Chicago; Maslow, A., (1954) Motivation and Personality, , New York, Harper & Row; McConnell, S., (2006) Software Estimation: Demystifying the Black Art, , Redmond, WA, Microsoft Press; McCune, W., Prover 9 and Mace 4 (2010), www.cs.unm.edu/~mccune/Prover9; Mill, J.S., (1863) Utilitarianism, , London, Parker, Son and Bourn; Moor, J.H., The Nature, Importance and Difficulty of Machine Ethics (2006) IEEE Intelligent Systems, 21 (4), pp. 18-21; Nagel, T., What Is It Like to Be a Bat? (1974) The Philosophical Review, 83 (4), pp. 435-450; Noddings, N., (2003) Caring: A Feminine Approach to Ethics and Moral Education, , Berkeley, University of California Press; O'Neill, O., A Simplified Account of Kant's Ethics (2007) The Elements of Philosophy, pp. 112-114. , T. Gendler, S. Siegel and S. M. Cahn. Oxford, OUP; Parfit, D., (2011) On What Matters, , Oxford; New York, Oxford University Press; Penrose, R., (1990) The Emperor's New Mind: Concerning Computers, Minds, and the Laws of Physics, , Oxford, Oxford University Press; Pereira, L.M., Saptawijaya, A., (2016) Programming Machine Ethics, , London, Springer; Rachels, S., Rachels, J., (2014) The Elements of Moral Philosophy, , Dubuque, McGraw- Hill Education; Rand, A., The Objectivist Ethics (1961), http://aynrandlexicon.com/ayn-rand-ideas/the-objectivist-ethics.html, Retrieved 23rd Oct., 2014; Rawls, J., Outline of a Decision Procedure for Ethics (1951) The Philosophical Review, 60 (2), pp. 177-197; Rawls, J., (1972) A Theory of Justice, , Oxford, Clarendon Press; Raz, J., Numbers, with and without Contractualism (2003) Ratio, 16 (4), pp. 346-367; Reader, S., (2007) Needs and Moral Necessity, , London; New York, Routledge; Ross, W.D., (1930) The Right and the Good, , Oxford, The Clarendon Press; Rousseau, J.-J., The Social Contract (1762), www.gutenberg.org/files/46333/46333-h/46333-h.htm, Retrieved 1st Dec., 2015; Russell, S., Norvig, P., (2010) Artificial Intelligence: A Modern Approach, , Upper Saddle River, NJ, Prentice Hall; Scanlon, T., (1998) What We Owe to Each Other, , Cambridge, MA, Harvard University Press; Scheffler, S., Introduction (2011) On What Matters, pp. 19-32. , S. Scheffler. Oxford, Oxford University Press. 1; Scherer, K., The Component Process Model: Architecture for a Comprehensive Computational Model of Emergent Emotion (2010) Blueprint for Affective Computing: A Sourcebook and Manual, pp. 47-84. , K. Scherer, T. Banziger and E. Roesch. Oxford, Oxford University Press; Scherer, K., BÃ¤nziger, T., Roesch, E., (2010) A Blueprint for Affective Computing: A Sourcebook and Manual, , Oxford, Oxford University Press; Scheutz, M., The Inherent Dangers of Unidirectional Emotional Bonds between Humans and Social Robots (2012) Robot Ethics, pp. 205-222. , P. Lin, K. Abney and G. Bekey. Cambridge, MA, MIT Press; Sen, A.K., (1985) Commodities and Capabilities, , Amsterdam; New York, North-Holland; Sidgwick, H., The Methods of Ethics (1907), www.gutenberg.org/files/46743/46743-h/46743-h.htm, Seventh Edition. Retrieved 5th Oct., 2016; Singer, P.W., (2009) Wired for War: The Robotics Revolution and Conflict in the Twenty- First Century, , London, Penguin; Sperry, R.W., (1983) Science & Moral Priority: Merging Mind, Brain, and Human Values, , Oxford, Blackwell; Timmons, M., (2002) Moral Theory: An Introduction, , Lanham, Rowman & Littlefield; Tonkens, R., Out of Character: On the Creation of Virtuous Machines (2012) Ethics and Information Technology, 14 (2), pp. 137-149; Turing, A.M., Computing Machinery and Intelligence (1950) Mind, 59 (236), pp. 433-460; von Wright, G.H., Deontic Logic (1951) Mind, 60 (237), pp. 1-15; Wallech, W., Allen, C., (2009) Moral Machines, , Oxford; New York, Oxford University Press; Weizenbaum, J., McCarthy, J., (1977) Computer Power and Human Reason: From Judgment to Calculation, , San Francisco, WH Freeman; Wiggens, D., (1982) Needs, Values and Truth: Essays in the Philosophy of Value, , Oxford, Oxford University Press; Wood, A.W., (2008) Kantian Ethics, , Cambridge; New York, Cambridge University Press; Allen, C., Varner, A., Zinser, J., Prolegomena to Any Future Artificial Moral Agent (2000) Journal of Experimental and Theoretical Artificial Intelligence, 12 (3), pp. 251-261; Arnold, T., Scheutz, M., Against the Moral Turing Test: Accountable Design and the Moral Reasoning of Autonomous Systems (2016) Ethics and Information Technology, 18 (2), pp. 103-115; Beck, K., (2003) Test-Driven Development: By Example, , Boston, MA, Addison-Wesley; Arkin, R.C., (2009) Governing Lethal Behaviour in Autonomous Robots, , Boca Rouge, CRC Press; Explainable Artificial Intelligence (2016), www.darpa.mil/program/explainable-artificial-intelligence, Retrieved 13th Sep., 2016; Goodfellow, I., Bengio, Y., Courville, A., (2016) Deep Learning, , Cambridge, MA, MIT Press; Goodman, B., Flaxman, S., European Union regulations on algorithmic decisionmaking and a "right to explanation" (2016), https://arxiv.org/abs/1606.08813, Retrieved 14th Sept, 2017; Guarini, M., Particularism and Classification and Reclassification of Moral Cases (2006) IEEE Intelligent Systems [H.W.Wilson-AST], 21 (4), p. 22; Learning from Tay's Introduction (2016), https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/, Retrieved 9th Sep., 2016; Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Ostrovski, G., Human-Level Control through Deep Reinforcement Learning (2015) Nature, 518 (7540), pp. 529-533; Parfit, D., (2011) On What Matters, , Oxford; New York, Oxford University Press; Pereira, L.M., Saptawijaya, A., (2016) Programming Machine Ethics, , London, Springer; Arkin, R.C., (2009) Governing Lethal Behaviour in Autonomous Robots, , Boca Rouge, CRC Press; Belnap, N., Perloff, M., Seeing to It That: A Canonical Form for Agentives (1988) Theoria, 54, pp. 175-199; Bringsjord, S., Arkoudas, C., Bello, P., Toward a General Logicist Methodology for Engineering Ethical Correct Robots (2006) IEEE Intelligent Systems, 21 (4), pp. 38-44; Bringsjord, S., Naveen Sundar, G., Deontic Cognitive Event Calculus (2013), www.cs.rpi.edu/~govinn/dcec.pdf, Retrieved 11th Nov., 2015; Bringsjord, S., Taylor, J., The Divine-Command Approach to Robot Ethics (2012) Robot Ethics: The Ethical and Social Implications of Robotics, pp. 85-108. , P. Lin, K. Abney and G. Bekey. Cambridge, MIT Press; Bringsjord, S., Arkoudas, C., Bello, P., Toward a General Logicist Methodology for Engineering Ethical Correct Robots (2006) IEEE Intelligent Systems, 21 (4), pp. 38-44; Brooks, R.A., Elephants Don't Play Chess (1990) Robotics and Autonomous Systems, 6 (1), pp. 3-15; Brooks, R.A., Intelligence without Representation (1991) Artificial Intelligence, 47 (1), pp. 139-159; CastaÃ±eda, H.-N., The Paradoxes of Deontic Logic: The Simplest Solution to All of Them in One Fell Swoop (1981) New Studies in Deontic Logic, pp. 37-86. , R. Hilpinen. Dordrecht, D. Reidel Publishing Company; Dancy, J., (2004) Ethics Without Principles, , Oxford, OUP; Franklin, S., (1995) Artificial Minds, , Cambridge, MA, MIT Press; Gert, J., (2012) Normative Bedrock, , Oxford, Oxford University Press; Guarini, M., Particularism, Analogy and Moral Cognition (2010) Minds and Machines, 20, pp. 385-422; Haikonen, P., (2012) Consciousness and Robot Sentience, , Hackensack, NJ, World Scientific; Hansen, J., The Paradoxes of Deontic Logic: Alive and Kicking (2006) Theoria, 72, pp. 221-232; Horty, J.F., (2001) Agency and Deontic Logic, , Oxford, Oxford University Press; Hughes, G.E., Cresswell, M.J., (1996) A New Introduction to Modal Logic, , New York, Routledge; Maslow, A., (1954) Motivation and Personality, , New York, Harper & Row; McLaren, B., Lessons in Machine Ethics from the Perspective of Two Computational Models of Ethical Reasoning, 2005 (2005), AAAI Fall Symposium on Machine Ethics, AAAI Technical Report FS-05-06, 2005; Parfit, D., (2011) On What Matters, , Oxford; New York, Oxford University Press; Pereira, L.M., Saptawijaya, A., (2016) Programming Machine Ethics, , London, Springer; Poole, D.L., Mackworth, A.K., (2010) Artificial Intelligence: Foundations of Computational Agents, , Cambridge, Cambridge University Press; Reader, S., (2007) Needs and Moral Necessity, , London; New York, Routledge; Baxter (2015), www.rethinkrobotics.com/baxter/, Retrieved 17th Feb., 2015; Scanlon, T., How I am Not a Kantian (2011) On What Matters, pp. 119-138. , S. Scheffler. Oxford, Oxford University Press. 2; Wallech, W., Allen, C., (2009) Moral Machines, , Oxford; New York, Oxford University Press; Xu, M., Actions as Events (2012) Journal of Philosophical Logic, 41 (4), pp. 765-809; Arkin, R.C., Integrating Behavioral, Perceptual, and World Knowledge in Reactive Navigation (1990) Robotics and Autonomous Systems, 6 (1-2), pp. 105-122; Arkin, R.C., (2009) Governing Lethal Behaviour in Autonomous Robots, , Boca Rouge, CRC Press; Killer Robots: UK Government Policy on Fully Autonomous Weapons (2013), www.article36.org/wp-content/uploads/2013/04/Policy_Paper1.pdf, Retrieved 26th May, 2015; Beauchamp, Z., Savulescu, J., Robot Guardians: Teleoperated Combat Vehicles in Humanitarian Intervention (2013) Killing by Remote Control: The Ethics of an Unmanned Military, pp. 106-125. , B. J. Strawser. Oxford, USA, Oxford University Press; Belnap, N., Perloff, M., Seeing to It That: A Canonical Form for Agentives (1988) Theoria, 54, pp. 175-199; Bringsjord, S., Arkoudas, C., Bello, P., Toward a General Logicist Methodology for Engineering Ethical Correct Robots (2006) IEEE Intelligent Systems, 21 (4), pp. 38-44; Bringsjord, S., Naveen Sundar, G., Deontic Cognitive Event Calculus (2013), www.cs.rpi.edu/~govinn/dcec.pdf, Retrieved 8th Aug., 2015; Croitoru, M., Oren, N., Miles, S., Luck, M., Graphical Norms via Conceptual Graphs (2012) Knowledge-Based Systems, 29, pp. 31-43; Everett, J.A., Pizarro, D.A., Crockett, M.J., Inference of Trustworthiness from Intuitive Moral Judgments (2016) Journal of Experimental Psychology: General, 145 (6), p. 772; Hansen, J., The Paradoxes of Deontic Logic: Alive and Kicking (2006) Theoria, 72, pp. 221-232; Hansson, S.O., Alternative Semantics for Deontic Logic (2013) Handbook of Deontic Logic and Normative Systems, pp. 445-498. , D. Gabbay, J. Horty, X. Parent, R. Van der Mayden and L. Van der Torre. Milton Keynes, College Publications; Harris, S., (2010) The Moral Landscape: How Science Can Determine Human Values, , London, Bantam; Hilpinen, R., Preface (1981) New Studies in Deontic Logic, pp. 7-9. , R. Hilpinen. Dordrecht, D. Reidel Publishing Company; Horty, J.F., (2001) Agency and Deontic Logic, , Oxford, Oxford University Press; Hursthouse, R., (1999) On Virtue Ethics, , Oxford, Oxford University Press; Protocol on Prohibitions or Restrictions on the Use of Mines, Booby-Traps and Other Devices as amended on 3 May 1996 (Protocol II to the 1980 CCW Convention as amended on 3 May 1996) (1996), www.icrc.org/ihl/INTRO/575, Retrieved 6th Mar., 2015; Convention on the Prohibition of the Use, Stockpiling, Production and Transfer of Anti-Personnel Mines and on Their Destruction, 18 September 1997 (1997), www.icrc.org/ihl/INTRO/580, Retrieved 6th Mar., 2015; Kagan, S., (1989) The Limits of Morality, , Oxford, Oxford University Press; Malle, B., Scheutz, M., Arnold, T., Voiklis, J., Cusimano, C., (2015) Sacrifice One for the Good of Many: People Apply Different Moral Norms to Humans and Robots, , 10th ACM/ IEEE International Conference on Human-Robot Interaction 2015, Portland, ACM; Pereira, L.M., Saptawijaya, A., (2016) Programming Machine Ethics, , London, Springer; Pigden, C.R., Logic and the Autonomy of Ethics (1989) Australasian Journal of Philosophy, 67 (2), pp. 127-151; Rachels, J., Active and Passive Euthanasia (1975) The New England Journal of Medicine, 292 (2), p. 78; Ross, W.D., (1930) The Right and the Good, , Oxford, The Clarendon Press; US Commander Hails Iraqi Forces Beating ISIS Drones and VBIEDs (2017), www.rudaw.net/english/middleeast/iraq/110120171, Retrieved 3rd Feb., 2017; Scharre, P., Autonomous Weapons and Operational Risk (2016), www.cnas.org/autonomous-weapons-and-operational-risk#.VtSsW_l97IU, Retrieved 1st Mar., 2016; Sperry, R.W., (1983) Science & Moral Priority: Merging Mind, Brain, and Human Values, , Oxford, Blackwell; van den Hoven, J., Lokhorst, G.-J., Deontic Logic and Computer-Supported Computer Ethics (2002) Metaphilosophy, 33, pp. 376-386; Nichomachean Ethics http://classics.mit.edu/Aristotle/nicomachaen.html, Retrieved 29th Nov., 2015; Glimscher, P., Fehr, E., (2013) Neuroeconomics, , Burlington, Elsevier Science; Hansson, S.O., The Varieties of Permission (2013) Handbook of Deontic Logic and Normative Systems, pp. 195-240. , D. Gabbay, J. Horty, X. Parent, R. Van der Mayden and L. Van der Torre. Milton Keynes, College Publications. 1; Jackson, F., Critical Notice (1992) Australasian Journal of Philosophy, 70 (4), pp. 475-488; Krotzsch, M., Simanzik, F., Horrocks, I., A Description Logic Primer (2012), http://arxiv.org/pdf/1201.4089.pdf, Retrieved 12th Dec., 2014; Nozick, R., (1981) Philosophical Explanations, , Cambridge, MA, Belknap Press; Robinson, I., Webber, J., Eifrem, E., (2015) Graph Databases, , Sebastapol, CA, O'Reilly; Anderson, S.L., The Unacceptability of Asimov's Three Laws as a Basis for Machine Ethics (2011) Machine Ethics, pp. 285-296. , M. Anderson and S. L. Anderson. Cambridge, Cambridge University Press; Aquinas, T., (1947) Summa Theologica, , (trans. Fathers of the Dominican Province) New York, Benziger Bros; Bourget, D., Chalmers, D., What Do Philosophers Believe? (2014) Philosophical Studies, 170 (3), pp. 465-500; Brock, G., Needs and Global Justice (2005) The Philosophy of Need. S. Reader. Cambridge, Cambridge University Press. Royal Institute of Philosophy Supplement, 57, pp. 51-72; Everett, J.A., Pizarro, D.A., Crockett, M.J., Inference of Trustworthiness from Intuitive Moral Judgments (2016) Journal of Experimental Psychology: General, 145 (6), p. 772; Foot, P., The Problem of Abortion and the Principle of Double Effect (1967) Oxford Review, 5, pp. 5-15; Gert, J., (2012) Normative Bedrock, , Oxford, Oxford University Press; Greene, J.D., The Secret Joke of Kant's Soul (2007) Moral Psychology: The Neuroscience of Morality: Emotion, Brain Disorders, and Development, pp. 35-80. , W. Sinnott-Armstrong. Cambridge, MA, MIT Press. 3; Hauser, M.D., (2006) Moral Minds: How Nature Designed Our Universal Sense of Right and Wrong, , New York, HarperCollins; The Martian (2015), www.imdb.com/title/tt3659388/, Retrieved 14th Oct., 2016; Jackson, F., Critical Notice (1992) Australasian Journal of Philosophy, 70 (4), pp. 475-488; McIntyre, A., Doing Away with Double Effect (2001) Ethics, 111 (2), pp. 219-255; Melaugh, M., The Hunger Strike of 1981-List of Dead and Other Hunger Strikers (2016), http://cain.ulst.ac.uk/events/hstrike/chronology.htm, Retrieved 27th Oct., 2016; Peel, M., Hunger Strikes (1997) BMJ, 315, pp. 829-830; Pereira, L.M., Saptawijaya, A., (2016) Programming Machine Ethics, , London, Springer; Prior, A.N., (1957) Time and Modality, , Oxford, Oxford University Press; Prior, A.N., The Autonomy of Ethics (1960) Australasian Journal of Philosophy, 38 (3), pp. 199-206; Prior, A.N., (1967) Past, Present and Future, , Oxford, Clarendon Press Oxford; Rachels, J., Active and Passive Euthanasia (1975) The New England Journal of Medicine, 292 (2), p. 78; Rawls, J., (1972) A Theory of Justice, , Oxford, Clarendon Press; Reader, S., (2007) Needs and Moral Necessity, , London; New York, Routledge; Ross, A., Imperatives and Logic (1941) Theoria, 7 (1941), pp. 53-71; Scanlon, T., (1998) What We Owe to Each Other, , Cambridge, MA, Harvard University Press; Singer, P., The Drowning Child and the Expanding Circle (1997) New Internationalist, 289, pp. 28-30; (2016) Public Safety and Aquatic Rescue, , Chatswood NSW, Elsevier Australia; Timmons, M., (2002) Moral Theory: An Introduction, , Lanham, Rowman & Littlefield; Weir, A., (2014) The Martian, , London, Del Ray; Wiggens, D., (1982) Needs, Values and Truth: Essays in the Philosophy of Value, , Oxford, Oxford University Press; Wood, A., Trolley Problems (2011) On What Matters, pp. 66-82. , D. Parfit. Oxford, Oxford University Press. 2; Aquinas, T., (1947) Summa Theologica, , (trans. Fathers of the Dominican Province) New York: Benziger Bros; Arntz, M., Gregory, T., Zierahn, U., The Risk of Automation for Jobs in OECD Countries: A Comparative Analysis (2016), http://dx.doi.org/10.1787/5jlz9h56dvq7-en, OECD Social, Employment and Migration Working Papers, No. 189, OECD Publishing, Paris; Ashford, E., Mulgan, T., Contractualism (2012), http://plato.stanford.edu/archives/fall2012/entries/contractualism/, Stanford Encyclopedia of Philosophy. Retrieved 31st Oct., 2016; Brock, G., Needs and Global Justice (2005) The Philosophy of Need, pp. 51-72. , S. Reader. Cambridge, Cambridge University Press. Royal Institute of Philosophy Supplement 57; Dunlop, T., (2016) Why the Future Is Workless, , Sydney, NewSouth; Ford, M., (2015) Rise of the Robots: Technology and the Threat of a Jobless Future, , London, Oneworld; Frey, C.B., Osborne, M.A., Holmes, C., Rahbari, E., Curmi, E., Garlick, R., Chua, J., Wilkie, M., Technology at Work v. 2.0: The Future Is Not What It Used To Be (2016), www.oxfordmartin.ox.ac.uk/downloads/reports/Citi_GPS_Technology_Work_2.pdf, Retrieved 28th Sep., 2016; Frohlich, N., Oppenheimer, J., (1992) Choosing Justice: An Experimental Approach to Ethical Theory, , Berkeley, CA, University of California Press; Hauser, M.D., (2006) Moral Minds: How Nature Designed Our Universal Sense of Right and Wrong, , New York, HarperCollins; The Martian (2015), www.imdb.com/title/tt3659388/, Retrieved 14th Oct., 2016; Kant, I., Groundwork of the Metaphysics of Morals (1785), http://www.gutenberg.org/ebooks/5682, Retrieved 29th Nov, 2015; Nozick, R., (1974) Anarchy, State and Utopia, , Oxford, Blackwell; O'Neill, O., Consequences for Non-Consequentialists (2004) Utilitas, 16 (1), pp. 1-11; Parfit, D., (2011) On What Matters, , Oxford; New York, Oxford University Press; Piketty, T., (2014) Capital in the Twenty-First Century, , Cambridge, MA, Harvard University Press; Weir, A., (2014) The Martian, , London, Del Ray; Korsgaard, C.M., (2009) Self-Constitution: Agency, Identity, and Integrity, , Oxford; New York, Oxford University Press; Rachels, S., Rachels, J., (2014) The Elements of Moral Philosophy, , Dubuque, McGraw- Hill Education; Wolf, S., Hiking the Range (2011) On What Matters, pp. 33-57. , S. Schleffer. Oxford, Oxford University Press. 2; Dancy, J., (2004) Ethics without Principles, , Oxford, Oxford University Press; Arkin, R.C., (2009) Governing Lethal Behaviour in Autonomous Robots, , Boca Rouge, CRC Press; Arkin, R.C., The Case for Ethical Autonomy in Unmanned Systems (2010) Journal of Military Ethics, 9 (4), pp. 332-341; Bekey, G.A., (2005) Autonomous Robots: From Biological Inspiration to Implementation and Control, , Cambridge, MA, MIT press; (2016) Robots and Robotic Devices: Guide to the Ethical Design and Application of Robots and Robotic Systems, , London, BSI Standards Ltd; The COSMIC Functional Size Measurement Method (2015), http://cosmic-sizing.org/publications/introduction-to-the-cosmicmethod-of-measuring-software/, Retrieved 30th Aug., 2015; Croitoru, M., Oren, N., Miles, S., Luck, M., Graphical Norms via Conceptual Graphs (2012) Knowledge-Based Systems, 29, pp. 31-43; The Role of Autonomy in DoD Systems (2012), http://fas.org/irp/agency/dod/dsb/autonomy.pdf; Principles of Robotics (2010), www.epsrc.ac.uk/research/ourportfolio/themes/engineering/activities/principlesofrobotics/, Retrieved 19th Jan., 2017; Galliott, J., Responsibility and War Machines: Towards a Forward-Looking and Functional Account (2015) Rethinking Machine Ethics in the Age of Ubiquitous Technology, pp. 152-165. , J. White and R. Searle. Hershey, PA, IGI Global; Heyns, C., Panel on Human Rights and Lethal Autonomous Weapons Systems (LAWS) Comments by Christof Heyns, United Nations Special Rapporteur on Extrajudicial, Summary or Arbitrary Executions (as Finalised after the Meeting) (2015), http://unog.ch/80256EDD006B8954/(httpAssets)/1869331AFF45728BC1257E2D0050EFE0/$file/2015_LAWS_MX_Heyns_Transcript.pdf, Retrieved 28th May, 2015; Leveringhaus, A., (2016) Ethics and Autonomous Weapons, , London, Palgrave Macmillan; Lin, P., The Right to Life and the Martens Clause (2015), http://unog.ch/80256EDD006B8954/(httpAssets)/2B52D16262272AE2C1257E2900419C50/$file/24+Patrick+Lin_Patrick+SS.pdf, Retrieved 21st Apr., 2015; Madl, T., Franklin, S., Constrained Incrementalist Moral Decision Making for a Biologically Inspired Cognitive Architecture (2015) A Construction Manual for Robots' Ethical Systems, pp. 137-153. , R. Trappl. London, Springer; Scharre, P., Autonomous Weapons and Operational Risk (2016), www.cnas.org/autonomous-weapons-and-operational-risk#.VtSsW_l97IU, Retrieved 1st Mar., 2016; Sparrow, R., Can Machines Be People? Reflections on the Turing Triage Test (2012) Robot Ethics: The Ethical and Social Implications of Robotics, pp. 301-316. , P. Lin, K. Abney and G. Bekey. Cambridge MA, MIT Press},
correspondence_address1={Welsh, S.},
editor={NA},
publisher={Taylor and Francis},
issn={NA},
isbn={9781315168951; 9781138050228},
language={English},
abbrev_source_title={Ethics and Secur. Autom.: Policy and Tech. Chall. of the Robot. Use of Force},
document_type={Book},
source={Scopus},
number={NA},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={NA},
address={NA},
screened_abstracts={selected},
notes={NA},
}

@ARTICLE{Bendel20177,
type={CONFERENCE},
author={Bendel, O. and Schwegler, K. and Richards, B.},
title={Towards kant machines},
journal={AAAI Spring Symposium - Technical Report},
year={2017},
volume={SS-17-01 - SS-17-08},
pages={7-11},
doi={NA},
note={cited By 6; Conference of 2017 AAAI Spring Symposium ; Conference Date: 27 March 2017 Through 29 March 2017;  Conference Code:129736},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028711580&partnerID=40&md5=56a50b4ae498bd9bb83b32bab0549b13},
affiliation={School of Business, FHNW, Bahnhofstrasse 6, Windisch, CH-5210, Switzerland},
abstract={For some years now, ethics no longer only means human ethics. The young discipline of machine ethics researches the morality of semi-autonomous and autonomous systems like self-driving cars, robots and drones. Interactive software systems such as chatbots are also relevant. In 2013, the School of Business at the University of Applied Sciences and Arts Northwestern Switzerland FHNW implemented a prototype of the GOODBOT, which is a novelty chatbot and a simple moral machine. One of its meta-rules was that it should not lie unless not lying would hurt the user. In a follow-up project in 2016, the LIEBOT was developed, a kind of Munchausen machine. This article describes the background and the foundations of this project and lists the chatbot's strategies of lying. Then it discusses how Munchausen machines as immoral machines can contribute to the construction and optimization of moral machines, for example Kant machines, which prefer the truth. The LIEBOT serves as a contribution to machine ethics as well as a critical review of electronic language-based systems and services. Â© Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
author_keywords={NA},
keywords={Artificial intelligence;  Learning systems;  Philosophical aspects, Autonomous systems;  Critical review;  Electronic languages;  Interactive software;  Meta rules;  Self drivings;  Switzerland;  University of applied science, Machine oriented languages},
references={Aegerter, A., FHNW forscht an "moralisch gutem" Chatbot (2014) Netzwoche, p. 18. , 4/2014; Anderson, M., Anderson, S.L., (2011) Machine Ethics, , Cambridge: Cambridge University Press; Arkin, R.C., Robots that need to mislead: Biologically inspired machine deception (2016) Technical Report GIT-MRL12-04, Georgia Tech, , http://www.cc.gatech.edu/ai/robot-lab/online-publications/Robots_that_Need_to_Misleadv5.pdf, Atlanta, GA; Asimov, I., (1973) The Best of Isaac Asimov, , Stamford, CT: Sphere; Bendel, O., Schwegler, K., Richards, B., The L1EBOT project (2016) Machine Ethics and Machine Law, , http://machinelaw.philosophyinscience.com/wp-content/uploads/2016/06/PROCEEDINGS-verl-2.pdf, Extended abstract for the international conference in Krakow, November 18-19, 2016; Bendel, O., Towards munchausen machines (2016) Whitepaper, , http://luegenbot.ch/res/LIEBOT_Whitepaper.pdf; Bendel, O., KÃ¶nnen Maschinen lÃ¼gen? Die Wahrheit Ã¼ber MÃ¼nchhausen-Maschinen (2015) Telepolis, , http://www.heise.de/tp/artikel/44/44242/1.html, March 1, 2015; Bendel, O., Good bot, bad bot: Dialog zwischen mensch und maschine (2013) UnternehmerZeitung, 7 (19), pp. 30-31. , (2013); Bendel, O., Der LÃ¼genbot und andere MÃ¼nchhausen-Maschinen (2013) CyberPress, , http://cyberpress.de/wiki/Maschinenethik, September 11, 2013; Bendel, O., Maschinenethik (2012) Gabler Wirtschaftslexikon, , http://wirtschaftslexikon.gabler.de/Definition/maschinenethik.html, Wiesbaden: Springer Gabler; HammwÃ¶hner, R., KÃ¶nnen Computer lÃ¼gen? (2003) Kulturen der LÃ¼ge, pp. 299-320. , Mayer, M. ed. KÃ¶ln: BÃ¶hlau; Kant, I., (1914) Werke (Akademie-Ausgabe), 6. , Berlin: KÃ¶niglich PreuÃŸische Akademie der Wissenschaften; Mill, J.S., (1976) Der Utilitarismus, , Ditzingen: Reclam; Rojas, R., (2013) KÃ¶nnen Roboter LÃ¼gen? Essays zur Robotik und KÃ¼nstlichen Intelligenz, , Hannover: Heise Zeitschriften Verlag; Schwegler, K., (2016) Gefahrenpotenzial von LÃ¼genbots, , Bachelor Thesis. School of Business FHNW. Olten; Wagner, A.R., Arkin, R.C., Acting deceptively: Providing robots with the capacity for deception (2011) International Journal of Social Robotics, 3 (1), pp. 5-26. , January 2011; Wallach, W., Allen, C., (2009) Moral Machines: Teaching Robots Right from Wrong, , Oxford: Oxford University Press; Williams, H., (2016) Microsoft's Teen Chatbot has Gone Wild, , http://www.gizmodo.com.au/2016/03/microsofts-teen-chatbot-has-gone-wild, March 25, 2016},
correspondence_address1={NA},
editor={NA},
publisher={AI Access Foundation},
issn={NA},
isbn={9781577357797},
language={English},
abbrev_source_title={AAAI Spring Symp. Tech. Rep.},
document_type={Conference Paper},
source={Scopus},
number={NA},
art_number={NA},
funding_details={NA},
coden={NA},
pubmed_id={NA},
sponsors={},
address={NA},
screened_abstracts={selected},
notes={NA},
}

